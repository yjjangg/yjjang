{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPR5nVxlgTHUoJHvXy2TE7F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yjjangg/yjjang/blob/main/Ch2~Ch5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2-1. pandas data structure**"
      ],
      "metadata": {
        "id": "YR1WjD_agyIG"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SiS65E7d0D3K"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dURWavPK0MNT"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "os.listdir()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23HBy9ge0VWZ"
      },
      "source": [
        "cd drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVppfTZH0YR2"
      },
      "source": [
        "os.listdir()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_bX15D70iCh"
      },
      "source": [
        "cd MyDrive/Hands-On-Data-Analysis-with-Pandas-2nd-edition/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mewaNbkj0oW6"
      },
      "source": [
        "cd ch_02"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kARB_l8Iz9ew"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "data = np.genfromtxt(\n",
        "    'data/example_data.csv', delimiter=';', \n",
        "    names=True, dtype=None, encoding='UTF'\n",
        ")\n",
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHDgyg_Dz9ez"
      },
      "source": [
        "We can find the dimensions with the `shape` attribute:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOoq6eZnz9e0"
      },
      "source": [
        "data[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "992Q7iRYz9e1"
      },
      "source": [
        "We can find the data types with the `dtype` attribute:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enTHMPD-z9e1"
      },
      "source": [
        "data.dtype"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFfkSAtNz9e2"
      },
      "source": [
        "Each of the entries in the array is a row from the CSV file. NumPy arrays contain a single data type (unlike lists, which allow mixed types); this allows for fast, vectorized operations. When we read in the data, we got an array of `numpy.void` objects, which are created to store flexible types. This is because NumPy has to store several different data types per row: four strings, a float, and an integer. This means we can't take advantage of the performance improvements NumPy provides for single data type objects.\n",
        "\n",
        "Say we want to find the maximum magnitude&mdash;we can use a **[list comprehension](https://www.python.org/dev/peps/pep-0202/)** to select the third index of each row, which is represented as a `numpy.void` object. This makes a list, meaning that we can take the maximum using the `max()` function:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "hLwNBEwj_T5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80OADjhL067P"
      },
      "source": [
        "max([row[3] for row in data])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[row[3] for row in data]"
      ],
      "metadata": {
        "id": "BrFd4_BEFx8z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8mG9bByz9e4"
      },
      "source": [
        "%%timeit\n",
        "max([row[3] for row in data])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZDd2je2z9e5"
      },
      "source": [
        "If we, instead, create a NumPy array for each column, this operation is much easier (and more efficient) to perform. We can use a **[dictionary comprehension](https://www.python.org/dev/peps/pep-0274/)** to make a dictionary where the keys are the column names and the values are NumPy arrays of the data:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.array([row[i] for row in data])"
      ],
      "metadata": {
        "id": "SyWOcc5QARV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKKRz0qlz9e6"
      },
      "source": [
        "array_dict = {\n",
        "    col: np.array([row[i] for row in data])\n",
        "    for i, col in enumerate(data.dtype.names)\n",
        "}\n",
        "array_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-McAtRXz9e7"
      },
      "source": [
        "Grabbing the maximum magnitude is now simply a matter of selecting the `mag` key and calling the `max()` method. This is nearly twice as fast as the list comprehension implementation when dealing with just 5 entries, imagine how much worse the first attempt will perform on large data sets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bq51yaJz1UiR"
      },
      "source": [
        "array_dict['mag']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0Gxim6Xz9e8"
      },
      "source": [
        "%%timeit\n",
        "array_dict['mag'].max()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0801WbCXz9e8"
      },
      "source": [
        "However, this representation has other issues. Say we wanted to grab all the information for the earthquake with the maximum magnitude, how would we go about that? We would need to find the index of the maximum and then for each of the keys in the dictionary grab that index:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "array_dict.items()"
      ],
      "metadata": {
        "id": "XHdk4th9Br63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OMO_aaOz9e9"
      },
      "source": [
        "np.array([\n",
        "    value[array_dict['mag'].argmax()] \n",
        "    for key, value in array_dict.items()\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpMkQQW6z9e-"
      },
      "source": [
        "The result is now a NumPy array of strings (our numeric values were converted), and we are now in the format from earlier. Also, consider trying to sort the data by magnitude from smallest to largest. In the first representation, we would have to sort the rows by examining the 3rd index. With the second representation, we would have to determine the order for the indices from the `mag` column, and then sort all the other arrays with those same indices. Clearly, working with several NumPy arrays of different data types at once is a bit cumbersome. However, `pandas` builds on top of NumPy arrays to make this easier. Let's start our exploration of `pandas` with an overview of the data structures.\n",
        "\n",
        "## `Series`\n",
        "The `Series` class provides a data structure for arrays of a single type with some additional functionality."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKfLDQPH1pdw"
      },
      "source": [
        "array_dict['place']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glxHjiVqz9e-"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "place = pd.Series(array_dict['place'], name='place')\n",
        "place"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "place.index.values"
      ],
      "metadata": {
        "id": "YKNBROoACjAP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6T4871w71zY6"
      },
      "source": [
        "place.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9t4dk3jyz9e_"
      },
      "source": [
        "Here are some commonly used attributes with `Series` objects:\n",
        "\n",
        "|Attribute | Returns |\n",
        "| --- | --- |\n",
        "| `name` | The name of the `Series` object |\n",
        "| `dtype` | The data type of the `Series` object |\n",
        "| `shape` | Dimensions of the `Series` object in a tuple of the form `(number of rows,)` |\n",
        "| `index` | The `Index` object that is part of the `Series` object |\n",
        "| `values` | The data in the `Series` object |\n",
        "\n",
        "For the most part, `pandas` objects use NumPy arrays for their internal data representations. However, for some data types, `pandas` builds upon NumPy to create its own [arrays](https://pandas.pydata.org/pandas-docs/stable/reference/arrays.html). For this reason, depending on the data type, `values` can either be a `pandas.array` or `numpy.array` object. Therefore, if we need to ensure we get a specific type back, then it is recommended to use the `array` attribute or `to_numpy()` method, respectively, instead of `values`.\n",
        "\n",
        "Now let's see some examples using these attributes.\n",
        "\n",
        "### Getting the name of the series\n",
        "The NumPy array held the name of the data in the `dtype` attribute; here, we can access it directly: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ixi0NZ25z9e_"
      },
      "source": [
        "place.index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSLSVfShz9fA"
      },
      "source": [
        "### Getting the data type\n",
        "A `Series` object holds a single data type. Here it is `'O'` for object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1EeN4-kz9fB"
      },
      "source": [
        "place.dtype"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irZLkNimz9fB"
      },
      "source": [
        "### Getting the dimensions of the series\n",
        "Just as with NumPy, we can use `shape` to get the dimensions as `(rows, columns)`. `Series` objects are a single column, so they only have values for the rows dimension. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1M2ZOVbZz9fC"
      },
      "source": [
        "place.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGfv7EMHz9fC"
      },
      "source": [
        "### Isolating the values from the series\n",
        "This `Series` object is storing its values as a NumPy array:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AcASEUwIz9fD"
      },
      "source": [
        "place.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbTpRpzxz9fD"
      },
      "source": [
        "## `Index`\n",
        "The addition of the `Index` class makes the `Series` class more powerful than a NumPy array. We can get the index from the `index` attribute of a `Series` object:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ya3HXqJz9fD"
      },
      "source": [
        "place_index = place.index\n",
        "place_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-U3eNhx4z9fD"
      },
      "source": [
        "As with `Series` objects, we can access the underlying data via the `values` attribute. Note that this `Index` object is also built on top of a NumPy array:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLHAkeHiz9fE"
      },
      "source": [
        "place_index.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEI5narQz9fE"
      },
      "source": [
        "Here are some commonly used attributes with `Index` objects:\n",
        "\n",
        "|Attribute | Returns |\n",
        "| --- | --- |\n",
        "| `name` | The name of the `Index` object |\n",
        "| `dtype` | The data type of the `Index` object |\n",
        "| `shape` | Dimensions of the `Index` object |\n",
        "| `values` | The data in the `Index` object |\n",
        "| `is_unique` | Check if the `Index` object has all unique values |\n",
        "\n",
        "We can check the type of the underlying data, just like with a `Series` object:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLT6ftnUz9fE"
      },
      "source": [
        "place_index.dtype"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ion5rmhz9fE"
      },
      "source": [
        "Same for the dimensions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uksiwcxz9fE"
      },
      "source": [
        "place_index.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2zb75ehz9fF"
      },
      "source": [
        "We can check if the values are unique:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6ZE7Fagz9fF"
      },
      "source": [
        "place_index.is_unique"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8SP-4Nhz9fF"
      },
      "source": [
        "With NumPy we can perform arithmetic operations element-wise between arrays:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEyNjarcz9fG"
      },
      "source": [
        "np.array([1, 1, 1]) + np.array([-1, 0, 1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OC9J_hB9z9fG"
      },
      "source": [
        "Pandas supports this as well, and the index determines how element-wise operations are performed. With addition, only the matching indices are summed:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UrXMfBFhz9fG"
      },
      "source": [
        "numbers = np.linspace(0, 10, num=5) # makes numpy array([0, 2.5, 5, 7.5, 10])\n",
        "x = pd.Series(numbers) # index is [0, 1, 2, 3, 4]\n",
        "y = pd.Series(numbers, index=pd.Index([1, 2, 3, 4, 5]))\n",
        "x + y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzXpB07az9fG"
      },
      "source": [
        "We aren't limited to the integer indices of list-like structures, and we can label our rows. The labels can be altered at any time and be things like dates or even another column. In chapter 3, we will discuss how to perform some operations on the index in order to change it. Then, in chapter 4, we will use the index for operations merging data and aggregating it.\n",
        "\n",
        "## `DataFrame`\n",
        "Having a `Series` object for each column is an improvement over the NumPy representation; however, we still have the same problem when wanting to sort based on a value or grab an entire row out. The `DataFrame` gives us a representation of a table formed from many `Series` objects that form the columns and a shared `Index` object that labels the rows. We can create a `DataFrame` object from either of the NumPy representations we were working with earlier (we could also make a `Series` object for each column, but there is no need to do so):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crJTpHzg3lsv"
      },
      "source": [
        "array_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifqVaWKgz9fH"
      },
      "source": [
        "df = pd.DataFrame(array_dict) \n",
        "\n",
        "# this will also work with the first representation\n",
        "# df = pd.DataFrame(data)\n",
        "\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oui4R4yWz9fH"
      },
      "source": [
        "We can check the type of the underlying data with `dtypes` (note that it is not `dtype` as with `Series` and `Index` objects since each column will have its own data type):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aw_b2s1Jz9fH"
      },
      "source": [
        "df.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYdPBH_Qz9fH"
      },
      "source": [
        "We can get the underlying data with the `values` attribute. Note that this looks very similar to our initial NumPy representation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cd-a4l7Qz9fI"
      },
      "source": [
        "df.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzWcjk-vz9fI"
      },
      "source": [
        "We can isolate the columns with the `columns` attribute. Notice that the columns are actually an `Index` object just on a different axis (columns are the horizontal index while rows are the vertical index)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkrhkYdAz9fI"
      },
      "source": [
        "df.index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLN4mBeXz9fI"
      },
      "source": [
        "Here are some commonly used attributes:\n",
        "\n",
        "|Attribute | Returns |\n",
        "| --- | --- |\n",
        "| `dtypes` | The data types of each column |\n",
        "| `shape` | Dimensions of the `DataFrame` object in a tuple of the form `(number of rows, number of columns)` |\n",
        "| `index` | The `Index` object along the rows of the `DataFrame` object |\n",
        "| `columns` | The name of the columns (as an `Index` object) |\n",
        "| `values` | The data in the `DataFrame` object |\n",
        "| `empty` | Check if the `DataFrame` object is empty |\n",
        "\n",
        "The `Index` object along the rows of the dataframe can be accessed via the `index` attribute (just as with `Series` objects):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PB0Eedwz9fL"
      },
      "source": [
        "df.index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7z7m4cKez9fM"
      },
      "source": [
        "As with both `Series` and `Index` objects, we can get the dimensions of the dataframe with the `shape` attribute. The result is of the form `(nrows, ncols)`. Our dataframe has 5 rows and 6 columns:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8wb5FF-z9fM"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRm5waBGz9fM"
      },
      "source": [
        "Note that we can also perform arithmetic on dataframes. Pandas will only perform the operation when both the index and column match. Here, we demonstrate addition. Since addition with strings means concatenation, `pandas` concatenated the string columns (`time`, `place`, `magType`, and `alert`) across dataframes. The numeric columns (`mag` and `tsunami`) were summed:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MShF-ghz9fM"
      },
      "source": [
        "df + df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2-2. creating dataframes**"
      ],
      "metadata": {
        "id": "ZG9yAgFwhAUL"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15_dyguN6JsG"
      },
      "source": [
        "import datetime as dt\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FkF3ncc6JsI"
      },
      "source": [
        "## Creating a `Series` object"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XI_qNfI56JsJ"
      },
      "source": [
        "np.random.seed(0) # set a seed for reproducibility\n",
        "pd.Series(np.random.rand(5), name='random')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sSNyiTV6JsK"
      },
      "source": [
        "## Creating a `DataFrame` object from a `Series` object\n",
        "Use the `to_frame()` method:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clyVO32M6JsL"
      },
      "source": [
        "pd.Series(np.linspace(0, 10, num=5)).to_frame()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJY6qPM06JsL"
      },
      "source": [
        "## Creating a `DataFrame` from Python Data Structures\n",
        "### From a dictionary of list-like structures\n",
        "The dictionary values can be lists, NumPy arrays, etc. as long as they have length (generators don't have length so we can't use them here):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vAmOAM4y6JsM"
      },
      "source": [
        "np.random.seed(0) # set seed so result is reproducible\n",
        "pd.DataFrame(\n",
        "    {\n",
        "        'random': np.random.rand(5),\n",
        "        'text': ['hot', 'warm', 'cool', 'cold', None],\n",
        "        'truth': [np.random.choice([True, False]) for _ in range(5)]\n",
        "    }, \n",
        "    index=pd.date_range(\n",
        "        end=dt.date(2019, 4, 21),\n",
        "        freq='1D',\n",
        "        periods=5, \n",
        "        name='date'\n",
        "    )\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-klYq9W6JsN"
      },
      "source": [
        "### From a list of dictionaries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mvqbgbfd6JsO"
      },
      "source": [
        "pd.DataFrame([\n",
        "    {'mag': 5.2, 'place': 'California'},\n",
        "    {'mag': 1.2, 'place': 'Alaska'},\n",
        "    {'mag': 0.2, 'place': 'California'},\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqioL4e76JsP"
      },
      "source": [
        "### From a list of tuples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJ6NWNHo6JsP"
      },
      "source": [
        "list_of_tuples = [(n, n**2, n**3) for n in range(5)]\n",
        "list_of_tuples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKibS4op6JsQ"
      },
      "source": [
        "pd.DataFrame(\n",
        "    list_of_tuples, \n",
        "    columns=['n', 'n_squared', 'n_cubed']\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHTEwxG_6JsR"
      },
      "source": [
        "### From a NumPy array"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJzOKgGY6JsR"
      },
      "source": [
        "df_1=pd.DataFrame(\n",
        "    np.array([\n",
        "        [0, 0, 0],\n",
        "        [1, 1, 1],\n",
        "        [2, 4, 8],\n",
        "        [3, 9, 27],\n",
        "        [4, 16, 64]\n",
        "    ]), columns=['n', 'n_squared', 'n_cubed']\n",
        ")\n",
        "df_1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_1.mean"
      ],
      "metadata": {
        "id": "4ug8jTVF5BWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGvY9hhA6JsR"
      },
      "source": [
        "## Creating a `DataFrame` object from the contents of a CSV File\n",
        "\n",
        "### Finding information on the file before reading it in\n",
        "Before attempting to read in a file, we can use the command line to see important information about the file that may determine how we read it in. We can run command line code from Jupyter Notebooks (thanks to IPython) by using `!` before the code.\n",
        "\n",
        "#### Number of lines (row count)\n",
        "For example, we can find out how many lines are in the file by using the `wc` utility (word count) and counting lines in the file (`-l`). The file has 9,333 lines:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Frw0Ag_b6JsS"
      },
      "source": [
        "!wc -l data/earthquakes.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ikPpftG6JsT"
      },
      "source": [
        "**Windows users**: if the above doesn't work for you (depends on your setup), then use this instead:\n",
        "\n",
        "```\n",
        "!find /c /v \"\" data\\earthquakes.csv\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "#### File size\n",
        "We can find the file size by using `ls` to list the files in the `data` directory, and passing in the flags `-lh` to include the file size in human readable format. Then we use `grep` to find the file in question. Note that `|` passes the result of `ls` to `grep`. The `grep` utility is used for finding items that match patterns.\n",
        "\n",
        "This tells us the file is 3.4 MB:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFiQIKAf6JsU"
      },
      "source": [
        "!ls -lh data | grep earthquakes.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEZSaIbu6JsV"
      },
      "source": [
        "**Windows users**: if the above doesn't work for you (depends on your setup), then use this instead:\n",
        "\n",
        "```\n",
        "!dir data | findstr \"earthquakes.csv\"\n",
        "```\n",
        "\n",
        "We can even capture the result of a command and use it in our Python code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHAGXY-n6JsW"
      },
      "source": [
        "files = !ls -lh data\n",
        "[file for file in files if 'earthquake' in file]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqzxSk3J6JsX"
      },
      "source": [
        "**Windows users**: if the above doesn't work for you (depends on your setup), then use this instead:\n",
        "\n",
        "```\n",
        "files = !dir data\n",
        "[file for file in files if 'earthquake' in file]\n",
        "```\n",
        "\n",
        "\n",
        "#### Examining a few rows\n",
        "We can use `head` to look at the top `n` rows of the file. With the `-n` flag, we can specify how many. This shows use that the first row of the file contains headers and that it is comma-separated (just because the file extension is `.csv` doesn't it contains comma-separated values):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWMzIXk06JsX"
      },
      "source": [
        "!head -n 2 data/earthquakes.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCd3TXPb6JsY"
      },
      "source": [
        "**Windows users**: if the above doesn't work for you (depends on your setup), then use this instead:\n",
        "\n",
        "```\n",
        "n = 2\n",
        "with open('data/earthquakes.csv', 'r') as file:\n",
        "    for _ in range(n):\n",
        "        print(file.readline())\n",
        "```\n",
        "\n",
        "\n",
        "Just like `head` gives rows from the top, `tail` gives rows from the bottom. This can help us check that there is no extraneous data on the bottom of the field, like perhaps some metadata about the fields that actually isn't part of the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJt1MzZn6JsY"
      },
      "source": [
        "!tail -n 1 data/earthquakes.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCrqXO_a6JsZ"
      },
      "source": [
        "#### Column count\n",
        "We can use `awk` to find the column count. This is a utility for pattern scanning and processing. The `-F` flag allows us to specify the delimiter (comma, in this case). Then we specify what to do for each record in the file. We choose to print `NF` which is a predefined variable whose value is the number of fields in the current record. Here, we say `exit` so that we print the number of fields (columns, here) in the first row of the file, then we stop. \n",
        "\n",
        "This tells us we have 26 data columns:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exQpM0bo6JsZ"
      },
      "source": [
        "!awk -F',' '{print NF; exit}' data/earthquakes.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qmz4uo3I6Jsa"
      },
      "source": [
        "**Windows users**: if the above or below don't work for you (depends on your setup), then use this instead:\n",
        "\n",
        "```\n",
        "import os\n",
        "\n",
        "with open('data/earthquakes.csv', 'rb') as file:\n",
        "    file.seek(-2, os.SEEK_END)\n",
        "    while file.read(1) != b'\\n':\n",
        "        file.seek(-2, os.SEEK_CUR)\n",
        "    print(file.readline().decode())\n",
        "```\n",
        "\n",
        "\n",
        "Since we know the 1st line of the file had headers, and the file is comma-separated, we can also count the columns by using `head` to get headers and parsing them in Python:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGhcBKck6Jsa"
      },
      "source": [
        "headers = !head -n 1 data/earthquakes.csv\n",
        "len(headers[0].split(','))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xelSepQL6Jsa"
      },
      "source": [
        "### Reading in the file\n",
        "Our file is small in size, has headers in the first row, and is comma-separated, so we don't need to provide any additional arguments to read in the file with `pd.read_csv()`, but be sure to check the [documentation](http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html) for possible arguments:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RN0XZUK86Jsb"
      },
      "source": [
        "df = pd.read_csv('data/earthquakes.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m77e9CpX7iIM"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WS3ScrN6Jsb"
      },
      "source": [
        "Note that we can also pass in a URL. Let's read this same file from GitHub:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lxL5pet6Jsb"
      },
      "source": [
        "df = pd.read_csv(\n",
        "    'https://github.com/stefmolin/'\n",
        "    'Hands-On-Data-Analysis-with-Pandas-2nd-edition'\n",
        "    '/blob/master/ch_02/data/earthquakes.csv?raw=True'\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JeiOUbr6Jsb"
      },
      "source": [
        "Pandas is usually very good at figuring out which options to use based on the input data, so we often won't need to add arguments to the call; however, there are many options available should we need them, some of which include the following:\n",
        "\n",
        "| Parameter | Purpose |\n",
        "| --- | --- |\n",
        "| `sep` | Specifies the delimiter |\n",
        "| `header` | Row number where the column names are located; the default option has `pandas` infer whether they are present |\n",
        "| `names` | List of column names to use as the header |\n",
        "| `index_col` | Column to use as the index |\n",
        "| `usecols` | Specifies which columns to read in |\n",
        "| `dtype` | Specifies data types for the columns | \n",
        "| `converters` | Specifies functions for converting data in certain columns |\n",
        "| `skiprows` | Rows to skip |\n",
        "| `nrows` | Number of rows to read at a time (combine with `skiprows` to read a file bit by bit) |\n",
        "| `parse_dates` | Automatically parse columns containing dates into datetime objects |\n",
        "| `chunksize` | For reading the file in chunks |\n",
        "| `compression` | For reading in compressed files without extracting beforehand |\n",
        "| `encoding` | Specifies the file encoding |\n",
        "\n",
        "## Writing a `DataFrame` Object to a CSV File\n",
        "Note that the index of `df` is just row numbers, so we don't want to keep it. Therefore, we pass `index=False` to the `to_csv()` method:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "id": "Kv09vBrXsdal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGj1teVh6Jsc"
      },
      "source": [
        "df.to_csv('output2.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38bqCjGD6Jsc"
      },
      "source": [
        "## Writing a `DataFrame` Object to a Database\n",
        "Note the `if_exists` parameter. By default, it will give you an error if you try to write a table that already exists. Here, we don't care if it is overwritten. Lastly, if we are interested in appending new rows, we set that to `'append'`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-E068q96Jsc"
      },
      "source": [
        "import sqlite3\n",
        "\n",
        "with sqlite3.connect('data/quakes.db') as connection:\n",
        "    pd.read_csv('data/tsunamis.csv').to_sql(\n",
        "        'tsunamis', connection, index=False, if_exists='replace'\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZZ5nUDa6Jsc"
      },
      "source": [
        "## Creating a `DataFrame` Object by Querying a Database\n",
        "Using a SQLite database. Otherwise you need to install [SQLAlchemy](https://www.sqlalchemy.org/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibGI5jSs6Jsc"
      },
      "source": [
        "import sqlite3\n",
        "\n",
        "with sqlite3.connect('data/quakes.db') as connection:\n",
        "    tsunamis = pd.read_sql('SELECT * FROM tsunamis', connection)\n",
        "\n",
        "tsunamis.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2-3. making dataframe from api**"
      ],
      "metadata": {
        "id": "3QXt2TZuhXlr"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgk_bLcryqc8"
      },
      "source": [
        "import datetime as dt\n",
        "import pandas as pd\n",
        "import requests\n",
        "\n",
        "yesterday = dt.date.today() - dt.timedelta(days=1)\n",
        "api = 'https://earthquake.usgs.gov/fdsnws/event/1/query'\n",
        "payload = {\n",
        "    'format': 'geojson',\n",
        "    'starttime': yesterday - dt.timedelta(days=30),\n",
        "    'endtime': yesterday\n",
        "}\n",
        "response = requests.get(api, params=payload)\n",
        "\n",
        "# let's make sure the request was OK\n",
        "response.status_code"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtJyBb5tyqc_"
      },
      "source": [
        "Response of 200 means OK, so we can pull the data out of the result. Since we asked the API for a JSON payload, we can extract it from the response with the `json()` method.\n",
        "\n",
        "### Isolate the Data from the JSON Response\n",
        "We need to check the structures of the response data to know where our data is."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klLab4Bqyqc_"
      },
      "source": [
        "earthquake_json = response.json()\n",
        "earthquake_json.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MoSHlatyqdA"
      },
      "source": [
        "The USGS API provides information about our request in the `metadata` key. Note that your result will be different, regardless of the date range you chose, because the API includes a timestamp for when the data was pulled:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhMXfJGfyqdA"
      },
      "source": [
        "earthquake_json['metadata']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CniLNQkiyqdB"
      },
      "source": [
        "Each element in the JSON array `features` is a row of data for our dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfO6UDdGyqdC"
      },
      "source": [
        "type(earthquake_json['features'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akZdfrwHyqdD"
      },
      "source": [
        "Your data will be different depending on the date you run this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wz5d_K7yqdD"
      },
      "source": [
        "earthquake_json['features']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sF1PzpguyqdE"
      },
      "source": [
        "### Convert to DataFrame\n",
        "We need to grab the `properties` section out of every entry in the `features` JSON array to create our dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhXanNKCz1WH"
      },
      "source": [
        "earthquake_properties_data = [\n",
        "    quake['properties'] for quake in earthquake_json['features']\n",
        "]\n",
        "earthquake_properties_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcEM69NryqdE"
      },
      "source": [
        "earthquake_properties_data = [\n",
        "    quake['properties'] for quake in earthquake_json['features']\n",
        "]\n",
        "df = pd.DataFrame(earthquake_properties_data)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvoPN5ymyqdE"
      },
      "source": [
        "### (Optional) Write Data to CSV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-poz6NFkyqdF"
      },
      "source": [
        "df.to_csv('earthquakes.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2-4. inspecting dataframe**"
      ],
      "metadata": {
        "id": "P4ohoe28hqWL"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4KXD_N_1B21"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('data/earthquakes.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1h0QghG1B22"
      },
      "source": [
        "## Examining dataframes\n",
        "### Is it empty?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgjvdzPl1B23"
      },
      "source": [
        "df.empty"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxgjrbG11B24"
      },
      "source": [
        "### What are the dimensions?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRE5tNpR1B24"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlnZ4uhS1B24"
      },
      "source": [
        "### What columns do we have?\n",
        "We know there are 26 columns, but what are they? Let's use the `columns` attribute to see:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-1uuYoy1B25"
      },
      "source": [
        "df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFgKgkAw1B25"
      },
      "source": [
        "### What does the data look like?\n",
        "View rows from the top with `head()`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2uN224x1B26"
      },
      "source": [
        "df.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StoluxE01B26"
      },
      "source": [
        "View rows from the bottom with `tail()`. Let's view 2 rows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXSdTBzR1B26"
      },
      "source": [
        "df.tail(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUDyF4VZ1B27"
      },
      "source": [
        "*Tip: we can modify the display options in order to see more columns:*\n",
        "\n",
        "```python\n",
        "# check the max columns setting\n",
        ">>> pd.get_option('display.max_columns')\n",
        "20\n",
        "\n",
        "# set the max columns to show when printing the dataframe to 26\n",
        ">>> pd.set_option('display.max_columns', 26)\n",
        "# OR\n",
        ">>> pd.options.display.max_columns = 26\n",
        "\n",
        "# reset the option\n",
        ">>> pd.reset_option('display.max_columns')\n",
        "\n",
        "# get information on all display settings\n",
        ">>> pd.describe_option('display')\n",
        "```\n",
        "\n",
        "*More information can be found in the documentation [here](https://pandas.pydata.org/pandas-docs/stable/user_guide/options.html).*\n",
        "\n",
        "### What data types do we have?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08kfPq2q1B27"
      },
      "source": [
        "df.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGGsxCJF1B27"
      },
      "source": [
        "### Getting extra info and finding nulls"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2VqwnwQ1B28"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ys1WmQ0_1B28"
      },
      "source": [
        "## Describing and Summarizing\n",
        "### Get summary statistics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXoANJO_1B28"
      },
      "source": [
        "df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dBEGZhI1B29"
      },
      "source": [
        "Specifying the 5<sup>th</sup> and 95<sup>th</sup> percentile:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wA20cTR51B29"
      },
      "source": [
        "df.describe(percentiles=[0.05, 0.95])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBkrlL5y1B2-"
      },
      "source": [
        "Describe specific data types:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSRe2ATb1B2-"
      },
      "source": [
        "df.describe(include=np.object)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1ZTIOor1B2-"
      },
      "source": [
        "Or describe all of them:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gww13kdl1B2-"
      },
      "source": [
        "df.describe(include='all')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcv5f9Q81B2_"
      },
      "source": [
        "This works on columns also:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JK0jQcHF1B2_"
      },
      "source": [
        "df.felt.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVz6w7Y21B2_"
      },
      "source": [
        "There are methods for specific statistics as well. Here is a sampling of them:\n",
        "\n",
        "| Method | Description | Data types |\n",
        "| --- | --- | --- |\n",
        "| `count()` | The number of non-null observations | Any |\n",
        "| `nunique()` | The number of unique values | Any |\n",
        "| `sum()` | The total of the values | Numerical or Boolean |\n",
        "| `mean()` | The average of the values | Numerical or Boolean |\n",
        "| `median()` | The median of the values | Numerical |\n",
        "| `min()` | The minimum of the values | Numerical |\n",
        "| `idxmin()` | The index where the minimum values occurs | Numerical |\n",
        "| `max()` | The maximum of the values | Numerical |\n",
        "| `idxmax()` | The index where the maximum value occurs | Numerical |\n",
        "| `abs()` | The absolute values of the data | Numerical |\n",
        "| `std()` | The standard deviation | Numerical |\n",
        "| `var()` | The variance |  Numerical |\n",
        "| `cov()` | The covariance between two `Series`, or a covariance matrix for all column combinations in a `DataFrame` | Numerical |\n",
        "| `corr()` | The correlation between two `Series`, or a correlation matrix for all column combinations in a `DataFrame` | Numerical |\n",
        "| `quantile()` | Calculates a specific quantile | Numerical |\n",
        "| `cumsum()` | The cumulative sum | Numerical or Boolean |\n",
        "| `cummin()` | The cumulative minimum | Numerical |\n",
        "| `cummax()` | The cumulative maximum | Numerical |\n",
        "\n",
        "For example, finding the unique values in the `alert` column:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhiaCWI71B3A"
      },
      "source": [
        "df.alert.unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qjbj2RnB1B3A"
      },
      "source": [
        "We can then use `value_counts()` to see how many of each unique value we have:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ps4PxaGX1B3A"
      },
      "source": [
        "df.mag.argmax()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fg83NAy1B3A"
      },
      "source": [
        "Note that `Index` objects also have several methods to help describe and summarize our data:\n",
        "\n",
        "| Method | Description |\n",
        "| --- | --- |\n",
        "| `argmax()`/`argmin()` | Find the location of the maximum/minimum value in the index |\n",
        "| `equals()` | Compare the index to another `Index` object for equality |\n",
        "| `isin()` | Check if the index values are in a list of values and return an array of Booleans |\n",
        "| `max()`/`min()` | Find the maximum/minimum value in the index |\n",
        "| `nunique()` | Get the number of unique values in the index |\n",
        "| `to_series()` | Create a `Series` object from the index |\n",
        "| `unique()` | Find the unique values of the index |\n",
        "| `value_counts()`| Create a frequency table for the unique values in the index |"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2-5. subsetting data**"
      ],
      "metadata": {
        "id": "M_d8HD2mh7Et"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PZfad9yh5TPS"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('data/earthquakes.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vg1kM8ik5TPV"
      },
      "source": [
        "## Selecting columns\n",
        "Grab an entire column using attribute notation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GTdDHJyx5TPW"
      },
      "outputs": [],
      "source": [
        "df.mag"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reG9hMW05TPY"
      },
      "source": [
        "Grab an entire column using dictionary syntax:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AMghcwtk5TPZ"
      },
      "outputs": [],
      "source": [
        "df['mag']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sI6MxiZ25TPb"
      },
      "source": [
        "Selecting multiple columns:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TbDwwrNQ5TPc"
      },
      "outputs": [],
      "source": [
        "df[['mag', 'title']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7IY_nrw5TPe"
      },
      "source": [
        "Selecting columns using list comprehensions and string operations:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[\n",
        "    ['title', 'time'\n",
        "     ,'mag','magType']\n",
        "]"
      ],
      "metadata": {
        "id": "f5e8YTZDaQrz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tiqbG4dS5TPg"
      },
      "outputs": [],
      "source": [
        "df[\n",
        "    ['title', 'time']\n",
        "    + [col for col in df.columns if col.startswith('mag')]\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXUOi_D-5TPh"
      },
      "source": [
        "Breaking down this example:\n",
        "1. the list comprehension"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i1f4SU4e5TPi"
      },
      "outputs": [],
      "source": [
        "[col for col in df.columns if col.startswith('mag')]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjvngO6_5TPj"
      },
      "source": [
        "2. assembling the list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SyqdutTy5TPj"
      },
      "outputs": [],
      "source": [
        "['title', 'time'] \\\n",
        "+ [col for col in df.columns if col.startswith('mag')]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWbcHnP65TPl"
      },
      "source": [
        "3. using this list as the list of columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z5LaVpLu5TPl"
      },
      "outputs": [],
      "source": [
        "df[\n",
        "    ['title', 'time']\n",
        "    + [col for col in df.columns if col.startswith('mag')]\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-M9W5SgE5TPm"
      },
      "source": [
        "## Slicing\n",
        "### Selecting rows\n",
        "Using row numbers (inclusive of first index, exclusive of last):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gyu2IRau5TPn"
      },
      "outputs": [],
      "source": [
        "df[100:103]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAk4b0k75TPn"
      },
      "source": [
        "### Selecting rows and columns with chaining"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tWYwMI4r5TPo"
      },
      "outputs": [],
      "source": [
        "df[['title', 'time']][100:103]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnmZRC935TPp"
      },
      "source": [
        "Order doesn't matter here:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nrcNTUgY5TPp"
      },
      "outputs": [],
      "source": [
        "df[100:103][['title', 'time']].equals(\n",
        "    df[['title', 'time']][100:103]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ls8AMEZW5TPq"
      },
      "source": [
        "So we know how to select rows and columns, but can we update values? Well, if we try using what we have learned so far, we will see the following warning:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8utGV46j6psE"
      },
      "outputs": [],
      "source": [
        "df[110:113]['title']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nfnq82Bu5TPq"
      },
      "outputs": [],
      "source": [
        "df[110:113]['title'] = df[110:113]['title'].str.lower()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7oazt_C5TPr"
      },
      "source": [
        "Note that it worked here, but `pandas` says we were setting a value on a copy of a slice and that we should use `loc` instead (topic of the following section):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A_3UiWGM5TPr"
      },
      "outputs": [],
      "source": [
        "df[110:113]['title']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Wq8mEnJ5TPs"
      },
      "source": [
        "## Indexing\n",
        "\n",
        "Now if we do this with `loc` as the warning suggests, everything goes smoothly. Note we have to lower the end index by one since `loc` is inclusive of endpoints:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45aO7bql5TPs"
      },
      "outputs": [],
      "source": [
        "df.loc[110:112, 'title'] = df.loc[110:112, 'title'].str.lower()\n",
        "df.loc[110:112, 'title']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94zzym_e5TPs"
      },
      "source": [
        "### Indexing with `loc`\n",
        "Selection of the format `loc[row_indexer, column_indexer]` where `:` can be used to select all:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YyAzIrs45TPt"
      },
      "outputs": [],
      "source": [
        "df.loc[:,'title']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBoeJXmP5TPt"
      },
      "source": [
        "We can use `loc` to select specific rows and columns without chaining. If we use row numbers with `loc`, they are now **inclusive** of the end index:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ROZVH67d5TPt"
      },
      "outputs": [],
      "source": [
        "df.loc[10:15, ['title', 'mag']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWFG483S5TPt"
      },
      "source": [
        "#### Indexing with `iloc`\n",
        "Exclusive of the endpoint just as Python slicing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xXl424N-5TPu"
      },
      "outputs": [],
      "source": [
        "df.iloc[10:15, [19, 8]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhrbSMnA5TPu"
      },
      "source": [
        "We can use slicing syntax with `iloc` for both rows and columns:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24dcM4205TPv"
      },
      "outputs": [],
      "source": [
        "df.iloc[10:15, 6:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwT-i9zY5TPv"
      },
      "source": [
        "When using `loc`, we can slice on column names. This will be inclusive of the endpoint because you can't be expected to know what the next column name will be. As such, we have multiple ways to achieve the same end goal:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w4Qhx4Sc5TPv"
      },
      "outputs": [],
      "source": [
        "df.iloc[10:15, 6:10].equals(\n",
        "    df.loc[10:14, 'gap':'magType']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvSSV-rd5TPw"
      },
      "source": [
        "### Looking up scalar values\n",
        "We used `loc` and `iloc` to grab subsets of the dataframe. However, if we are just interested in the specific value at a given `[row, column]`, then we can use `iat` and `at`. We use `at` with labels:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PYRFcJEO5TPw"
      },
      "outputs": [],
      "source": [
        "df.at[10, 'mag']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tTIYAyg5TPw"
      },
      "source": [
        "...and `iat` with integer indices:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZrgD9F835TPx"
      },
      "outputs": [],
      "source": [
        "df.iat[10, 8]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnUuM0D35TPx"
      },
      "source": [
        "## Filtering\n",
        "We can filter our dataframes using a **Boolean mask**, which can be made as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yCar-u1X5TPx"
      },
      "outputs": [],
      "source": [
        "df.mag > 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoQTsN3V5TPx"
      },
      "source": [
        "To use a mask for selection, we simply place it inside the brackets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCDKSBoG5TPx"
      },
      "outputs": [],
      "source": [
        "df[df.mag >= 7.0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZJgf2dO5TPy"
      },
      "source": [
        "We can use masks with `loc`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ue5oGYhL5TPy"
      },
      "outputs": [],
      "source": [
        "df.loc[\n",
        "    df.mag >= 7.0,\n",
        "    ['alert', 'mag', 'magType', 'title', 'tsunami', 'type']\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1-Vq9c65TPy"
      },
      "source": [
        "Masks can be created using multiple criteria when combined with bitwise operators `&` for AND and `|` for OR. We must also surround each criterion with parentheses. We can't use `and`/`or` here because we need to evaluate row by row:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QxOcSC865TPy"
      },
      "outputs": [],
      "source": [
        "df.loc[\n",
        "    (df.tsunami == 1) & (df.alert == 'red'),\n",
        "    ['alert', 'mag', 'magType', 'title', 'tsunami', 'type']\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiWrAK605TPy"
      },
      "source": [
        "An example with an OR condition, which is less restrictive:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qd4iNm0t5TPz"
      },
      "outputs": [],
      "source": [
        "df.loc[\n",
        "    (df.tsunami == 1) | (df.alert == 'red'),\n",
        "    ['alert', 'mag', 'magType', 'title', 'tsunami', 'type']\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZscuWmK5TPz"
      },
      "source": [
        "Masks can be created from any criteria that results in a Boolean. For example, we can select all earthquakes with the string `Alaska` in the `place` column with a non-null value for the `alert` column. To get non-nulls, we can use the `isnull()` method with the bitwise negation operator (`~`) or the `notnull()` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Puc8eb3T5TPz"
      },
      "outputs": [],
      "source": [
        "df.loc[\n",
        "    (df.place.str.contains('Alaska')) & (df.alert.notnull()),\n",
        "    ['alert', 'mag', 'magType', 'title', 'tsunami', 'type', 'place']\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yr7qmjKz5TPz"
      },
      "source": [
        "We can even use regular expressions here:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "syJmlJRF5TP0"
      },
      "outputs": [],
      "source": [
        "df.loc[\n",
        "    (df.place.str.contains('CA')) & (df.mag > 3.8),\n",
        "    ['alert', 'mag', 'magType', 'title', 'tsunami', 'type','place']\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMbiMDhO5TP0"
      },
      "source": [
        "We can use the `between()` method to turn 2 individual checks (is less than or equal to some maximum value and is greater than or equal to some minimum value) into a single one. Note this is inclusive of the endpoint by default:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nke20EFZ5TP0"
      },
      "outputs": [],
      "source": [
        "df.loc[\n",
        "    df.mag.between(6.5, 7.5),\n",
        "    ['alert', 'mag', 'magType', 'title', 'tsunami', 'type']\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-IBRtOq5TP0"
      },
      "source": [
        "We can use the `isin()` method to check for membership in a list of values:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01BTCoHd5TP1"
      },
      "outputs": [],
      "source": [
        "df.loc[\n",
        "    df.magType.isin(['mw', 'mwb']),\n",
        "    ['alert', 'mag', 'magType', 'title', 'tsunami', 'type']\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2v3-vgAD5TP1"
      },
      "source": [
        "We can grab the index of the minimum and maximum values of a given column and use those to select the entire row where they occur:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y79-iXJi5TP1"
      },
      "outputs": [],
      "source": [
        "[df.mag.idxmin(), df.mag.idxmax()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VP95HhAj5TP1"
      },
      "outputs": [],
      "source": [
        "df.loc[\n",
        "    [df.mag.idxmin(), df.mag.idxmax()],\n",
        "    ['alert', 'mag', 'magType', 'title', 'tsunami', 'type']\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u06tNO7f5TP1"
      },
      "source": [
        "Note that there is a `filter()` method, but it doesn't filter the data in the same sense as we discussed in this section. Here are a few things you can do with this method.\n",
        "\n",
        "- grab columns of a dataframe by passing a list to `items`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Evmwplr5TP1"
      },
      "outputs": [],
      "source": [
        "df.filter(items=['mag', 'magType']).head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCs2eFSN5TP5"
      },
      "source": [
        "- grab all the columns that contain a string with the `like` parameter:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_3V9ltH05TP5"
      },
      "outputs": [],
      "source": [
        "df.filter(like='mag').head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nrjmHCd5TP5"
      },
      "source": [
        "- use regular expressions; here, we select any columns that start with `t`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EUYR0OX_5TP5"
      },
      "outputs": [],
      "source": [
        "df.filter(regex='^t').head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxHvu4s45TP5"
      },
      "source": [
        "- use `filter()` along the rows, by passing in `axis=0`. Here, we will use the `place` column as the index (we will cover `set_index()` in chapter 3):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7ozYkTz5TP5"
      },
      "outputs": [],
      "source": [
        "df.set_index('place').filter(like='Japan',axis=0).filter(items=['mag', 'magType', 'title']).head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wm6uYTWQ5TP6"
      },
      "source": [
        "This also works on `Series` objects and will run on the index:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n5vWCBLY5TP6"
      },
      "outputs": [],
      "source": [
        "df.set_index('place').title.filter(like='Japan').head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2-6.adding and removing data**"
      ],
      "metadata": {
        "id": "X985IwuHiINk"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jyMr3gABmcd"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\n",
        "    'data/earthquakes.csv', \n",
        "    usecols=['time', 'title', 'place', 'magType', 'mag', 'alert', 'tsunami']\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5O1ss3BBmce"
      },
      "source": [
        "## Creating new data\n",
        "### Adding new columns\n",
        "New columns get added to the right of the original columns and can be a single value, which will be **broadcast** along the rows of the dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nV2_WL8Bmce"
      },
      "source": [
        "df['source'] = 'USGS API'\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snBUA2wWBmcf"
      },
      "source": [
        "...or a Boolean mask:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JeoQMOFYBmcg"
      },
      "source": [
        "df['mag_negative'] = df.mag < 0\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxgUDgonBmcg"
      },
      "source": [
        "#### Adding the `parsed_place` column\n",
        "We have an entity recognition problem on our hands with the `place` column. There are several entities that have multiple names in the data (e.g., CA and California, NV and Nevada)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kThxKu7bCHvv"
      },
      "source": [
        "df.place"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_R28sB-hCOGs"
      },
      "source": [
        "df.place.str.extract(', (.*$)')[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DOpxIQBBmcg"
      },
      "source": [
        "df.place.str.extract(', (.*$)')[0].sort_values().unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JE5atQ1hBmcg"
      },
      "source": [
        "Replace parts of the `place` names to fit our needs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzziF2yvBmch"
      },
      "source": [
        "df['parsed_place'] = df.place.str.replace(\n",
        "    r'.* of ', '', regex=True # remove anything saying <something> of <something>\n",
        ").str.replace(\n",
        "    'the ', '' # remove \"the \"\n",
        ").str.replace(\n",
        "    r'CA$', 'California', regex=True # fix California\n",
        ").str.replace(\n",
        "    r'NV$', 'Nevada', regex=True # fix Nevada\n",
        ").str.replace(\n",
        "    r'MX$', 'Mexico', regex=True # fix Mexico\n",
        ").str.replace(\n",
        "    r' region$', '', regex=True # chop off endings with \" region\"\n",
        ").str.replace(\n",
        "    'northern ', '' # remove \"northern \"\n",
        ").str.replace(\n",
        "    'Fiji Islands', 'Fiji' # line up the Fiji places\n",
        ").str.replace(\n",
        "    r'^.*, ', '', regex=True # remove anything else extraneous from the beginning\n",
        ").str.strip() # remove any extra spaces"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yI4_iqTBmch"
      },
      "source": [
        "Now we can use a single name to get all earthquakes for that place (although this still isn't perfect):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jg1h-gopBmch"
      },
      "source": [
        "df.parsed_place.sort_values().unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JE2kGXIOBmci"
      },
      "source": [
        "#### Using the `assign()` method to create columns\n",
        "To create many columns at once or update existing columns, we can use `assign()`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMTG86tTBmci"
      },
      "source": [
        "df.assign(\n",
        "    in_ca=df.parsed_place.str.endswith('California'),\n",
        "    in_alaska=df.parsed_place.str.endswith('Alaska')\n",
        ").sample(5, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3O9FtjiBmci"
      },
      "source": [
        "With the use of `lambda` functions, the `assign()` method becomes even more powerful. **Lambda functions** are anonymous functions usually defined in one line and for single use. The `assign()` method passes the entire dataframe into the `lambda` function as `x`; from there, we can select the columns `in_ca` and `in_alaska`, which are being created in that same call to `assign()`. Here, we use a `lambda` function to create a new column, `neither`, which tells if the earthquake was neither in Alaska nor California:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEzj8c_NBmcj"
      },
      "source": [
        "df.assign(\n",
        "    in_ca=df.parsed_place == 'California',\n",
        "    in_alaska=df.parsed_place == 'Alaska',\n",
        "    neither=lambda x: ~x.in_ca & ~x.in_alaska\n",
        ").sample(5, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipczzEFwBmck"
      },
      "source": [
        "#### Concatenation\n",
        "Say we were working with two separate dataframes, one with earthquakes accompanied by tsunamis and the other with earthquakes without tsunamis. If we wanted to look at earthquakes as a whole, we would want to concatenate the dataframes into a single one:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKtHhW1ABmck"
      },
      "source": [
        "tsunami = df[df.tsunami == 1]\n",
        "no_tsunami = df[df.tsunami == 0]\n",
        "\n",
        "tsunami.shape, no_tsunami.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpqAfOnsBmcl"
      },
      "source": [
        "Concatenating along the row axis (`axis=0`) is equivalent to appending to the bottom. By concatenating our earthquakes with tsunamis and those without tsunamis, we get the full earthquake data set back:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ObR-0jbBmcl"
      },
      "source": [
        "pd.concat([tsunami, no_tsunami])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByslVhXOBmcm"
      },
      "source": [
        "Note that the previous result is equivalent to running the `append()` method of the dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kON0fPAGBmcm"
      },
      "source": [
        "tsunami.append(no_tsunami).shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xlg52v0aBmcm"
      },
      "source": [
        "We have been working with a subset of the columns from the CSV file, but suppose that now we want to get some of the columns we ignored when we read in the data. Since we have added new columns in this notebook, we won't want to read in the file and perform those operations again. Instead, we will concatenate along the columns (`axis=1`) to add back what we are missing:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "additional_columns"
      ],
      "metadata": {
        "id": "ZkJhH_h0mRA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4T0G1L-KBmcn"
      },
      "source": [
        "additional_columns = pd.read_csv(\n",
        "    'data/earthquakes.csv', usecols=['tz', 'felt', 'ids']\n",
        ")\n",
        "pd.concat([df.head(2), additional_columns.head(2)], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tL9rSGXBmcn"
      },
      "source": [
        "Notice what happens if the index doesn't align though:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "additional_columns"
      ],
      "metadata": {
        "id": "7mWVy_w6mx9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYe75djxBmco"
      },
      "source": [
        "additional_columns = pd.read_csv(\n",
        "    'data/earthquakes.csv', usecols=['tz', 'felt', 'ids', 'time'], index_col='time'\n",
        ")\n",
        "pd.concat([df.head(2), additional_columns.head(2)], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvLGifxcBmco"
      },
      "source": [
        "If the index doesn't align, we can align it before attempting the concatentation, which we will discuss in chapter 3.\n",
        "\n",
        "Say we want to join the `tsunami` and `no_tsunami` dataframes, but the `no_tsunami` dataframe has an additional column. The `join` parameter specifies how to handle any overlap in column names (when appending to the bottom) or in row names (when concatenating to the left/right). By default, this is `outer`, so we keep everything; however, if we use `inner`, we will only keep what is in common:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "no_tsunami.columns"
      ],
      "metadata": {
        "id": "ajcI0U-jnPZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tsunami.columns"
      ],
      "metadata": {
        "id": "r66FjHXfsiz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pHbzLuyBmco"
      },
      "source": [
        "pd.concat(\n",
        "    [tsunami.head(2), no_tsunami.head(2).assign(type='earthquake')], join='outer'\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5-bpJN4Bmcp"
      },
      "source": [
        "In addition, we use `ignore_index`, since the index doesn't mean anything for us here. This gives us sequential values instead of what we had in the previous result:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uaZW7Mb8Bmcp"
      },
      "source": [
        "pd.concat(\n",
        "    [tsunami.head(2), no_tsunami.head(2).assign(type='earthquake')], join='inner', ignore_index=True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8rhfAPyBmcq"
      },
      "source": [
        "## Deleting Unwanted Data\n",
        "Columns can be deleted using dictionary syntax with `del`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMOs2w-SEcdO"
      },
      "source": [
        "df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyvE4upABmcq"
      },
      "source": [
        "del df['source']\n",
        "df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vp4Lrn-nBmcq"
      },
      "source": [
        "If we don't know whether the column exists, we should use a `try`/`except` block:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHBJNDjEBmcq"
      },
      "source": [
        "try:\n",
        "    del df['source']\n",
        "except KeyError:\n",
        "    # handle the error here\n",
        "    print('not there anymore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrr55qNyBmcr"
      },
      "source": [
        "We can also use `pop()`. This will allow us to use the series we remove later. Note there will be an error if the key doesn't exist, so we can also use a `try`/`except` here:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEXzjeSVBmcr"
      },
      "source": [
        "mag_negative = df.pop('mag_negative')\n",
        "df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    del df['mag_negative']\n",
        "except KeyError:\n",
        "    # handle the error here\n",
        "    print('not there anymore')"
      ],
      "metadata": {
        "id": "r73dnYFLo15t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zXpqA4cBmcr"
      },
      "source": [
        "Notice we have a mask in `mag_negative` now:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUTnotwUBmcs"
      },
      "source": [
        "mag_negative.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HosKJZVIBmcs"
      },
      "source": [
        "Now, we can use `mag_negative` to filter our data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDc19uKxBmcs"
      },
      "source": [
        "df[mag_negative].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsRMQlpaBmcs"
      },
      "source": [
        "### Using the `drop()` method\n",
        "We can drop rows by passing a list of indices to the `drop()` method. Notice in the following example that when asking for the first 2 rows with `head()` we get the 3rd and 4th rows because we dropped the original first 2 with `drop([0, 1])`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVgBbAuGBmct"
      },
      "source": [
        "df.drop([0, 1]).head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSvVIMTmBmct"
      },
      "source": [
        "The `drop()` method drops along the row axis by default. If we pass in a list of columns with the `columns` argument, we can delete columns:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2DbQghcE7Na"
      },
      "source": [
        "cols_to_drop = [\n",
        "    col for col in df.columns\n",
        "    if col not in ['alert', 'mag', 'title', 'time', 'tsunami']\n",
        "]\n",
        "cols_to_drop"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ss4h3KQHBmct"
      },
      "source": [
        "cols_to_drop = [\n",
        "    col for col in df.columns\n",
        "    if col not in ['alert', 'mag', 'title', 'time', 'tsunami']\n",
        "]\n",
        "df.drop(columns=cols_to_drop).head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4E3BIb0jBmct"
      },
      "source": [
        "We also have the option of using `axis=1`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjtPXyF4Bmct"
      },
      "source": [
        "df.drop(columns=cols_to_drop).equals(\n",
        "    df.drop(cols_to_drop, axis=1)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozxkWJ3MBmcu"
      },
      "source": [
        "By default, `drop()`, along with the majority of `DataFrame` methods, will return a new `DataFrame` object. If we just want to change the one we are working with, we can pass `inplace=True`. This should be used with care:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G67S71LSBmcu"
      },
      "source": [
        "df.drop(columns=cols_to_drop, inplace=True)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3-1. wide and long**"
      ],
      "metadata": {
        "id": "y8uE9L8viXhl"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5h6rMhqIquNF"
      },
      "source": [
        "cd /content/drive/MyDrive/Hands-On-Data-Analysis-with-Pandas-2nd-edition/ch_03/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTFMGpDkqhzv"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "wide_df = pd.read_csv('data/wide_data.csv', parse_dates=['date'])\n",
        "long_df = pd.read_csv(\n",
        "    'data/long_data.csv', \n",
        "    usecols=['date', 'datatype', 'value'], \n",
        "    parse_dates=['date']\n",
        ")[['date', 'datatype', 'value']] # sort columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4Ti4co_qhzw"
      },
      "source": [
        "## Wide format\n",
        "Our variables each have their own column:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17WPjPJfqhzx"
      },
      "source": [
        "wide_df.head(6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXRZUCePqhzy"
      },
      "source": [
        "Describing all the columns is easy:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZmpZu-zrqhzy"
      },
      "source": [
        "wide_df.describe(include='all', datetime_is_numeric=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDhicewxqhzy"
      },
      "source": [
        "It's easy to graph with `pandas` (covered in chapter 5):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8xlw_7qqhzz"
      },
      "source": [
        "wide_df.plot(\n",
        "    x='date', y=['TMAX', 'TMIN', 'TOBS'], figsize=(15, 5), \n",
        "    title='Temperature in NYC in October 2018'\n",
        ").set_ylabel('Temperature in Celsius')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nG8cgtATqhzz"
      },
      "source": [
        "## Long format\n",
        "Our variable names are now in the `datatype` column and their values are in the `value` column. We now have 3 rows for each date, since we have 3 different `datatypes`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vpt71PUuqhzz"
      },
      "source": [
        "long_df.head(6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DHgYOPXqhz0"
      },
      "source": [
        "Since we have many rows for the same date, using `describe()` is not that helpful:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ByMkNyutqhz0"
      },
      "source": [
        "long_df.describe(include='all', datetime_is_numeric=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfBuzaA3q9xt"
      },
      "source": [
        "grouped_df = long_df.groupby('datatype')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4st5mFJrK5O"
      },
      "source": [
        "grouped_df.count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgTvw6FJrT2l"
      },
      "source": [
        "grouped_df.mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtJJ94Nirld1"
      },
      "source": [
        "grouped_df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJ9Hflz8rrL-"
      },
      "source": [
        "grouped_df.describe().T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88TmL30pqhz0"
      },
      "source": [
        "Plotting long format data in `pandas` can be rather tricky. Instead we use `seaborn` (covered in [`ch_06/1-introduction_to_seaborn.ipynb`](../ch_06/1-introduction_to_seaborn.ipynb)):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tipp3zLWqhz1"
      },
      "source": [
        "import seaborn as sns\n",
        "\n",
        "sns.set(rc={'figure.figsize': (15, 5)}, style='white')\n",
        "\n",
        "ax = sns.lineplot(\n",
        "    data=long_df, x='date', y='value', hue='datatype'\n",
        ")\n",
        "ax.set_ylabel('Temperature in Celsius')\n",
        "ax.set_title('Temperature in NYC in October 2018')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdG0S6Wkqhz1"
      },
      "source": [
        "With long data and `seaborn`, we can easily facet our plots:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5z__2Ovqhz1"
      },
      "source": [
        "sns.set(\n",
        "    rc={'figure.figsize': (20, 10)}, style='white', font_scale=2\n",
        ")\n",
        "\n",
        "g = sns.FacetGrid(long_df, col='datatype', height=10)\n",
        "g = g.map(plt.plot, 'date', 'value')\n",
        "g.set_titles(size=25)\n",
        "g.set_xticklabels(rotation=45)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3-2. using the weather api**"
      ],
      "metadata": {
        "id": "-Yak7akUilve"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QfT2nInBs2UH"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "def make_request(endpoint, payload=None):\n",
        "    \"\"\"\n",
        "    Make a request to a specific endpoint on the weather API\n",
        "    passing headers and optional payload.\n",
        "    \n",
        "    Parameters:\n",
        "        - endpoint: The endpoint of the API you want to \n",
        "                    make a GET request to.\n",
        "        - payload: A dictionary of data to pass along \n",
        "                   with the request.\n",
        "    \n",
        "    Returns:\n",
        "        A response object.\n",
        "    \"\"\"\n",
        "    return requests.get(\n",
        "        f'https://www.ncdc.noaa.gov/cdo-web/api/v2/{endpoint}',\n",
        "        headers={\n",
        "            'token': 'PASTE_TOKEN_HERE'\n",
        "        },\n",
        "        params=payload\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGLtuxIFs2UL"
      },
      "source": [
        "**Note: the API limits us to 5 requests per second and 10,000 requests per day.**\n",
        "\n",
        "## See which datasets are available\n",
        "We can make requests to the `datasets` endpoint to see which datasets are available. We also pass in a dictionary for the payload to get datasets that have data after the start date of October 1, 2018."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J9rXpxWPs2UM"
      },
      "outputs": [],
      "source": [
        "response = make_request('datasets', {'startdate': '2018-10-01'})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJj6xrfEs2UO"
      },
      "source": [
        "Status code of `200` means everything is OK. More codes can be found [here](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ODknIElis2UP"
      },
      "outputs": [],
      "source": [
        "response.status_code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sj_TirK0s2UR"
      },
      "source": [
        "Alternatively, we can check the `ok` attribute:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xcK6GxXPs2UT"
      },
      "outputs": [],
      "source": [
        "response.ok"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nP7YJ7KKs2UU"
      },
      "source": [
        "### Get the keys of the result\n",
        "The result is a JSON payload, which we can access with the `json()` method of our response object. JSON objects can be treated like dictionaries, so we can access the keys just like we would a dictionary:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f72_6XWLs2UV"
      },
      "outputs": [],
      "source": [
        "payload = response.json()\n",
        "payload.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "taWtQ9Uts2UW"
      },
      "source": [
        "The metadata of the response will tell us information about the request and data we got back:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1WJk_RGs2UX"
      },
      "outputs": [],
      "source": [
        "payload['metadata']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riEMpa49s2UY"
      },
      "source": [
        "### Figure out what data is in the result\n",
        "The `results` key contains the data we requested. This is a list of what would be rows in our dataframe. Each entry in the list is a dictionary, so we can look at the keys to get the fields:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2QSnkMS1s2Ua"
      },
      "outputs": [],
      "source": [
        "payload['results'][0].keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVHHqtYYs2Ub"
      },
      "source": [
        "### Parse the result\n",
        "We don't want all those fields, so we will use a list comphrension to take only the `id` and `name` fields out:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W1IUSa0Ss2Uc"
      },
      "outputs": [],
      "source": [
        "[(data['id'], data['name']) for data in payload['results']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHC_WzdAs2Uc"
      },
      "source": [
        "## Figure out which data category we want\n",
        "The `GHCND` data containing daily summaries is what we want. Now we need to make another request to figure out which data categories we want to collect. This is the `datacategories` endpoint. We have to pass the `datasetid` for `GHCND` as the payload so the API knows which dataset we are asking about:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iVLnKMRys2Ud"
      },
      "outputs": [],
      "source": [
        "# get data category id\n",
        "response = make_request(\n",
        "    'datacategories', payload={'datasetid': 'GHCND'}\n",
        ")\n",
        "response.status_code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIBUu_dDs2Ue"
      },
      "source": [
        "Since we know the API gives us a `metadata` and a `results` key in each response, we can see what is in the `results` portion of the JSON payload:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XgbWpT1Ws2Uf"
      },
      "outputs": [],
      "source": [
        "response.json()['results']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfFs_Opss2Uf"
      },
      "source": [
        "## Grab the data type ID for the temperature category\n",
        "We will be working with temperatures, so we want the `TEMP` data category. Now, we need to find the `datatypes` to collect. For this, we use the `datatypes` endpoint and provide the `datacategoryid` which was `TEMP`. We also specify a limit for the number of `datatypes` to return with the payload. If there are more than this we can make another request later, but for now, we just want to pick a few out:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T9Hf0OWws2Ug"
      },
      "outputs": [],
      "source": [
        "# get data type id\n",
        "response = make_request(\n",
        "    'datatypes',\n",
        "    payload={\n",
        "        'datacategoryid': 'TEMP', \n",
        "        'limit': 100\n",
        "    }\n",
        ")\n",
        "response.status_code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxmcXwY_s2Uh"
      },
      "source": [
        "We can grab the `id` and `name` fields for each of the entries in the `results` portion of the data. The fields we are interested in are at the bottom:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xJA4yYk7s2Ui"
      },
      "outputs": [],
      "source": [
        "[(datatype['id'], datatype['name']) for datatype in response.json()['results']][-5:] # look at the last 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVJj4yNcs2Uj"
      },
      "source": [
        "## Determine which location category we want\n",
        "Now that we know which `datatypes` we will be collecting, we need to find the location to use. First, we need to figure out the location category. This is obtained from the `locationcategories` endpoint by passing the `datasetid`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_QbtxvXNs2Uj"
      },
      "outputs": [],
      "source": [
        "# get location category id \n",
        "response = make_request(\n",
        "    'locationcategories', \n",
        "    payload={'datasetid': 'GHCND'}\n",
        ")\n",
        "response.status_code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xn4AZWT3s2Uk"
      },
      "source": [
        "We can use `pprint` to print dictionaries in an easier-to-read format. After doing so, we can see there are 12 different location categories, but we are only interested in `CITY`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16P2gikus2Uk"
      },
      "outputs": [],
      "source": [
        "import pprint\n",
        "pprint.pprint(response.json())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxrKFcXqs2Ul"
      },
      "source": [
        "## Get NYC Location ID\n",
        "In order to find the location ID for New York, we need to search through all the cities available. Since we can ask the API to return the cities sorted, we can use binary search to find New York quickly without having to make many requests or request lots of data at once. The following function makes the first request to see how big the list is and looks at the first value. From there it decides if it needs to move towards the beginning or end of the list by comparing the item we are looking for to others alphabetically. Each time it makes a request it can rule out half of the remaining data to search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6-Avw02s2Un"
      },
      "outputs": [],
      "source": [
        "def get_item(name, what, endpoint, start=1, end=None):\n",
        "    \"\"\"\n",
        "    Grab the JSON payload for a given field by name using binary search.\n",
        "\n",
        "    Parameters:\n",
        "        - name: The item to look for.\n",
        "        - what: Dictionary specifying what the item in `name` is.\n",
        "        - endpoint: Where to look for the item.\n",
        "        - start: The position to start at. We don't need to touch this, but the\n",
        "                 function will manipulate this with recursion.\n",
        "        - end: The last position of the items. Used to find the midpoint, but\n",
        "               like `start` this is not something we need to worry about.\n",
        "\n",
        "    Returns:\n",
        "        Dictionary of the information for the item if found otherwise \n",
        "        an empty dictionary.\n",
        "    \"\"\"\n",
        "    # find the midpoint which we use to cut the data in half each time\n",
        "    mid = (start + (end or 1)) // 2\n",
        "    \n",
        "    # lowercase the name so this is not case-sensitive\n",
        "    name = name.lower()\n",
        "    \n",
        "    # define the payload we will send with each request\n",
        "    payload = {\n",
        "        'datasetid': 'GHCND',\n",
        "        'sortfield': 'name',\n",
        "        'offset': mid, # we will change the offset each time\n",
        "        'limit': 1 # we only want one value back\n",
        "    }\n",
        "    \n",
        "    # make our request adding any additional filter parameters from `what`\n",
        "    response = make_request(endpoint, {**payload, **what})\n",
        "    \n",
        "    if response.ok:\n",
        "        payload = response.json()\n",
        "\n",
        "        # if response is ok, grab the end index from the response metadata the first time through\n",
        "        end = end or payload['metadata']['resultset']['count']\n",
        "        \n",
        "        # grab the lowercase version of the current name\n",
        "        current_name = payload['results'][0]['name'].lower()\n",
        "        \n",
        "        # if what we are searching for is in the current name, we have found our item\n",
        "        if name in current_name:\n",
        "            return payload['results'][0] # return the found item\n",
        "        else:\n",
        "            if start >= end: \n",
        "                # if our start index is greater than or equal to our end, we couldn't find it\n",
        "                return {}\n",
        "            elif name < current_name:\n",
        "                # our name comes before the current name in the alphabet, so we search further to the left\n",
        "                return get_item(name, what, endpoint, start, mid - 1)\n",
        "            elif name > current_name:\n",
        "                # our name comes after the current name in the alphabet, so we search further to the right\n",
        "                return get_item(name, what, endpoint, mid + 1, end)    \n",
        "    else:\n",
        "        # response wasn't ok, use code to determine why\n",
        "        print(f'Response not OK, status: {response.status_code}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMI4mfUts2Uo"
      },
      "source": [
        "When we use binary search to find New York, we find it in just 8 requests despite it being close to the middle of 1,983 entries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FdMDs5DAs2Up"
      },
      "outputs": [],
      "source": [
        "# get NYC id \n",
        "nyc = get_item('New York', {'locationcategoryid': 'CITY'}, 'locations')\n",
        "nyc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsBSS17Ns2Uq"
      },
      "source": [
        "## Get the station ID for Central Park\n",
        "The most granular data is found at the station level:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LHO0m9Hds2Uq"
      },
      "outputs": [],
      "source": [
        "central_park = get_item('NY City Central Park', {'locationid': nyc['id']}, 'stations')\n",
        "central_park"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0RmKJ938s2Ur"
      },
      "source": [
        "## Request the temperature data\n",
        "Finally, we have everything we need to make our request for the New York temperature data. For this, we use the `data` endpoint and provide all the parameters we picked up throughout our exploration of the API:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Al3JUHEFs2Us"
      },
      "outputs": [],
      "source": [
        "# get NYC daily summaries data \n",
        "response = make_request(\n",
        "    'data', \n",
        "    {\n",
        "        'datasetid': 'GHCND',\n",
        "        'stationid': central_park['id'],\n",
        "        'locationid': nyc['id'],\n",
        "        'startdate': '2018-10-01',\n",
        "        'enddate': '2018-10-31',\n",
        "        'datatypeid': ['TAVG', 'TMAX', 'TMIN'], # average, max, and min temperature\n",
        "        'units': 'metric',\n",
        "        'limit': 1000\n",
        "    }\n",
        ")\n",
        "response.status_code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R29vjLqNs2Ut"
      },
      "source": [
        "## Create a DataFrame\n",
        "The Central Park station only has the daily minimum and maximum temperatures."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZdLhFqzYs2Ut"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(response.json()['results'])\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqjK5taOs2Uu"
      },
      "source": [
        "We didn't get `TAVG` because the station doesn't measure that:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FgcW1uIks2Uv"
      },
      "outputs": [],
      "source": [
        "df.datatype.unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gh-A9VQRs2Uv"
      },
      "source": [
        "Despite showing up in the data as measuring it... Real-world data is dirty!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S561PNRcs2Uw"
      },
      "outputs": [],
      "source": [
        "if get_item(\n",
        "    'NY City Central Park', {'locationid': nyc['id'], 'datatypeid': 'TAVG'}, 'stations'\n",
        "):\n",
        "    print('Found!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tneocsis2Uw"
      },
      "source": [
        "## Using a different station\n",
        "Let's use LaGuardia airport instead. It contains `TAVG` (average daily temperature):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1TNNAun4s2Uw"
      },
      "outputs": [],
      "source": [
        "laguardia = get_item(\n",
        "    'LaGuardia', {'locationid': nyc['id']}, 'stations'\n",
        ")\n",
        "laguardia"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgSTqCqgs2Ux"
      },
      "source": [
        "We make our request using the LaGuardia airport station this time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ssl2WcPVs2Ux"
      },
      "outputs": [],
      "source": [
        "# get NYC daily summaries data \n",
        "response = make_request(\n",
        "    'data', \n",
        "    {\n",
        "        'datasetid': 'GHCND',\n",
        "        'stationid': laguardia['id'],\n",
        "        'locationid': nyc['id'],\n",
        "        'startdate': '2018-10-01',\n",
        "        'enddate': '2018-10-31',\n",
        "        'datatypeid': ['TAVG', 'TMAX', 'TMIN'], # temperature at time of observation, min, and max\n",
        "        'units': 'metric',\n",
        "        'limit': 1000\n",
        "    }\n",
        ")\n",
        "response.status_code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHul990rs2Uz"
      },
      "source": [
        "The request was successful, so let's make a dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xaua4Npms2U0"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(response.json()['results'])\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBajmhE1s2U0"
      },
      "source": [
        "We should check that we got what we wanted: 31 entries for TAVG, TMAX, and TMIN (1 per day):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AkWxkigcs2U1"
      },
      "outputs": [],
      "source": [
        "df.datatype.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaRcUMq0s2U1"
      },
      "source": [
        "Write the data to a CSV file for use in other notebooks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZlD8uT7Ds2U1"
      },
      "outputs": [],
      "source": [
        "df.to_csv('data/nyc_temperatures.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3-3. cleaning data**"
      ],
      "metadata": {
        "id": "XBLnB7DRkbvn"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AWOpFO7jI6h"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('data/nyc_temperatures.csv')\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5qxjE4FjI6i"
      },
      "source": [
        "## Renaming Columns\n",
        "We start out with the following columns:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mW-M2VRijI6j"
      },
      "source": [
        "df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AVsx5rWjI6j"
      },
      "source": [
        "We want to rename the `value` column to indicate it contains the temperature in Celsius and the `attributes` column to say `flags` since each value in the comma-delimited string is a different flag about the data collection. For this task, we use the `rename()` method and pass in a dictionary mapping the column names to their new names. We pass `inplace=True` to change our original dataframe instead of getting a new one back:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMLQDVZJjI6j"
      },
      "source": [
        "df.rename(\n",
        "    columns={\n",
        "        'value': 'temp_C',\n",
        "        'attributes': 'flags'\n",
        "    }, inplace=True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Yxd05EEjI6j"
      },
      "source": [
        "Those columns have been successfully renamed:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjKa3iHRjI6k"
      },
      "source": [
        "df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvF5J__ijI6k"
      },
      "source": [
        "We can also perform string operations on the column names with `rename()`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2Op23xsjI6k"
      },
      "source": [
        "df.rename(str.upper, axis='columns').columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMxOWYzzjI6l"
      },
      "source": [
        "## Type Conversion\n",
        "The `date` column is not currently being stored as a `datetime`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuixDLowjI6l"
      },
      "source": [
        "df.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yi55MC-njI6l"
      },
      "source": [
        "Let's perform the conversion with `pd.to_datetime()`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5OOnAFxjI6l"
      },
      "source": [
        "df.loc[:,'date'] = pd.to_datetime(df.date)\n",
        "df.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svzQ6u_GjI6l"
      },
      "source": [
        "Now we get useful information when we use `describe()` on this column:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O__EATyOjI6m"
      },
      "source": [
        "df.date.describe(datetime_is_numeric=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_ULdrnVjI6m"
      },
      "source": [
        "We can use `tz_localize()` on a `DatetimeIndex` object to convert to a desired timezone:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQtWtFkUjI6m"
      },
      "source": [
        "pd.date_range(start='2018-10-25', periods=2, freq='D').tz_localize('EST')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnusqbsOjI6m"
      },
      "source": [
        "This also works with `Series`/`DataFrame` objects that have an index of type `DatetimeIndex`. Let's read in the CSV again for this example and set the `date` column to be the index and stored as a datetime:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_fTqYefjI6n"
      },
      "source": [
        "eastern = pd.read_csv(\n",
        "    'data/nyc_temperatures.csv', index_col='date', parse_dates=True\n",
        ").tz_localize('EST')\n",
        "eastern.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6iI5QJ1PjI6n"
      },
      "source": [
        "We can use `tz_convert()` to convert to another timezone from there. If we convert the Eastern datetimes to UTC, they will now be at 5 AM, since `pandas` will use the offsets to convert:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfNTSOtDjI6n"
      },
      "source": [
        "eastern.tz_convert('UTC').head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHaZnVUfjI6o"
      },
      "source": [
        "We can change the period of the index as well. We could change the period to be monthly to make it easier to aggregate later. (Aggregation will be discussed in chapter 4.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jf7FZwSOjI6o"
      },
      "source": [
        "eastern.tz_localize(None).to_period('M').index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atLFyUeejI6o"
      },
      "source": [
        "We now get a `PeriodIndex` object, which we can change back into a `DatetimeIndex` object with `to_timestamp()`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGOqBQEWjI6o"
      },
      "source": [
        "eastern.tz_localize(None).to_period('M').to_timestamp().index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48_tNAqpjI6p"
      },
      "source": [
        "We can use the `assign()` method for working with multiple columns at once (or creating new ones). Since our `date` column has already been converted, we need to read in the data again:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4hCYzfPjI6p"
      },
      "source": [
        "df = pd.read_csv('data/nyc_temperatures.csv').rename(\n",
        "    columns={\n",
        "        'value': 'temp_C',\n",
        "        'attributes': 'flags'\n",
        "    }\n",
        ")\n",
        "\n",
        "new_df = df.assign(\n",
        "    date=pd.to_datetime(df.date),\n",
        "    temp_F=(df.temp_C * 9/5) + 32\n",
        ")\n",
        "new_df.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xW66tMDZjI6p"
      },
      "source": [
        "The `date` column now has datetimes and the `temp_F` column was added:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZECYj0wjI6p"
      },
      "source": [
        "new_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ku_OK_SxjI6q"
      },
      "source": [
        "We can also use `astype()` to perform conversions. Let's create columns of the integer portion of the temperatures in Celsius and Fahrenheit. We will use **lambda functions** (first introduced in *Chapter 2, Working with Pandas DataFrames*), so that we can use the values being created in the `temp_F` column to calculate the `temp_F_whole` column. It is very common (and useful) to use lambda functions with `assign()`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_pmTQmhjI6q"
      },
      "source": [
        "df = df.assign(\n",
        "    date=lambda x: pd.to_datetime(x.date),\n",
        "    temp_C_whole=lambda x: x.temp_C.astype('int'),\n",
        "    temp_F=lambda x: (x.temp_C * 9/5) + 32,\n",
        "    temp_F_whole=lambda x: x.temp_F.astype('int')\n",
        ")\n",
        "\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkmLV24qjI6q"
      },
      "source": [
        "Creating categories:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btHKTODnjI6q"
      },
      "source": [
        "df_with_categories = df.assign(\n",
        "    station=df.station.astype('category'),\n",
        "    datatype=df.datatype.astype('category')\n",
        ")\n",
        "df_with_categories.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzyazxOpjI6r"
      },
      "source": [
        "df_with_categories.describe(include='category')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xO4txUzXjI6r"
      },
      "source": [
        "Our categories have no order, but this is something that `pandas` supports:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5OD7i6njI6r"
      },
      "source": [
        "pd.Categorical(\n",
        "    ['med', 'med', 'low', 'high'], \n",
        "    categories=['low', 'med', 'high'],\n",
        "    ordered=True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMOUlqbqjI6r"
      },
      "source": [
        "## Reordering, reindexing, and sorting\n",
        "Say we want to find the days that reached the hottest temperatures in the weather data; we can sort our values by the `temp_C` column with the largest on top to find this: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_poPoUwpjI6r"
      },
      "source": [
        "df[df.datatype == 'TMAX'].sort_values(by='temp_C', ascending=False).head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xK_x9m1YjI6r"
      },
      "source": [
        "However, this isn't perfect because we have some ties, and they aren't sorted consistently. In the first tie between the 7th and the 10th, the earlier date comes first, but the opposite is true with the tie between the 4th and the 2nd. We can use other columns to break ties and specify how to sort each with `ascending`. Let's break ties with the date column and show earlier dates before later ones:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dd9UBq3hjI6s"
      },
      "source": [
        "df[df.datatype == 'TMAX'].sort_values(by=['temp_C', 'date'], ascending=[False, True]).head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F36GqflAjI6s"
      },
      "source": [
        "Notice that the index was jumbled in the past 2 results. Here, our index only stores the row number in the original data, but we may not need to keep track of that information. In this case, we can pass in `ignore_index=True` to get a new index after sorting:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JcaTgbj2jI6s"
      },
      "source": [
        "df[df.datatype == 'TMAX'].sort_values(by=['temp_C', 'date'], ascending=[False, True], ignore_index=True).head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M64Y-WqCjI6s"
      },
      "source": [
        "When just looking for the n-largest values, rather than wanting to sort all the data, we can use `nlargest()`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKF63s4EjI6s"
      },
      "source": [
        "df[df.datatype == 'TAVG'].nlargest(n=10, columns='temp_C')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDA3UE2ejI6s"
      },
      "source": [
        "We use `nsmallest()` for the n-smallest values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEigjbnfjI6s"
      },
      "source": [
        "df.nsmallest(n=5, columns=['temp_C', 'date'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5IQE_xwjI6t"
      },
      "source": [
        "The `sample()` method will give us rows (or columns with `axis=1`) at random. We can provide a seed (`random_state`) to make this reproducible. The index after we do this is jumbled:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWXFEAfmjI6t"
      },
      "source": [
        "df.sample(5, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejRbGfN4jI6t"
      },
      "source": [
        "We can use `sort_index()` to order it again:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDV-fyS3jI6t"
      },
      "source": [
        "df.sample(5, random_state=0).sort_index().index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0wOUGqvjI6t"
      },
      "source": [
        "The `sort_index()` method can also sort columns alphabetically:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9umj-BvjI6t"
      },
      "source": [
        "df.sort_index(axis=1).head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vao87tgrjI6t"
      },
      "source": [
        "This can make selection with `loc` easier for many columns:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0dMT0mtjI6u"
      },
      "source": [
        "df.sort_index(axis=1).head().loc[:,'temp_C':'temp_F_whole']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBIk1fKcjI6u"
      },
      "source": [
        "We must sort the index to compare two dataframes. If the index is different, but the data is the same, they will be marked not-equal:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQUhN1lrjI6u"
      },
      "source": [
        "df.equals(df.sort_values(by='temp_C'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIW2hszmjI6u"
      },
      "source": [
        "Sorting the index solves this issue:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXY83qUrjI6u"
      },
      "source": [
        "df.equals(df.sort_values(by='temp_C').sort_index())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPJe4RY_jI6u"
      },
      "source": [
        "Let's set the `date` column as our index:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28_C_x6AjI6u"
      },
      "source": [
        "df.set_index('date', inplace=True)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KS8FRJWzjI6v"
      },
      "source": [
        "Now that we have an index of type `DatetimeIndex`, we can do datetime slicing and indexing. As long as we provide a date format that pandas understands, we can grab the data. To select all of 2018, we simply use `df.loc['2018']`, for the fourth quarter of 2018 we can use `df.loc['2018-Q4']`, grabbing October is as simple as using `df.loc['2018-10']`; these can also be combined to build ranges. Let's grab October 11, 2018 through October 12, 2018 (inclusive of both endpoints)&mdash;note that using `loc[]` is optional for ranges:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPj9mXDsjI6v"
      },
      "source": [
        "df['2018-10-11':'2018-10-12']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ykq8lanwjI6v"
      },
      "source": [
        "We can also use `reset_index()` to get a fresh index and move our current index into a column for safe keeping. This is especially useful if we had data, such as the date, in the index that we don't want to lose:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7g0eF6ZjI6v"
      },
      "source": [
        "df['2018-10-11':'2018-10-12'].reset_index()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8WSghZ-jI6v"
      },
      "source": [
        "Reindexing allows us to conform our axis to contain a given set of labels. Let's turn to the S&P 500 stock data in the `sp500.csv` file to see an example of this. Notice we only have data for trading days (weekdays, excluding holidays):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wABvjMitjI6v"
      },
      "source": [
        "sp = pd.read_csv(\n",
        "    'data/sp500.csv', index_col='date', parse_dates=True\n",
        ").drop(columns=['adj_close'])\n",
        "\n",
        "sp.head(10).assign(\n",
        "    day_of_week=lambda x: x.index.day_name()\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZHdCFYrjI6v"
      },
      "source": [
        "If we want to look at the value of a portfolio (group of assets) that trade on different days, we need to handle the mismatch in the index. Bitcoin, for example, trades daily. If we sum up all the data we have for each day (aggregations will be covered in chapter 4, so don't fixate on this part), we get the following:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIMvZslAjI6w"
      },
      "source": [
        "bitcoin = pd.read_csv(\n",
        "    'data/bitcoin.csv', index_col='date', parse_dates=True\n",
        ").drop(columns=['market_cap'])\n",
        "\n",
        "# every day's closing price = S&P 500 close + Bitcoin close (same for other metrics)\n",
        "portfolio = pd.concat([sp, bitcoin], sort=False).groupby(level='date').sum()\n",
        "\n",
        "portfolio.head(10).assign(\n",
        "    day_of_week=lambda x: x.index.day_name()\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdE9bgx4jI6w"
      },
      "source": [
        "It may not be immediately obvious what is wrong with the previous data, but with a visualization we can easily see the cyclical pattern of drops on the days the stock market is closed. (Don't worry about the plotting code too much, we will cover it in depth in chapters 5 and 6).\n",
        "\n",
        "We will need to import `matplotlib` now:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ly-dWN73jI6w"
      },
      "source": [
        "import matplotlib.pyplot as plt # we use this module for plotting\n",
        "from matplotlib.ticker import StrMethodFormatter # for formatting the axis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXldwi0YjI6w"
      },
      "source": [
        "Now we can see why we need to reindex:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9IZv6C6OjI6x"
      },
      "source": [
        "# plot the closing price from Q4 2017 through Q2 2018\n",
        "ax = portfolio['2017-Q4':'2018-Q2'].plot(\n",
        "    y='close', figsize=(15, 5), legend=False,\n",
        "    title='Bitcoin + S&P 500 value without accounting for different indices'\n",
        ")\n",
        "\n",
        "# formatting\n",
        "ax.set_ylabel('price')\n",
        "ax.yaxis.set_major_formatter(StrMethodFormatter('${x:,.0f}'))\n",
        "for spine in ['top', 'right']:\n",
        "    ax.spines[spine].set_visible(False)\n",
        "\n",
        "# show the plot\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPF-YMBijI6x"
      },
      "source": [
        "We need to align the index of the S&P 500 to match bitcoin in order to fix this. We will use the `reindex()` method, but by default we get `NaN` for the values that we don't have data for:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Il0Rz-4zjI6x"
      },
      "source": [
        "sp.reindex(bitcoin.index).head(10).assign(\n",
        "    day_of_week=lambda x: x.index.day_name()\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kkmxdTojI6x"
      },
      "source": [
        "So now we have rows for every day of the year, but all the weekends and holidays have `NaN` values. To address this, we can specify how to handle missing values with the `method` argument. In this case, we want to forward-fill, which will put the weekend and holiday values as the value they had for the Friday (or end of trading week) before:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53XeY3apjI6x"
      },
      "source": [
        "sp.reindex(bitcoin.index, method='ffill').head(10)\\\n",
        "    .assign(day_of_week=lambda x: x.index.day_name())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MxQHEBmjI6x"
      },
      "source": [
        "To isolate the changes happening with the forward-filling, we can use the `compare()` method. It shows us the values that differ across identically-labeled dataframes (same names and same columns). Here, we can see that only weekends and holidays (Monday, January 16, 2017 was MLK day) have values forward-filled. Notice that consecutive days have the same values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0BrY_B1jI6x"
      },
      "source": [
        "sp.reindex(bitcoin.index)\\\n",
        "    .compare(sp.reindex(bitcoin.index, method='ffill'))\\\n",
        "    .head(10).assign(day_of_week=lambda x: x.index.day_name())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVRRWvS7jI6y"
      },
      "source": [
        "This isn't perfect though. We probably want 0 for the volume traded and to put the closing price for the open, high, low, and close on the days the market is closed:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-xqwJpgjI6y"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "sp_reindexed = sp.reindex(bitcoin.index).assign(\n",
        "    volume=lambda x: x.volume.fillna(0), # put 0 when market is closed\n",
        "    close=lambda x: x.close.fillna(method='ffill'), # carry this forward\n",
        "    # take the closing price if these aren't available\n",
        "    open=lambda x: np.where(x.open.isnull(), x.close, x.open),\n",
        "    high=lambda x: np.where(x.high.isnull(), x.close, x.high),\n",
        "    low=lambda x: np.where(x.low.isnull(), x.close, x.low)\n",
        ")\n",
        "sp_reindexed.head(10).assign(\n",
        "    day_of_week=lambda x: x.index.day_name()\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ly9h5YCUjI6y"
      },
      "source": [
        "If we create a visualization comparing the reindexed data to the first attempt, we see how reindexing helped maintain the asset value when the market was closed:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntq7FcirjI6y"
      },
      "source": [
        "# every day's closing price = S&P 500 close adjusted for market closure + Bitcoin close (same for other metrics)\n",
        "fixed_portfolio = sp_reindexed + bitcoin\n",
        "\n",
        "# plot the reindexed portfolio's closing price from Q4 2017 through Q2 2018\n",
        "ax = fixed_portfolio['2017-Q4':'2018-Q2'].plot(\n",
        "    y='close', label='reindexed portfolio of S&P 500 + Bitcoin', figsize=(15, 5), linewidth=2, \n",
        "    title='Reindexed portfolio vs. portfolio with mismatched indices'\n",
        ")\n",
        "\n",
        "# add line for original portfolio for comparison\n",
        "portfolio['2017-Q4':'2018-Q2'].plot(\n",
        "    y='close', ax=ax, linestyle='--', label='portfolio of S&P 500 + Bitcoin w/o reindexing'\n",
        ")\n",
        "\n",
        "# formatting\n",
        "ax.set_ylabel('price')\n",
        "ax.yaxis.set_major_formatter(StrMethodFormatter('${x:,.0f}'))\n",
        "for spine in ['top', 'right']:\n",
        "    ax.spines[spine].set_visible(False)\n",
        "\n",
        "# show the plot\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3-4. reshaping data**"
      ],
      "metadata": {
        "id": "9JMhMWOrkv9q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "long_data=pd.read_csv('data/long_data.csv')\n",
        "long_data"
      ],
      "metadata": {
        "id": "nss_srdB8P7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6I0uAD5-G4G"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "long_df = pd.read_csv(\n",
        "    'data/long_data.csv', usecols=['date', 'datatype', 'value']\n",
        ").rename(\n",
        "    columns={'value': 'temp_C'}\n",
        ").assign(\n",
        "    date=lambda x: pd.to_datetime(x.date),\n",
        "    temp_F=lambda x: (x.temp_C * 9/5) + 32\n",
        ")\n",
        "long_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcFUDYon-G4K"
      },
      "source": [
        "## Transposing\n",
        "Transposing swaps the rows and the columns. We use the `T` attribute to do so:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "long_df.set_index('date').T"
      ],
      "metadata": {
        "id": "emGG8D5-8qZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZydWCyPR-G4K"
      },
      "source": [
        "long_df.set_index('date').head(6).T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "go4W4RSM-G4L"
      },
      "source": [
        "## Pivoting\n",
        "Going from long to wide format.\n",
        "\n",
        "### `pivot()`\n",
        "We can restructure our data by picking a column to go in the index (`index`), a column whose unique values will become column names (`columns`), and the values to place in those columns (`values`). The `pivot()` method can be used when we don't need to perform any aggregation in addition to our restructuring (when our index is unique); if this is not the case, we need the `pivot_table()` method which we will cover in chapter 4. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfNzZkg7-G4M"
      },
      "source": [
        "pivoted_df = long_df.pivot(\n",
        "    index='date', columns='datatype', values='temp_C'\n",
        ")\n",
        "pivoted_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4LsIWrS-G4M"
      },
      "source": [
        "Now that the data is pivoted, we have wide format data that we can grab summary statistics with:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mS8eO-DK-G4N"
      },
      "source": [
        "pivoted_df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94_vOTB1-G4O"
      },
      "source": [
        "We can also provide multiple values to pivot on, which will result in a hierarchical index:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0zMFCIB-G4P"
      },
      "source": [
        "pivoted_df = long_df.pivot(\n",
        "    index='date', columns='datatype', values=['temp_C', 'temp_F']\n",
        ")\n",
        "pivoted_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIwzm1ok-G4Q"
      },
      "source": [
        "With the hierarchical index, if we want to select `TMIN` in Fahrenheit, we will first need to select `temp_F` and then `TMIN`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIRDypK2-G4Q"
      },
      "source": [
        "pivoted_df['temp_F']['TMIN'].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bdm8I7LH-G4S"
      },
      "source": [
        "### `unstack()`\n",
        "\n",
        "We have been working with a single index throughout this chapter; however, we can create an index from any number of columns with `set_index()`. This gives us an index of type `MultiIndex`, where the outermost level corresponds to the first element in the list provided to `set_index()`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhIwaIMF-G4S"
      },
      "source": [
        "multi_index_df = long_df.set_index(['date', 'datatype'])\n",
        "multi_index_df.head().index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "taa35dHf-G4T"
      },
      "source": [
        "Notice there are now 2 index sections of the dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n50mzeRm-G4U"
      },
      "source": [
        "multi_index_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0l0Szha6-G4V"
      },
      "source": [
        "With an index of type `MultiIndex`, we can no longer use `pivot()`. We must now use `unstack()`, which by default moves the innermost index onto the columns:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROW1FeG_-G4W"
      },
      "source": [
        "unstacked_df = multi_index_df.unstack()\n",
        "unstacked_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnCwBrR5-G4X"
      },
      "source": [
        "The `unstack()` method also provides the `fill_value` parameter, which let's us fill-in any `NaN` values that might arise from this restructuring of the data. Consider the case that we have data for the average temperature on October 1, 2018, but no other date:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "extra_data = long_df.append([{\n",
        "    'datatype': 'TAVG', \n",
        "    'date': '2018-10-01', \n",
        "    'temp_C': 10, \n",
        "    'temp_F': 50\n",
        "}]).set_index(['date', 'datatype'])\n",
        "\n",
        "extra_data"
      ],
      "metadata": {
        "id": "ZlzgCsEd_5vW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5nIicTE-G4Y"
      },
      "source": [
        "extra_data = long_df.append([{\n",
        "    'datatype': 'TAVG', \n",
        "    'date': '2018-10-01', \n",
        "    'temp_C': 10, \n",
        "    'temp_F': 50\n",
        "}]).set_index(['date', 'datatype']).sort_index()\n",
        "\n",
        "extra_data['2018-10-01':'2018-10-02']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHXox_G2-G4Z"
      },
      "source": [
        "If we use `unstack()` in this case, we will have `NaN` for the `TAVG` columns every day but October 1, 2018:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-F2ULIR5-G4Z"
      },
      "source": [
        "extra_data.unstack().head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzFY6b8Z-G4a"
      },
      "source": [
        "To address this, we can pass in an appropriate `fill_value`. However, we are restricted to passing in a value for this, not a strategy (like we saw with `fillna()`), so while `-40` is definitely not be the best value, we can use it to illustrate how this works, since this is the temperature at which Fahrenheit and Celsius are equal:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lJRqmQl-G4a"
      },
      "source": [
        "extra_data.unstack(fill_value=-40).head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FImlx7vF-G4b"
      },
      "source": [
        "## Melting\n",
        "Going from wide to long format.\n",
        "\n",
        "### Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ezyjrfL-G4b"
      },
      "source": [
        "wide_df = pd.read_csv('data/wide_data.csv')\n",
        "wide_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGvce4CZ-G4b"
      },
      "source": [
        "### `melt()`\n",
        "In order to go from wide format to long format, we use the `melt()` method. We have to specify:\n",
        "- `id_vars`: which column(s) uniquely identify a row in the wide format (`date`, here)\n",
        "- `value_vars`: the column(s) that contain(s) the values (`TMAX`, `TMIN`, and `TOBS`, here)\n",
        "\n",
        "Optionally, we can also provide:\n",
        "- `value_name`: what to call the column that will contain all the values once melted\n",
        "- `var_name`: what to call the column that will contain the names of the variables being measured"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-SJIYFK--G4c"
      },
      "source": [
        "melted_df = wide_df.melt(\n",
        "    id_vars='date',\n",
        "    value_vars=['TMAX', 'TMIN', 'TOBS'],\n",
        "    value_name='temp_C',\n",
        "    var_name='measurement'\n",
        ")\n",
        "melted_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqvvhIZe-G4c"
      },
      "source": [
        "### `stack()`\n",
        "Another option is `stack()`, which will pivot the columns of the dataframe into the innermost level of the index (resulting in an index of type `MultiIndex`). To illustrate this, let's set our index to be the `date` column:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPyGuS7q-G4c"
      },
      "source": [
        "wide_df.set_index('date', inplace=True)\n",
        "wide_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCPn_zm--G4d"
      },
      "source": [
        "By running `stack()` now, we will create a second level in our index which will contain the column names of our dataframe (`TMAX`, `TMIN`, `TOBS`). This will leave us with a `Series` object containing the values:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RhANSR7-G4d"
      },
      "source": [
        "stacked_series = wide_df.stack()\n",
        "stacked_series.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKVnJKdp-G4d"
      },
      "source": [
        "We can use the `to_frame()` method on our `Series` object to turn it into a `DataFrame` object. Since the series doesn't have a name at the moment, we will pass in the name as an argument:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dFB7WL9-G4d"
      },
      "source": [
        "stacked_df = stacked_series.to_frame('values')\n",
        "stacked_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2z-uGdv-G4e"
      },
      "source": [
        "Once again, we have an index of type `MultiIndex`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPMKd42w-G4e"
      },
      "source": [
        "stacked_df.head().index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msZkYIkb-G4f"
      },
      "source": [
        "Unfortunately, we don't have a name for the `datatype` level:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hz1ya-Oi-G4f"
      },
      "source": [
        "stacked_df.index.names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIhMIxrY-G4f"
      },
      "source": [
        "We can use `set_names()` to address this though:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HvL5F0xb-G4f"
      },
      "source": [
        "stacked_df.index.set_names(['date', 'datatype'], inplace=True)\n",
        "stacked_df.index.names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3-5. handling data issues**"
      ],
      "metadata": {
        "id": "X6SUnW9ak5hJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tG0BZGi-lfZ_"
      },
      "source": [
        "# Handling duplicate, missing, or invalid data\n",
        "\n",
        "## About the data\n",
        "In this notebook, we will using daily weather data that was taken from the [National Centers for Environmental Information (NCEI) API](https://www.ncdc.noaa.gov/cdo-web/webservices/v2) and altered to introduce many common problems faced when working with data. \n",
        "\n",
        "*Note: The NCEI is part of the National Oceanic and Atmospheric Administration (NOAA) and, as you can see from the URL for the API, this resource was created when the NCEI was called the NCDC. Should the URL for this resource change in the future, you can search for \"NCEI weather API\" to find the updated one.*\n",
        "\n",
        "## Background on the data\n",
        "\n",
        "Data meanings:\n",
        "- `PRCP`: precipitation in millimeters\n",
        "- `SNOW`: snowfall in millimeters\n",
        "- `SNWD`: snow depth in millimeters\n",
        "- `TMAX`: maximum daily temperature in Celsius\n",
        "- `TMIN`: minimum daily temperature in Celsius\n",
        "- `TOBS`: temperature at time of observation in Celsius\n",
        "- `WESF`: water equivalent of snow in millimeters\n",
        "\n",
        "Some important facts to get our bearings:\n",
        "- According to the National Weather Service, the coldest temperature ever recorded in Central Park was -15°F (-26.1°C) on February 9, 1934: [source](https://www.weather.gov/media/okx/Climate/CentralPark/extremes.pdf) \n",
        "- The temperature of the Sun's photosphere is approximately 5,505°C: [source](https://en.wikipedia.org/wiki/Sun)\n",
        "\n",
        "## Setup\n",
        "We need to import `pandas` and read in the dirty data to get started:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "4kPmx3eNn2kn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/Hands-On-Data-Analysis-with-Pandas-2nd-edition/ch_03"
      ],
      "metadata": {
        "id": "pBbugmOKn9U_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7egSe7-UlfaJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('data/dirty_data.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ub3hsDc2lfaK"
      },
      "source": [
        "## Finding problematic data\n",
        "A good first step is to look at some rows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VQ8kqBaDlfaL"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fNSZKQXnoSnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLus0v5zlfaM"
      },
      "source": [
        "Looking at summary statistics can reveal strange or missing values:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cPOtd3nalfaM"
      },
      "outputs": [],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NkgX7rTlfaN"
      },
      "source": [
        "The `info()` method can pinpoint missing values and wrong data types:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-yTYEqQAlfaO"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIKcbdlPlfaP"
      },
      "source": [
        "We can use the `isna()`/`isnull()` method of the series to find nulls:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QF4fzlollfaQ"
      },
      "outputs": [],
      "source": [
        "contain_nulls = df[\n",
        "    df.SNOW.isna() | df.SNWD.isna() | df.TOBS.isna()\n",
        "    | df.WESF.isna() | df.inclement_weather.isna()\n",
        "]\n",
        "contain_nulls.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xSmdhN0FlfaS"
      },
      "outputs": [],
      "source": [
        "contain_nulls.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrKvG2NPlfaS"
      },
      "source": [
        "Note that we can't check if we have `NaN` like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L6GzvPRYlfaT"
      },
      "outputs": [],
      "source": [
        "df[df.inclement_weather == 'NaN'].shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBYku1balfaU"
      },
      "source": [
        "This is because it is actually `np.nan`. However, notice this also doesn't work:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cGRa5RYalfaU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "df[df.inclement_weather == np.nan].shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0Q7UKuYlfaY"
      },
      "source": [
        "We have to use one of the methods discussed earlier for this to work:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ck97hn0lfab"
      },
      "outputs": [],
      "source": [
        "df[df.inclement_weather.isna()].shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YgnVf7flfab"
      },
      "source": [
        "We can find `-inf`/`inf` by comparing to `-np.inf`/`np.inf`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bDFRwOd3lfac"
      },
      "outputs": [],
      "source": [
        "df[df.SNWD.isin([-np.inf, np.inf])]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5016a_Glfad"
      },
      "source": [
        "Rather than do this for each column, we can write a function that will use a [dictionary comprehension](https://www.python.org/dev/peps/pep-0274/) to check all the columns for us:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-vUph5vulfad"
      },
      "outputs": [],
      "source": [
        "def get_inf_count(df):\n",
        "    \"\"\"Find the number of inf/-inf values per column in the dataframe\"\"\"\n",
        "    return {\n",
        "        col: df[df[col].isin([np.inf, -np.inf])].shape[0] for col in df.columns\n",
        "    }\n",
        "\n",
        "get_inf_count(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlXH3fotlfae"
      },
      "source": [
        "Before we can decide how to handle the infinite values of snow depth, we should look at the summary statistics for snowfall, which forms a big part in determining the snow depth:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7d80U6vMlfaf"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame({\n",
        "    'np.inf Snow Depth': df[df.SNWD == np.inf].SNOW.describe(),\n",
        "    '-np.inf Snow Depth': df[df.SNWD == -np.inf].SNOW.describe()\n",
        "}).T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "456NRi3Plfag"
      },
      "source": [
        "Let's now look into the `date` and `station` columns. We saw the `?` for station earlier, so we know that was the other unique value. However, we see that some dates are present 8 times in the data and we only have 324 days meaning we are also missing days:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8vyGd0Anlfah"
      },
      "outputs": [],
      "source": [
        "df.describe(include='object')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHsLgAfslfah"
      },
      "source": [
        "We can use the `duplicated()` method to find duplicate rows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67BdiVKClfah"
      },
      "outputs": [],
      "source": [
        "df[df.duplicated()].shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByvWf97-lfai"
      },
      "source": [
        "The default for `keep` is `'first'` meaning it won't show the first row that the duplicated data was seen in; we can pass in `False` to see it though:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LMzIRm8Vlfai"
      },
      "outputs": [],
      "source": [
        "df[df.duplicated(keep=False)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nExqYOydlfaj"
      },
      "source": [
        "We can also specify the columns to use:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mn13OQbjlfaj"
      },
      "outputs": [],
      "source": [
        "df[df.duplicated(['date', 'station'])]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsQk6l-Rlfaj"
      },
      "source": [
        "Let's look at a few duplicates. Just in the few values we see here, we know that the top 4 are actually in the data 6 times because by default we aren't seeing their first occurrence:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ApZ2WtNRlfak"
      },
      "outputs": [],
      "source": [
        "df[df.duplicated()].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZxxdGdOlfak"
      },
      "source": [
        "## Mitigating Issues\n",
        "\n",
        "### Handling duplicated data\n",
        "Since we know we have NY weather data and noticed we only had two entries for `station`, we may decide to drop the `station` column because we are only interested in the weather data. However, when dealing with duplicate data, we need to think of the ramifications of removing it. Notice we only have data for the `WESF` column when the station is `?`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQa_XMPzlfal"
      },
      "outputs": [],
      "source": [
        "df[df.WESF.notna()].station.unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-o9HPVmlfal"
      },
      "source": [
        "If we determine it won't impact our analysis, we can use `drop_duplicates()` to remove them:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iEDNtPmslfam"
      },
      "outputs": [],
      "source": [
        "# 1. make the date a datetime\n",
        "df.date = pd.to_datetime(df.date)\n",
        "\n",
        "# 2. save this information for later\n",
        "station_qm_wesf = df[df.station == '?'].drop_duplicates('date').set_index('date').WESF\n",
        "\n",
        "# 3. sort ? to the bottom\n",
        "df.sort_values('station', ascending=False, inplace=True)\n",
        "\n",
        "# 4. drop duplicates based on the date column keeping the first occurrence \n",
        "# which will be the valid station if it has data\n",
        "df_deduped = df.drop_duplicates('date')\n",
        "\n",
        "# 5. remove the station column because we are done with it\n",
        "df_deduped = df_deduped.drop(columns='station').set_index('date').sort_index()\n",
        "\n",
        "# 6. take valid station's WESF and fall back on station ? if it is null\n",
        "df_deduped = df_deduped.assign(\n",
        "    WESF=lambda x: x.WESF.combine_first(station_qm_wesf)\n",
        ")\n",
        "\n",
        "df_deduped"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "station_qm_wesf"
      ],
      "metadata": {
        "id": "G2mjE_VMEWkV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wVDd_Eplfam"
      },
      "source": [
        "Here we used the `combine_first()` method to coalesce the values to the first non-null entry; this means that if we had data from both stations, we would first take the value provided by the named station and if (and only if) that station was null would we take the value from the station named `?`. The following table contains some examples of how this would play out:\n",
        "\n",
        "| station GHCND:USC00280907 | station ? | result of `combine_first()` |\n",
        "| :---: | :---: | :---: |\n",
        "| 1 | 17 | 1 |\n",
        "| 1 | `NaN` | 1 |\n",
        "| `NaN` | 17 | 17 |\n",
        "| `NaN` | `NaN` | `NaN` |\n",
        "\n",
        "Check out the 4th row&mdash;we have `WESF` in the correct spot thanks to the index:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V68F9hqWlfan"
      },
      "outputs": [],
      "source": [
        "df_deduped.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLAAvbCplfan"
      },
      "source": [
        "### Dealing with nulls\n",
        "We could drop nulls, replace them with some arbitrary value, or impute them using the surrounding data. Each of these options may have ramifications, so we must choose wisely.\n",
        "\n",
        "We can use `dropna()` to drop rows where any column has a null value. The default options leave us hardly any data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XU8huC52lfan"
      },
      "outputs": [],
      "source": [
        "df_deduped.dropna().shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ge9KHiOSlfap"
      },
      "source": [
        "If we pass `how='all'`, we can choose to only drop rows where everything is null, but this removes nothing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jPxIOumJlfap"
      },
      "outputs": [],
      "source": [
        "df_deduped.dropna(how='all').shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivIQ6eDClfaq"
      },
      "source": [
        "We can use just a subset of columns to determine what to drop with the `subset` argument:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7zYapgzLlfar"
      },
      "outputs": [],
      "source": [
        "df_deduped.dropna(\n",
        "    how='all', subset=['inclement_weather', 'SNOW', 'SNWD']\n",
        ").shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LURKQolelfar"
      },
      "source": [
        "This can also be performed along columns, and we can also require a certain number of null values before we drop the data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5RXtRy1Klfas"
      },
      "outputs": [],
      "source": [
        "df_deduped.dropna(axis='columns', thresh=df_deduped.shape[0] * .75)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9gL3PBnlfas"
      },
      "source": [
        "We can choose to fill in the null values instead with `fillna()`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oNd5gUOulfat"
      },
      "outputs": [],
      "source": [
        "df_deduped.loc[:,'WESF'].fillna(0, inplace=True)\n",
        "df_deduped.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZaCPBdUwlfau"
      },
      "source": [
        "At this point we have done everything we can without distorting the data. We know that we are missing dates, but if we reindex, we don't know how to fill in the `NaN` data. With the weather data, we can't assume because it snowed one day that it will snow the next or that the temperature will be the same. For this reason, note that the next few examples are just for illustrative purposes only—just because we can do something doesn't mean we should.\n",
        "\n",
        "That being said, let's try to address some of remaining issues with the temperature data. We know that when `TMAX` is the temperature of the Sun, it must be because there was no measured value, so let's replace it with `NaN`. We will also do so for `TMIN` which currently uses -40°C for its placeholder when we know that the coldest temperature ever recorded in NYC was -15°F (-26.1°C) on February 9, 1934:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iSU1j7jylfav"
      },
      "outputs": [],
      "source": [
        "df_deduped = df_deduped.assign(\n",
        "    TMAX=lambda x: x.TMAX.replace(5505, np.nan),\n",
        "    TMIN=lambda x: x.TMIN.replace(-40, np.nan),\n",
        ")\n",
        "\n",
        "df_deduped"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_jceAkzlfav"
      },
      "source": [
        "We will also make an assumption that the temperature won't change drastically day-to-day. Note that this is actually a big assumption, but it will allow us to understand how `fillna()` works when we provide a strategy through the `method` parameter. The `fillna()` method gives us 2 options for the `method` parameter:\n",
        "- `'ffill'` to forward-fill\n",
        "- `'bfill'` to back-fill\n",
        "\n",
        "*Note that `'nearest'` is missing because we are not reindexing.*\n",
        "\n",
        "Here, we will use `'ffill'` to show how this works:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNFc-bJSlfaw"
      },
      "outputs": [],
      "source": [
        "df_deduped.assign(\n",
        "    TMAX=lambda x: x.TMAX.fillna(method='ffill'),\n",
        "    TMIN=lambda x: x.TMIN.fillna(method='ffill'),\n",
        "    TOBS=lambda x: x.TOBS.fillna(method='ffill')\n",
        ").head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsIzAamElfax"
      },
      "source": [
        "We can use `np.nan_to_num()` to turn `np.nan` into 0 and `-np.inf`/`np.inf` into large negative or positive finite numbers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WP5gsqv9lfax"
      },
      "outputs": [],
      "source": [
        "df_deduped.assign(\n",
        "    SNWD=lambda x: np.nan_to_num(x.SNWD)\n",
        ").head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNvnjHc8lfay"
      },
      "source": [
        "Depending on the data we are working with, we can use the `clip()` method as an alternative to `np.nan_to_num()`. The `clip()` method makes it possible to cap values at a specific minimum and/or maximum threshold. Since `SNWD` can't be negative, let's use `clip()` to enforce a lower bound of zero. To show how the upper bound works, let's use the value of `SNOW`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ybS10Idvlfay"
      },
      "outputs": [],
      "source": [
        "df_deduped.assign(\n",
        "    SNWD=lambda x: x.SNWD.clip(0, x.SNOW)\n",
        ").head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCyc_ehClfay"
      },
      "source": [
        "We can couple `fillna()` with other types of calculations. Here we replace missing values of `TMAX` with the median of all `TMAX` values, `TMIN` with the median of all `TMIN` values, and `TOBS` to the average of the `TMAX` and `TMIN` values. Since we place `TOBS` last, we have access to the imputed values for `TMIN` and `TMAX` in the calculation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IAOILw-Ilfay"
      },
      "outputs": [],
      "source": [
        "df_deduped.assign(\n",
        "    TMAX=lambda x: x.TMAX.fillna(x.TMAX.median()),\n",
        "    TMIN=lambda x: x.TMIN.fillna(x.TMIN.median()),\n",
        "    # average of TMAX and TMIN\n",
        "    TOBS=lambda x: x.TOBS.fillna((x.TMAX + x.TMIN) / 2)\n",
        ").head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxsNqwAzlfay"
      },
      "source": [
        "We can also use `apply()` for running the same calculation across columns. For example, let's fill all missing values with their rolling 7-day median of their values, setting the number of periods required for the calculation to 0 to ensure we don't introduce more extra `NaN` values. Rolling calculations will be covered in chapter 4, so this is a preview:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0T61Rf2slfaz"
      },
      "outputs": [],
      "source": [
        "df_deduped.apply(\n",
        "    # rolling calculations will be covered in chapter 4, this is a rolling 7-day median\n",
        "    # we set min_periods (# of periods required for calculation) to 0 so we always get a result \n",
        "    lambda x: x.fillna(x.rolling(7, min_periods=0).median())\n",
        ").head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-LcUQPGlfaz"
      },
      "source": [
        "The last strategy we could try is interpolation with the `interpolate()` method. We specify the `method` parameter with the interpolation strategy to use. There are many options, but we will stick with the default of `'linear'`, which will treat values as evenly spaced and place missing values in the middle of existing ones. We have some missing data, so we will reindex first. Look at January 9th, which we didn't have before—the values for `TMAX`, `TMIN`, and `TOBS` are the average of values the day prior (January 8th) and the day after (January 10th):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2qqIvrtmlfa0"
      },
      "outputs": [],
      "source": [
        "df_deduped\\\n",
        "    .reindex(pd.date_range('2018-01-01', '2018-12-31', freq='D'))\\\n",
        "    .apply(lambda x: x.interpolate())\\\n",
        "    .head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Tsn7GN9lfa0"
      },
      "source": [
        "<hr>\n",
        "\n",
        "<div style=\"overflow: hidden; margin-bottom: 10px;\">\n",
        "    <div style=\"float: left;\">\n",
        "         <a href=\"./4-reshaping_data.ipynb\">\n",
        "            <button>&#8592; Previous Notebook</button>\n",
        "        </a>\n",
        "    </div>\n",
        "    <div style=\"float: right;\">\n",
        "        <a href=\"../../solutions/ch_03/solutions.ipynb\">\n",
        "            <button>Solutions</button>\n",
        "        </a>\n",
        "        <a href=\"../ch_04/1-querying_and_merging.ipynb\">\n",
        "            <button>Chapter 4 &#8594;</button>\n",
        "        </a>\n",
        "    </div>\n",
        "</div>\n",
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4-0. weather data collection**"
      ],
      "metadata": {
        "id": "P29AaAtZlQat"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xt0KTZalVs3"
      },
      "source": [
        "# Collecting weather data from an API\n",
        "\n",
        "This notebook contains the code that was used to collect the data for this chapter. Note that if you overwrite the data that came with this chapter by saving the data you collect here, your results in the remaining notebooks may not match the book due to changes in the NCEI API's data.\n",
        "\n",
        "## About the data\n",
        "In this notebook, we will be collecting daily weather data from the [National Centers for Environmental Information (NCEI) API](https://www.ncdc.noaa.gov/cdo-web/webservices/v2). We will use the Global Historical Climatology Network - Daily (GHCND) dataset; see the documentation [here](https://www1.ncdc.noaa.gov/pub/data/cdo/documentation/GHCND_documentation.pdf).\n",
        "\n",
        "*Note: The NCEI is part of the National Oceanic and Atmospheric Administration (NOAA) and, as you can see from the URL for the API, this resource was created when the NCEI was called the NCDC. Should the URL for this resource change in the future, you can search for \"NCEI weather API\" to find the updated one.*\n",
        "\n",
        "## Using the NCEI API\n",
        "Request your token [here](https://www.ncdc.noaa.gov/cdo-web/token) and then paste it below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KnZgmuTXlVs9"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "def make_request(endpoint, payload=None):\n",
        "    \"\"\"\n",
        "    Make a request to a specific endpoint on the weather API\n",
        "    passing headers and optional payload.\n",
        "    \n",
        "    Parameters:\n",
        "        - endpoint: The endpoint of the API you want to \n",
        "                    make a GET request to.\n",
        "        - payload: A dictionary of data to pass along \n",
        "                   with the request.\n",
        "    \n",
        "    Returns:\n",
        "        Response object.\n",
        "    \"\"\"\n",
        "    return requests.get(\n",
        "        f'https://www.ncdc.noaa.gov/cdo-web/api/v2/{endpoint}',\n",
        "        headers={\n",
        "            'token': 'PASTE_YOUR_TOKEN_HERE'\n",
        "        },\n",
        "        params=payload\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVYbbGJUlVs-"
      },
      "source": [
        "## Collect All Data Points for 2018 In NYC (Various Stations)\n",
        "We can make a loop to query for all the data points one day at a time, providing updates using `IPython.display`. Here we create a list of all the results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ph2UAbnlVs_"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "\n",
        "from IPython import display # for updating the cell dynamically\n",
        "\n",
        "current = datetime.date(2018, 1, 1)\n",
        "end = datetime.date(2019, 1, 1)\n",
        "\n",
        "results = []\n",
        "\n",
        "while current < end:\n",
        "    # update the cell with status information\n",
        "    display.clear_output(wait=True)\n",
        "    display.display(f'Gathering data for {str(current)}')\n",
        "    \n",
        "    response = make_request(\n",
        "        'data', \n",
        "        {\n",
        "            'datasetid': 'GHCND', # Global Historical Climatology Network - Daily (GHCND) dataset\n",
        "            'locationid': 'CITY:US360019', # NYC\n",
        "            'startdate': current,\n",
        "            'enddate': current,\n",
        "            'units': 'metric',\n",
        "            'limit': 1000 # max allowed\n",
        "        }\n",
        "    )\n",
        "\n",
        "    if response.ok:\n",
        "        # we extend the list instead of appending to avoid getting a nested list\n",
        "        results.extend(response.json()['results'])\n",
        "\n",
        "    # update the current date to avoid an infinite loop\n",
        "    current += datetime.timedelta(days=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpythtdUlVtB"
      },
      "source": [
        "Now, we can create a dataframe with all this data. Notice there are multiple stations with values for each `datatype` on a given day. We don't know what the stations are, but we can look them up and add them to the data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yCD-wnoYlVtC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(results)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nezHMuyolVtD"
      },
      "source": [
        "Save this data to a file:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ePATv5MlVtE"
      },
      "outputs": [],
      "source": [
        "df.to_csv('data/nyc_weather_2018.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tj1-ExIHlVtF"
      },
      "source": [
        "and write it to the database:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nGGNjIdmlVtF"
      },
      "outputs": [],
      "source": [
        "import sqlite3\n",
        "\n",
        "with sqlite3.connect('data/weather.db') as connection:\n",
        "    df.to_sql(\n",
        "        'weather', connection, index=False, if_exists='replace'\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kT-zVugylVtG"
      },
      "source": [
        "For learning about merging dataframes, we will also get the data mapping station IDs to information about the station:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QXaTTgZYlVtG"
      },
      "outputs": [],
      "source": [
        "response = make_request(\n",
        "    'stations', \n",
        "    {\n",
        "        'datasetid': 'GHCND', # Global Historical Climatology Network - Daily (GHCND) dataset\n",
        "        'locationid': 'CITY:US360019', # NYC\n",
        "        'limit': 1000 # max allowed\n",
        "    }\n",
        ")\n",
        "\n",
        "stations = pd.DataFrame(response.json()['results'])[['id', 'name', 'latitude', 'longitude', 'elevation']]\n",
        "stations.to_csv('data/weather_stations.csv', index=False)\n",
        "\n",
        "with sqlite3.connect('data/weather.db') as connection:\n",
        "    stations.to_sql(\n",
        "        'stations', connection, index=False, if_exists='replace'\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3w0IIoWlVtH"
      },
      "source": [
        "<hr>\n",
        "<div>\n",
        "    <a href=\"../ch_03/5-handling_data_issues.ipynb\">\n",
        "        <button>&#8592; Chapter 3</button>\n",
        "    </a>\n",
        "    <a href=\"./1-querying_and_merging.ipynb\">\n",
        "        <button style=\"float: right;\">Next Notebook &#8594;</button>\n",
        "    </a>\n",
        "</div>\n",
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4-1. querying and merging**"
      ],
      "metadata": {
        "id": "5zjia51KlscB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5k1xJb_7pct"
      },
      "source": [
        "# Performing Database-style Operations on Dataframes\n",
        "\n",
        "## About the data\n",
        "In this notebook, we will using daily weather data that was taken from the [National Centers for Environmental Information (NCEI) API](https://www.ncdc.noaa.gov/cdo-web/webservices/v2). The [`0-weather_data_collection.ipynb`](./0-weather_data_collection.ipynb) notebook contains the process that was followed to collect the data. Consult the dataset's [documentation](https://www1.ncdc.noaa.gov/pub/data/cdo/documentation/GHCND_documentation.pdf) for information on the fields.\n",
        "\n",
        "*Note: The NCEI is part of the National Oceanic and Atmospheric Administration (NOAA) and, as you can see from the URL for the API, this resource was created when the NCEI was called the NCDC. Should the URL for this resource change in the future, you can search for \"NCEI weather API\" to find the updated one.*\n",
        "\n",
        "\n",
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "mJY1py3mHKlq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/Hands-On-Data-Analysis-with-Pandas-2nd-edition/ch_04"
      ],
      "metadata": {
        "id": "9yUZHcvPHPOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Flio7DZ7pc1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "weather = pd.read_csv('data/nyc_weather_2018.csv')\n",
        "weather.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vh1P0lmx7pc3"
      },
      "source": [
        "## Querying DataFrames\n",
        "The `query()` method is an easier way of filtering based on some criteria. For example, we can use it to find all entries where snow was recorded from a station with `US1NY` in its station ID:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WSGBy44v7pc3"
      },
      "outputs": [],
      "source": [
        "snow_data = weather.query('datatype == \"SNOW\" and value > 0 and station.str.contains(\"US1NY\")',engine='python')\n",
        "snow_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weather[\n",
        "    (weather.datatype == 'SNOW')\n",
        "    & (weather.value > 0)\n",
        "    & (weather.station.str.contains('US1NY'))\n",
        "]"
      ],
      "metadata": {
        "id": "YfizCGbiIGp-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5W_qR_dJ7pc4"
      },
      "source": [
        "This is equivalent to querying the `weather.db` SQLite database for \n",
        "\n",
        "```sql\n",
        "SELECT * \n",
        "FROM weather \n",
        "WHERE datatype == \"SNOW\" AND value > 0 AND station LIKE \"%US1NY%\"\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ukMhmVJL7pc5"
      },
      "outputs": [],
      "source": [
        "import sqlite3\n",
        "\n",
        "with sqlite3.connect('data/weather.db') as connection:\n",
        "    snow_data_from_db = pd.read_sql(\n",
        "        'SELECT * FROM weather WHERE datatype == \"SNOW\" AND value > 0 and station LIKE \"%US1NY%\"', \n",
        "        connection\n",
        "    )\n",
        "\n",
        "snow_data.reset_index().drop(columns='index').equals(snow_data_from_db)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAyBHRSg7pc6"
      },
      "source": [
        "Note this is also equivalent to creating Boolean masks:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ijeBRLCX7pc6"
      },
      "outputs": [],
      "source": [
        "weather[\n",
        "    (weather.datatype == 'SNOW') \n",
        "    & (weather.value > 0)\n",
        "    & weather.station.str.contains('US1NY')\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-s-TpWj7pc7"
      },
      "source": [
        "## Merging DataFrames\n",
        "We have data for many different stations each day; however, we don't know what the stations are&mdash;just their IDs. We can join the data in the `weather_stations.csv` file which contains information from the `stations` endpoint of the NCEI API. Consult the [`0-weather_data_collection.ipynb`](./0-weather_data_collection.ipynb) notebook to see how this was collected. It looks like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GoKFFkBq7pc8"
      },
      "outputs": [],
      "source": [
        "station_info = pd.read_csv('data/weather_stations.csv')\n",
        "station_info.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqXUsK0l7pc-"
      },
      "source": [
        "As a reminder, the weather data looks like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7_XLPaRv7pc-"
      },
      "outputs": [],
      "source": [
        "weather.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "peTQNjgW7pc_"
      },
      "source": [
        "We can join our data by matching up the `station_info.id` column with the `weather.station` column. Before doing that though, let's see how many unique values we have:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-qkoqsF7pc_"
      },
      "outputs": [],
      "source": [
        "station_info.id.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvzzhIui7pdA"
      },
      "source": [
        "While `station_info` has one row per station, the `weather` dataframe has many entries per station. Notice it also has fewer uniques:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IA-jnfy57pdD"
      },
      "outputs": [],
      "source": [
        "weather.station.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuIrRJCn7pdE"
      },
      "source": [
        "When working with joins, it is important to keep an eye on the row count. Some join types will lead to data loss. Remember that we can get this with `shape`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rPXMp--Y7pdG"
      },
      "outputs": [],
      "source": [
        "station_info.shape[0], weather.shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ok1lcwHz7pdG"
      },
      "source": [
        "Since we will be doing this often, it makes more sense to write a function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DfJNjS1l7pdH"
      },
      "outputs": [],
      "source": [
        "def get_row_count(*dfs):\n",
        "    return [df.shape[0] for df in dfs]\n",
        "get_row_count(station_info, weather)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwQkdTFM7pdH"
      },
      "source": [
        "By default, `merge()` performs an inner join. We simply specify the columns to use for the join. The left dataframe is the one we call `merge()` on, and the right one is passed in as an argument:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5858hnEB7pdI"
      },
      "outputs": [],
      "source": [
        "inner_join = weather.merge(station_info, left_on='station', right_on='id')\n",
        "inner_join"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5Oj_B9j7pdI"
      },
      "source": [
        "We can remove the duplication of information in the `station` and `id` columns by renaming one of them before the merge and then simply using `on`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qY3rEMdk7pdI"
      },
      "outputs": [],
      "source": [
        "weather.merge(station_info.rename(dict(id='station'), axis=1), on='station').sample(5, random_state=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_HDRvDb7pdJ"
      },
      "source": [
        "We are losing stations that don't have weather observations associated with them, if we don't want to lose these rows, we perform a right or left join instead of the inner join:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jyG6R_1R7pdJ"
      },
      "outputs": [],
      "source": [
        "left_join = station_info.merge(weather, left_on='id', right_on='station', how='left')\n",
        "right_join = weather.merge(station_info, left_on='station', right_on='id', how='right')\n",
        "\n",
        "right_join"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkPBb1x07pdK"
      },
      "source": [
        "The left and right join as we performed above are equivalent because the side for which we kept the rows without matches was the same in both cases:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8N2f2W9R7pdK"
      },
      "outputs": [],
      "source": [
        "left_join.sort_index(axis=1).sort_values(['date', 'station'], ignore_index=True).equals(\n",
        "    right_join.sort_index(axis=1).sort_values(['date', 'station'], ignore_index=True)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ds8ryaFo7pdK"
      },
      "source": [
        "Note we have additional rows in the left and right joins because we kept all the stations that didn't have weather observations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64WYVqaJ7pdK"
      },
      "outputs": [],
      "source": [
        "get_row_count(inner_join, left_join, right_join)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8auyorN7pdM"
      },
      "source": [
        "If we query the station information for stations that have `US1NY` in their ID and perform an outer join, we can see where the mismatches occur:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6xtQYfRG7pdN"
      },
      "outputs": [],
      "source": [
        "outer_join = weather.merge(\n",
        "    station_info[station_info.id.str.contains('US1NY')], \n",
        "    left_on='station', right_on='id', how='outer', indicator=True\n",
        ")\n",
        "\n",
        "pd.concat([\n",
        "    outer_join.query(f'_merge == \"{kind}\"').sample(2, random_state=0) \n",
        "    for kind in outer_join._merge.unique()\n",
        "]).sort_index()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKmvX7cR7pdN"
      },
      "source": [
        "These joins are equivalent to their SQL counterparts. Below is the inner join. Note that to use `equals()` you will have to do some manipulation of the dataframes to line them up:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sZP80_KP7pdN"
      },
      "outputs": [],
      "source": [
        "import sqlite3\n",
        "\n",
        "with sqlite3.connect('data/weather.db') as connection:\n",
        "    inner_join_from_db = pd.read_sql(\n",
        "        'SELECT * FROM weather JOIN stations ON weather.station == stations.id', \n",
        "        connection\n",
        "    )\n",
        "\n",
        "inner_join_from_db.shape == inner_join.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGBE-xqw7pdN"
      },
      "source": [
        "Revisiting the dirty data from chapter 3's [`5-handling_data_issues.ipynb`](../ch_03/5-handling_data_issues.ipynb) notebook.\n",
        "\n",
        "Data meanings:\n",
        "- `PRCP`: precipitation in millimeters\n",
        "- `SNOW`: snowfall in millimeters\n",
        "- `SNWD`: snow depth in millimeters\n",
        "- `TMAX`: maximum daily temperature in Celsius\n",
        "- `TMIN`: minimum daily temperature in Celsius\n",
        "- `TOBS`: temperature at time of observation in Celsius\n",
        "- `WESF`: water equivalent of snow in millimeters\n",
        "\n",
        "\n",
        "Read in the data, dropping duplicates and the uninformative `SNWD` column:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wr2jDrjU7pdO"
      },
      "outputs": [],
      "source": [
        "dirty_data = pd.read_csv(\n",
        "    'data/dirty_data.csv', index_col='date'\n",
        ").drop_duplicates().drop(columns='SNWD')\n",
        "dirty_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoWrzH507pdO"
      },
      "source": [
        "We need to create two dataframes for the join. We will drop some unecessary columns as well for easier viewing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hGA9AFPC7pdO"
      },
      "outputs": [],
      "source": [
        "valid_station = dirty_data.query('station != \"?\"').drop(columns=['WESF', 'station'])\n",
        "station_with_wesf = dirty_data.query('station == \"?\"').drop(columns=['station', 'TOBS', 'TMIN', 'TMAX'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EHOW6-B7pdO"
      },
      "source": [
        "Our column for the join is the index in both dataframes, so we must specify `left_index` and `right_index`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GfKpDm8o7pdP"
      },
      "outputs": [],
      "source": [
        "valid_station.merge(\n",
        "    station_with_wesf, how='left', left_index=True, right_index=True\n",
        ").query('WESF > 0').head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "061u1GoN7pdP"
      },
      "source": [
        "The columns that existed in both dataframes, but didn't form part of the join got suffixes added to their names: `_x` for columns from the left dataframe and `_y` for columns from the right dataframe. We can customize this with the `suffixes` argument:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56k8z-fr7pdP"
      },
      "outputs": [],
      "source": [
        "valid_station.merge(\n",
        "    station_with_wesf, how='left', left_index=True, right_index=True, suffixes=('', '_?')\n",
        ").query('WESF > 0').head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duHIdZ2e7pdP"
      },
      "source": [
        "Since we are joining on the index, an easier way is to use the `join()` method instead of `merge()`. Note that the suffix parameter is now `lsuffix` for the left dataframe's suffix and `rsuffix` for the right one's:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hH_hIVt97pdP"
      },
      "outputs": [],
      "source": [
        "valid_station.join(station_with_wesf, how='left', rsuffix='_?').query('WESF > 0').head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLc7gTcR7pdP"
      },
      "source": [
        "Joins can be very resource-intensive, so it's a good idea to figure out what type of join you need using set operations before trying the join itself. The `pandas` set operations are performed on the index, so whichever columns we will be joining on will need to be the index. Let's go back to the `weather` and `station_info` dataframes and set the station ID columns as the index:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tf90pHYe7pdQ"
      },
      "outputs": [],
      "source": [
        "weather.set_index('station', inplace=True)\n",
        "station_info.set_index('id', inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIDzTYdW7pdQ"
      },
      "source": [
        "The intersection will tell us the stations that are present in both dataframes. The result will be the index when performing an inner join:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sDN8xy7c7pdQ"
      },
      "outputs": [],
      "source": [
        "weather.index.intersection(station_info.index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aoN8--L47pdQ"
      },
      "source": [
        "The set difference will tell us what we lose from each side. When performing an inner join, we lose nothing from the `weather` dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dt0mB64W7pdQ"
      },
      "outputs": [],
      "source": [
        "weather.index.difference(station_info.index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPgBEy0d7pdR"
      },
      "source": [
        "We lose 169 stations from the `station_info` dataframe, however:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X5XJ2_rk7pdS"
      },
      "outputs": [],
      "source": [
        "station_info.index.difference(weather.index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KOtRPiz7pdT"
      },
      "source": [
        "The symmetric difference tells us what we lose from both sides. It is the combination of the set differences in each direction:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "myeq2STr7pdU"
      },
      "outputs": [],
      "source": [
        "ny_in_name = station_info[station_info.index.str.contains('US1NY')]\n",
        "\n",
        "ny_in_name.index.difference(weather.index).shape[0]\\\n",
        "+ weather.index.difference(ny_in_name.index).shape[0]\\\n",
        "== weather.index.symmetric_difference(ny_in_name.index).shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLtwv2yp7pdU"
      },
      "source": [
        "The union will show us everything that will be present after a full outer join. Note that we pass in the unique values of the index to make sure we can see the number of stations we will be left with:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NyEZPkV-7pdU"
      },
      "outputs": [],
      "source": [
        "weather.index.unique().union(station_info.index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVRxJ8Fp7pdV"
      },
      "source": [
        "Note that the symmetric difference is actually the union of the set differences:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R7Ol8VFY7pdV"
      },
      "outputs": [],
      "source": [
        "ny_in_name = station_info[station_info.index.str.contains('US1NY')]\n",
        "\n",
        "ny_in_name.index.difference(weather.index).union(weather.index.difference(ny_in_name.index)).equals(\n",
        "    weather.index.symmetric_difference(ny_in_name.index)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39xiuBgu7pdW"
      },
      "source": [
        "<hr>\n",
        "<div>\n",
        "    <a href=\"../ch_03/5-handling_data_issues.ipynb\">\n",
        "        <button>&#8592; Chapter 3</button>\n",
        "    </a>\n",
        "    <a href=\"./0-weather_data_collection.ipynb\">\n",
        "        <button>Weather Data Collection</button>\n",
        "    </a>\n",
        "    <a href=\"./2-dataframe_operations.ipynb\">\n",
        "        <button style=\"float: right;\">Next Notebook &#8594;</button>\n",
        "    </a>\n",
        "</div>\n",
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4-2. dataframe operations**"
      ],
      "metadata": {
        "id": "OajZ7gqBl2fy"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fu3Nmnkc-Y5H"
      },
      "source": [
        "# DataFrame Operations\n",
        "\n",
        "## About the Data\n",
        "In this notebook, we will be working with 2 datasets:\n",
        "- Facebook's stock price throughout 2018 (obtained using the [`stock_analysis` package](https://github.com/stefmolin/stock-analysis)).\n",
        "- Daily weather data for NYC from the [National Centers for Environmental Information (NCEI) API](https://www.ncdc.noaa.gov/cdo-web/webservices/v2).\n",
        "\n",
        "*Note: The NCEI is part of the National Oceanic and Atmospheric Administration (NOAA) and, as you can see from the URL for the API, this resource was created when the NCEI was called the NCDC. Should the URL for this resource change in the future, you can search for \"NCEI weather API\" to find the updated one.*\n",
        "\n",
        "## Background on the weather data\n",
        "\n",
        "Data meanings:\n",
        "- `AWND`: average wind speed\n",
        "- `PRCP`: precipitation in millimeters\n",
        "- `SNOW`: snowfall in millimeters\n",
        "- `SNWD`: snow depth in millimeters\n",
        "- `TMAX`: maximum daily temperature in Celsius\n",
        "- `TMIN`: minimum daily temperature in Celsius\n",
        "\n",
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "2MeiMRVh-kOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/Hands-On-Data-Analysis-with-Pandas-2nd-edition/ch_04"
      ],
      "metadata": {
        "id": "hdIfyoGM-sh8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "afzghjdN-Y5O"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "weather = pd.read_csv('data/nyc_weather_2018.csv', parse_dates=['date'])\n",
        "weather.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NT42DqxV-Y5Q"
      },
      "outputs": [],
      "source": [
        "fb = pd.read_csv('data/fb_2018.csv', index_col='date', parse_dates=True)\n",
        "fb.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_NWB8dz-Y5R"
      },
      "source": [
        "## Arithmetic and statistics\n",
        "We already saw that we can use mathematical operators like `+` and `/` with dataframes directly. However, we can also use methods, which allow us to specify the axis to perform the calculation over. By default, this is per column. Let's find the Z-scores for the volume traded and look at the days where this was more than 3 standard deviations from the mean:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_REiPqm-Y5R"
      },
      "outputs": [],
      "source": [
        "fb.assign(\n",
        "    abs_z_score_volume=lambda x: \\\n",
        "        x.volume.sub(x.volume.mean()).div(x.volume.std()).abs()\n",
        ").query('abs_z_score_volume > 3')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbqIUhmy-Y5S"
      },
      "source": [
        "We can use `rank()` and `pct_change()` to see which days had the largest change in volume traded from the day before:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBTnTU6W-Y5S"
      },
      "outputs": [],
      "source": [
        "fb.assign(\n",
        "    volume_pct_change=fb.volume.pct_change(),\n",
        "    pct_change_rank=lambda x: \\\n",
        "        x.volume_pct_change.abs().rank(ascending=False)\n",
        ").nsmallest(5, 'pct_change_rank')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y32tw4vd-Y5T"
      },
      "source": [
        "January 12th was when the news that Facebook changed its news feed product to focus more on content from a users' friends over the brands they follow. Given that Facebook's advertising is a key component of its business ([nearly 89% in 2017](https://www.investopedia.com/ask/answers/120114/how-does-facebook-fb-make-money.asp)), many shares were sold and the price dropped in panic:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "seK907Vd-Y5U"
      },
      "outputs": [],
      "source": [
        "fb['2018-01-11':'2018-01-12']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLtwhi0C-Y5U"
      },
      "source": [
        "Throughout 2018, Facebook's stock price never had a low above $215:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SA0DV8gq-Y5V"
      },
      "outputs": [],
      "source": [
        "(fb > 215).any()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6p-Pr17o-Y5V"
      },
      "source": [
        "Facebook's OHLC (open, high, low, and close) prices all had at least one day they were at $215 or less:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R9fc2KEP-Y5W"
      },
      "outputs": [],
      "source": [
        "(fb > 215).all()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RAzITxe-Y5X"
      },
      "source": [
        "## Binning\n",
        "When working with volume traded, we may be interested in ranges of volume rather than the exact values. No two days have the same volume traded:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ji2lDYCy-Y5X"
      },
      "outputs": [],
      "source": [
        "(fb.volume.value_counts() > 1).sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSINvzAv-Y5X"
      },
      "source": [
        "We can use `pd.cut()` to create 3 bins of even range in volume traded and name them. Then we can work with low, medium, and high volume traded categories:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wx3wvvdS-Y5X"
      },
      "outputs": [],
      "source": [
        "volume_binned = pd.cut(fb.volume, bins=3, labels=['low', 'med', 'high'])\n",
        "volume_binned.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-DI-j2o-Y5Y"
      },
      "source": [
        "Let's look at the days with high trading volume:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fv5jq0oK-Y5Y"
      },
      "outputs": [],
      "source": [
        "fb[volume_binned == 'high'].sort_values('volume', ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6v80Rwz-Y5Z"
      },
      "source": [
        "July 25th Facebook announced disappointing user growth and the stock tanked in the after hours:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "McaMkol7-Y5Z"
      },
      "outputs": [],
      "source": [
        "fb['2018-07-25':'2018-07-26']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uk7O-bIq-Y5Z"
      },
      "source": [
        "Cambridge Analytica scandal broke on Saturday, March 17th, so we look at the Monday after for the numbers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hacQIk5B-Y5a"
      },
      "outputs": [],
      "source": [
        "fb['2018-03-16':'2018-03-20']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElR_-qut-Y5b"
      },
      "source": [
        "Since most days have similar volume, but a few are very large, we have very wide bins. Most of the data is in the low bin. \n",
        "\n",
        "*Note: visualizations will be covered in chapters 5 and 6.*"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/Hands-On-Data-Analysis-with-Pandas-2nd-edition/visual-aids/')"
      ],
      "metadata": {
        "id": "ZueX4WFWB5h2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gW_DLRJC-Y5b"
      },
      "outputs": [],
      "source": [
        "from visual_aids.misc_viz import low_med_high_bins_viz\n",
        "\n",
        "low_med_high_bins_viz(\n",
        "    fb, 'volume', ylabel='volume traded',\n",
        "    title='Daily Volume Traded of Facebook Stock in 2018 (with bins)'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSWsbIuJ-Y5b"
      },
      "source": [
        "If we split using quantiles, the bins will have roughly the same number of observations. For this, we use `qcut()`. We will make 4 quartiles:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B4sPzaiW-Y5b"
      },
      "outputs": [],
      "source": [
        "volume_qbinned = pd.qcut(fb.volume, q=4, labels=['q1', 'q2', 'q3', 'q4'])\n",
        "volume_qbinned.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQYojIsm-Y5c"
      },
      "source": [
        "Notice the bins don't cover ranges of the same size anymore:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15S70QBI-Y5c"
      },
      "outputs": [],
      "source": [
        "from visual_aids.misc_viz import quartile_bins_viz\n",
        "\n",
        "quartile_bins_viz(\n",
        "    fb, 'volume', ylabel='volume traded', \n",
        "    title='Daily Volume Traded of Facebook Stock in 2018 (with quartile bins)'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNNep3QV-Y5c"
      },
      "source": [
        "## Applying Functions\n",
        "We can use the `apply()` method to run the same operation on all columns (or rows) of the dataframe. First, let's isolate the weather observations from the Central Park station and pivot the data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kdzsWzlq-Y5c"
      },
      "outputs": [],
      "source": [
        "central_park_weather = weather\\\n",
        "    .query('station == \"GHCND:USW00094728\"')\\\n",
        "    .pivot(index='date', columns='datatype', values='value')\n",
        "\n",
        "central_park_weather"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtGgxdY2-Y5c"
      },
      "source": [
        "Let's calculate the Z-scores of the TMIN, TMAX, and PRCP observations in Central Park in October 2018:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZMCbG_V5-Y5d"
      },
      "outputs": [],
      "source": [
        "oct_weather_z_scores = central_park_weather\\\n",
        "    .loc['2018-10', ['TMIN', 'TMAX', 'PRCP']]\\\n",
        "    .apply(lambda x: x.sub(x.mean()).div(x.std()))\n",
        "oct_weather_z_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trhd_jt5-Y5d"
      },
      "source": [
        "October 27th rained much more than the rest of the days:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNqWjY98-Y5d"
      },
      "outputs": [],
      "source": [
        "oct_weather_z_scores.query('PRCP > 3').PRCP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0RYmJiiq-Y5d"
      },
      "source": [
        "Indeed, this day was much higher than the rest:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xcey1hsf-Y5d"
      },
      "outputs": [],
      "source": [
        "central_park_weather.loc['2018-10', 'PRCP'].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqeHu-Q9-Y5e"
      },
      "source": [
        "When the function we want to apply isn't vectorized, we can:\n",
        "- use `np.vectorize()` to vectorize it (similar to how `map()` works) and then use it with `apply()`\n",
        "- use `applymap()` and pass it the non-vectorized function directly\n",
        "\n",
        "Say we wanted to count the digits of the whole numbers for the Facebook data; `len()` is not vectorized, so we can use `np.vectorize()` or `applymap()`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pl0lDpNa-Y5e"
      },
      "outputs": [],
      "source": [
        "fb.apply(\n",
        "    lambda x: np.vectorize(lambda y: len(str(np.ceil(y))))(x)\n",
        ").astype('int64').equals(\n",
        "    fb.applymap(lambda x: len(str(np.ceil(x))))\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WgQJ8oX-Y5e"
      },
      "source": [
        "A simple operation of addition to each element in a series grows linearly in time complexity when using `iteritems()`, but stays near 0 when using vectorized operations. `iteritems()` and related methods should only be used if there is no vectorized solution:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svcSDYE1-Y5e"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "np.random.seed(0)\n",
        "\n",
        "vectorized_results = {}\n",
        "iteritems_results = {}\n",
        "\n",
        "for size in [10, 100, 1000, 10000, 100000, 500000, 1000000, 5000000, 10000000]:\n",
        "    # set of numbers to use\n",
        "    test = pd.Series(np.random.uniform(size=size))\n",
        "    \n",
        "    # time the vectorized operation\n",
        "    start = time.time()\n",
        "    x = test + 10\n",
        "    end = time.time()\n",
        "    vectorized_results[size] = end - start\n",
        "    \n",
        "    # time the operation with `iteritems()`\n",
        "    start = time.time()\n",
        "    x = []\n",
        "    for i, v in test.iteritems():\n",
        "        x.append(v + 10)\n",
        "    x = pd.Series(x)\n",
        "    end = time.time()\n",
        "    iteritems_results[size] = end - start\n",
        "\n",
        "results = pd.DataFrame(\n",
        "    [pd.Series(vectorized_results, name='vectorized'), pd.Series(iteritems_results, name='iteritems')]\n",
        ").T    \n",
        "\n",
        "# plotting\n",
        "ax = results.plot(title='Time Complexity', color=['blue', 'red'], legend=False)\n",
        "\n",
        "# formatting\n",
        "ax.set(xlabel='item size (rows)', ylabel='time (s)')\n",
        "ax.text(0.5e7, iteritems_results[0.5e7] * .9, 'iteritems()', rotation=34, color='red', fontsize=12, ha='center', va='bottom')\n",
        "ax.text(0.5e7, vectorized_results[0.5e7], 'vectorized', color='blue', fontsize=12, ha='center', va='bottom')\n",
        "for spine in ['top', 'right']:\n",
        "    ax.spines[spine].set_visible(False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uh1iGnI_-Y5f"
      },
      "source": [
        "## Window Calculations\n",
        "*Consult the [`understanding_window_calculations.ipynb`](./understanding_window_calculations.ipynb) notebook for interactive visualizations using widgets to help understand window calculations.*\n",
        "\n",
        "The `rolling()` method allows us to perform rolling window calculations. We simply specify the window size (3 days here) and follow it with a call to an aggregation function (sum here):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "luuOR1vy-Y5f"
      },
      "outputs": [],
      "source": [
        "central_park_weather.loc['2018-10'].assign(\n",
        "    rolling_PRCP=lambda x: x.PRCP.rolling('3D').sum()\n",
        ")[['PRCP', 'rolling_PRCP']].head(7).T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ollvFNh6-Y5f"
      },
      "source": [
        "We can also perform the rolling calculations on the entire dataframe at once. This will apply the same aggregation function to each column:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_m5QH9RG-Y5f"
      },
      "outputs": [],
      "source": [
        "central_park_weather.loc['2018-10'].rolling('3D').mean().head(7).iloc[:,:6]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXVgHMoq-Y5f"
      },
      "source": [
        "We can use different aggregation functions per column if we use `agg()` instead. We pass in a dictionary mapping the column to the aggregation to perform on it. Here, we join the result to the original data to see what is happening:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UOIqm8RS-Y5g"
      },
      "outputs": [],
      "source": [
        "central_park_weather['2018-10-01':'2018-10-07'].rolling('3D').agg(\n",
        "    {'TMAX': 'max', 'TMIN': 'min', 'AWND': 'mean', 'PRCP': 'sum'}\n",
        ").join( # join with original data for comparison\n",
        "    central_park_weather[['TMAX', 'TMIN', 'AWND', 'PRCP']], \n",
        "    lsuffix='_rolling'\n",
        ").sort_index(axis=1) # sort columns so rolling calcs are next to originals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGi-MuN6-Y5g"
      },
      "source": [
        "Suppose we reindexed the Facebook stock data as we did with the S&P 500 data in chapter 3. If we were to use rolling calculations on this data, we would be including the values when the market was closed:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SRiAXjm_-Y5g"
      },
      "outputs": [],
      "source": [
        "fb_reindexed = fb\\\n",
        "    .reindex(pd.date_range('2018-01-01', '2018-12-31', freq='D'))\\\n",
        "    .assign(\n",
        "        volume=lambda x: x.volume.fillna(0),\n",
        "        close=lambda x: x.close.fillna(method='ffill'),\n",
        "        open=lambda x: x.open.combine_first(x.close),\n",
        "        high=lambda x: x.high.combine_first(x.close),\n",
        "        low=lambda x: x.low.combine_first(x.close)\n",
        "    )\n",
        "fb_reindexed.assign(day=lambda x: x.index.day_name()).head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEvl-7BI-Y5g"
      },
      "source": [
        "As of version 1.0, `pandas` supports defining custom windows for rolling calculations, which makes it possible for us to perform rolling calculations on the days the market was open. One way is to make a new class that inherits from `BaseIndexer` and provide the logic for determining the window bounds in the `get_window_bounds()` method (more info [here](https://pandas.pydata.org/pandas-docs/stable/user_guide/computation.html#custom-window-rolling)). For our use case, we can use the `VariableOffsetWindowIndexer` class, which was introduced in version 1.1, to perform rolling calculations over non-fixed time offsets (like business days). Let's perform a three business day rolling calculation on the reindexed Facebook stock data and join it with the reindexed data for comparison:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NDlua5Ql-Y5g"
      },
      "outputs": [],
      "source": [
        "from pandas.api.indexers import VariableOffsetWindowIndexer\n",
        "\n",
        "indexer = VariableOffsetWindowIndexer(\n",
        "    index=fb_reindexed.index, offset=pd.offsets.BDay(3)\n",
        ")\n",
        "fb_reindexed.assign(window_start_day=0).rolling(indexer).agg({\n",
        "    'window_start_day': lambda x: x.index.min().timestamp(),\n",
        "    'open': 'mean', 'high': 'max', 'low': 'min',\n",
        "    'close': 'mean', 'volume': 'sum'\n",
        "}).join(\n",
        "    fb_reindexed, lsuffix='_rolling'\n",
        ").sort_index(axis=1).assign(\n",
        "    day=lambda x: x.index.day_name(),\n",
        "    window_start_day=lambda x: pd.to_datetime(x.window_start_day, unit='s')\n",
        ").head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJEefy_N-Y5h"
      },
      "source": [
        "Rolling calculations (`rolling()`) use a sliding window. Expanding calculations (`expanding()`), however, grow in size. These are equivalent to cumulative aggregations like `cumsum()`; however, we can specify the minimum number of periods required to start calculating (default is 1), and we aren't limited to predefined aggregations. Therefore, while there is no method for the cumulative mean, we can calculate it using `expanding()`. Let's calculate the month-to-date average precipiation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HMwQNu0U-Y5h"
      },
      "outputs": [],
      "source": [
        "central_park_weather.loc['2018-06'].assign(\n",
        "    TOTAL_PRCP=lambda x: x.PRCP.cumsum(),\n",
        "    AVG_PRCP=lambda x: x.PRCP.expanding().mean()\n",
        ").head(10)[['PRCP', 'TOTAL_PRCP', 'AVG_PRCP']].T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58loUtrM-Y5h"
      },
      "source": [
        "We can also use `agg()` to specify aggregations per column. Note that this works with NumPy functions as well. Here, we join the expanding calculations with the original results for comparison:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r9t93kF--Y5h"
      },
      "outputs": [],
      "source": [
        "central_park_weather['2018-10-01':'2018-10-07'].expanding().agg(\n",
        "    {'TMAX': np.max, 'TMIN': np.min, 'AWND': np.mean, 'PRCP': np.sum}\n",
        ").join(\n",
        "    central_park_weather[['TMAX', 'TMIN', 'AWND', 'PRCP']], \n",
        "    lsuffix='_expanding'\n",
        ").sort_index(axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Am562EHL-Y5i"
      },
      "source": [
        "Pandas provides the `ewm()` method for exponentially weighted moving calculations. As we saw in chapter 1, we can use the exponentially weighted moving average to smooth the data. Let's compare the rolling mean to the exponentially weighted moving average with the maximum daily temperature. Note that `span` here is the periods to use:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NlqDN9m4-Y5i"
      },
      "outputs": [],
      "source": [
        "central_park_weather.assign(\n",
        "    AVG=lambda x: x.TMAX.rolling('30D').mean(),\n",
        "    EWMA=lambda x: x.TMAX.ewm(span=30).mean()\n",
        ").loc['2018-09-29':'2018-10-08', ['TMAX', 'EWMA', 'AVG']].T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDwgAoJU-Y5i"
      },
      "source": [
        "*Consult the [`understanding_window_calculations.ipynb`](./understanding_window_calculations.ipynb) notebook for interactive visualizations to help understand window calculations.*\n",
        "\n",
        "## Pipes\n",
        "Pipes are a way to streamline our `pandas` code and make it more readable and flexible. Using pipes, we can take a nested call like \n",
        "\n",
        "```python\n",
        "f(g(h(data), 20), x=True)\n",
        "```\n",
        "\n",
        "and turn it into something more readable:\n",
        "\n",
        "```python\n",
        "data.pipe(h)\\\n",
        "    .pipe(g, 20)\\\n",
        "    .pipe(f, x=True)\\\n",
        "```\n",
        "\n",
        "We can use pipes to apply any function that accepts our data as the first argument and pass in any additional arguments. This makes it easy to chain steps together regardless of whether they are methods or functions:\n",
        "\n",
        "We can pass any function that will accept the caller of `pipe()` as the first argument:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kfJB958i-Y5i"
      },
      "outputs": [],
      "source": [
        "def get_info(df):\n",
        "    return '%d rows, %d columns and max closing Z-score was %d' % (*df.shape, df.close.max())\n",
        "\n",
        "get_info(fb.loc['2018-Q1'].apply(lambda x: (x - x.mean())/x.std()))\\\n",
        "    == fb.loc['2018-Q1'].apply(lambda x: (x - x.mean())/x.std()).pipe(get_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51SwRjtb-Y5i"
      },
      "source": [
        "For example, passing `pd.DataFrame.rolling` to `pipe()` is equivalent to calling `rolling()` directly on the dataframe, except we have more flexiblity to change this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLNl5efy-Y5j"
      },
      "outputs": [],
      "source": [
        "fb.pipe(pd.DataFrame.rolling, '20D').mean().equals(fb.rolling('20D').mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZGBD1df-Y5j"
      },
      "source": [
        "The pipe takes the function passed in and calls it with the object that called `pipe()` as the first argument. Positional and keyword arguments are passed down:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7B9Y2KF-Y5j"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame.rolling(fb, '20D').mean().equals(fb.rolling('20D').mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUSXy6Xq-Y5j"
      },
      "source": [
        "We can use a pipe to make a function that we can use for all of our window calculation needs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_c2t2X9-Y5j"
      },
      "outputs": [],
      "source": [
        "from window_calc import window_calc\n",
        "window_calc??"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUpSXEdi-Y5k"
      },
      "source": [
        "We can use the same interface to calculate various window calculations now. Let's find the expanding median for the Facebook data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQsP8VC6-Y5k"
      },
      "outputs": [],
      "source": [
        "window_calc(fb, pd.DataFrame.expanding, np.median).head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtX9fAjD-Y5k"
      },
      "source": [
        "Using the exponentially weighted moving average requires we pass in a keyword argument:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AwkxhFzm-Y5k"
      },
      "outputs": [],
      "source": [
        "window_calc(fb, pd.DataFrame.ewm, 'mean', span=3).head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLBDSmH5-Y5k"
      },
      "source": [
        "With rolling calculations, we can pass in a positional argument for the window size:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fm6k2jdI-Y5l"
      },
      "outputs": [],
      "source": [
        "window_calc(\n",
        "    central_park_weather.loc['2018-10'], \n",
        "    pd.DataFrame.rolling, \n",
        "    {'TMAX': 'max', 'TMIN': 'min', 'AWND': 'mean', 'PRCP': 'sum'},\n",
        "    '3D'\n",
        ").head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWHRZy3k-Y5l"
      },
      "source": [
        "<hr>\n",
        "<div style=\"overflow: hidden; margin-bottom: 10px;\">\n",
        "    <div style=\"float: left;\">\n",
        "         <a href=\"./1-querying_and_merging.ipynb\">\n",
        "            <button>&#8592; Previous Notebook</button>\n",
        "        </a>\n",
        "        <a href=\"./understanding_window_calculations.ipynb\">\n",
        "            <button>Understanding Window Calculations</button>\n",
        "        </a>\n",
        "    </div>\n",
        "    <div style=\"float: right;\">\n",
        "        <a href=\"./3-aggregations.ipynb\">\n",
        "            <button>Next Notebook &#8594;</button>\n",
        "        </a>\n",
        "    </div>\n",
        "</div>\n",
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4-3. aggregations**"
      ],
      "metadata": {
        "id": "x8nAMZPNl87f"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCFQ1WPjIaT8"
      },
      "source": [
        "# Aggregating data with pandas and numpy\n",
        "\n",
        "## About the Data\n",
        "In this notebook, we will be working with 2 datasets:\n",
        "- Facebook's stock price throughout 2018 (obtained using the [`stock_analysis` package](https://github.com/stefmolin/stock-analysis)).\n",
        "- daily weather data for NYC from the [National Centers for Environmental Information (NCEI) API](https://www.ncdc.noaa.gov/cdo-web/webservices/v2).\n",
        "\n",
        "*Note: The NCEI is part of the National Oceanic and Atmospheric Administration (NOAA) and, as you can see from the URL for the API, this resource was created when the NCEI was called the NCDC. Should the URL for this resource change in the future, you can search for \"NCEI weather API\" to find the updated one.*\n",
        "\n",
        "## Background on the weather data\n",
        "\n",
        "Data meanings:\n",
        "- `AWND`: average wind speed\n",
        "- `PRCP`: precipitation in millimeters\n",
        "- `SNOW`: snowfall in millimeters\n",
        "- `SNWD`: snow depth in millimeters\n",
        "- `TMAX`: maximum daily temperature in Celsius\n",
        "- `TMIN`: minimum daily temperature in Celsius\n",
        "\n",
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "L-QksXNbIbzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/Hands-On-Data-Analysis-with-Pandas-2nd-edition/ch_04"
      ],
      "metadata": {
        "id": "dG8fcVInIeDi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2qL0nd_bIaUD"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "fb = pd.read_csv('data/fb_2018.csv', index_col='date', parse_dates=True).assign(\n",
        "    trading_volume=lambda x: pd.cut(x.volume, bins=3, labels=['low', 'med', 'high'])\n",
        ")\n",
        "fb.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJooOTzDIaUG"
      },
      "outputs": [],
      "source": [
        "weather = pd.read_csv('data/weather_by_station.csv', index_col='date', parse_dates=True)\n",
        "weather.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMUdBC9NIaUH"
      },
      "source": [
        "Before we dive into any calculations, let's make sure `pandas` won't put things in scientific notation. We will modify how floats are formatted for displaying. The format we will apply is `.2f`, which will provide the float with 2 digits after the decimal point:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rz1vBoAMIaUI"
      },
      "outputs": [],
      "source": [
        "pd.set_option('display.float_format', lambda x: '%.2f' % x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6yHnaEIIaUJ"
      },
      "source": [
        "## Summarizing DataFrames\n",
        "We learned about `agg()` in the [`2-dataframe_operations.ipynb`](./2-dataframe_operations.ipynb) notebook when we learned about window calculations; however, we can call this on the dataframe directly to aggregate its contents into a single series:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uFJBfipCIaUK"
      },
      "outputs": [],
      "source": [
        "fb.agg({\n",
        "    'open': np.mean, \n",
        "    'high': np.max, \n",
        "    'low': np.min, \n",
        "    'close': np.mean, \n",
        "    'volume': np.sum\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHkm2xZDIaUM"
      },
      "source": [
        "We can use this to find the total snowfall and precipitation recorded in Central Park in 2018:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oMA1qcq3IaUN"
      },
      "outputs": [],
      "source": [
        "weather.query('station == \"GHCND:USW00094728\"')\\\n",
        "    .pivot(columns='datatype', values='value')[['SNOW', 'PRCP']]\\\n",
        "    .sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UK9CGwf-IaUP"
      },
      "source": [
        "This is equivalent to passing `'sum'` to `agg()`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dUnKOP8fIaUQ"
      },
      "outputs": [],
      "source": [
        "weather.query('station == \"GHCND:USW00094728\"')\\\n",
        "    .pivot(columns='datatype', values='value')[['SNOW', 'PRCP']]\\\n",
        "    .agg('sum')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGqYJnedIaUR"
      },
      "source": [
        "Note that we aren't limited to providing a single aggregation per column. We can pass a list, and we will get a dataframe back instead of a series. Null values are placed where we don't have a calculation result to display:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-iBAwLMLIaUR"
      },
      "outputs": [],
      "source": [
        "fb.agg({\n",
        "    'open': 'mean',\n",
        "    'high': ['min', 'max'],\n",
        "    'low': ['min', 'max'],\n",
        "    'close': 'mean'\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "og2zuauCIaUS"
      },
      "source": [
        "## Using `groupby()`\n",
        "Often we won't want to aggregate on the entire dataframe, but on groups within it. For this purpose, we can run `groupby()` before the aggregation. If we group by the `trading_volume` column, we will get a row for each of the values it takes on:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31vcAHuzIaUT"
      },
      "outputs": [],
      "source": [
        "fb.groupby('trading_volume').mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9_IKU9jIaUT"
      },
      "source": [
        "After we call `groupby()`, we can still select columns for aggregation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CWTcXw1TIaUT"
      },
      "outputs": [],
      "source": [
        "fb.groupby('trading_volume')['close'].agg(['min', 'max', 'mean'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FjBaYrPIaUU"
      },
      "source": [
        "We can still provide a dictionary specifying the aggregations to perform, but passing a list for a column will result in a hierarchical index for the columns:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Woq3kwXIaUV"
      },
      "outputs": [],
      "source": [
        "fb_agg = fb.groupby('trading_volume').agg({\n",
        "    'open': 'mean',\n",
        "    'high': ['min', 'max'],\n",
        "    'low': ['min', 'max'],\n",
        "    'close': 'mean'\n",
        "})\n",
        "fb_agg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fc2QT022IaUW"
      },
      "source": [
        "The hierarchical index in the columns looks like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tkOzTKmzIaUW"
      },
      "outputs": [],
      "source": [
        "fb_agg.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jsb7S_vFIaUX"
      },
      "source": [
        "Using a list comprehension, we can join the levels (in a tuple) with an `_` at each iteration: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fzSiDiocIaUX"
      },
      "outputs": [],
      "source": [
        "fb_agg.columns = ['_'.join(col_agg) for col_agg in fb_agg.columns]\n",
        "fb_agg.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJq8dObdIaUX"
      },
      "source": [
        "We can group on values in the index if we tell `groupby()`, which `level` to use:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rciChcguIaUY"
      },
      "outputs": [],
      "source": [
        "weather.loc['2018-10'].query('datatype == \"PRCP\"')\\\n",
        "    .groupby(level=0).mean().head().squeeze()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLBNSp1BIaUY"
      },
      "source": [
        "We can also create a `Grouper` object, which can also roll up the datetimes in the index. Here, we find the quarterly total precipitation per station:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQRQ11feIaUY"
      },
      "outputs": [],
      "source": [
        "weather.query('datatype == \"PRCP\"').groupby(\n",
        "    ['station_name', pd.Grouper(freq='Q')]\n",
        ").sum().unstack().sample(5, random_state=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zosuDDCqIaUY"
      },
      "source": [
        "Note that we can use `filter()` to exclude some groups from aggregation. Here, we only keep groups with names ending in \"NY US\" in the group's `name` attribute, which is the station name in this case:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EPM_CEWPIaUZ"
      },
      "outputs": [],
      "source": [
        "weather.groupby('station_name').filter( # station names with \"NY US\" in them\n",
        "    lambda x: x.name.endswith('NY US')\n",
        ").query('datatype == \"SNOW\"').groupby('station_name').sum().squeeze() # aggregate and make a series (squeeze)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCnvfxbSIaUZ"
      },
      "source": [
        "Let's see which months have the most precipitation. First, we need to group by day and average the precipitation across the stations. Then we can group by month and sum the resulting precipitation. We use `nlargest()` to give the 5 months with the most precipitation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TAOecf1DIaUZ"
      },
      "outputs": [],
      "source": [
        "weather.query('datatype == \"PRCP\"')\\\n",
        "    .groupby(level=0).mean()\\\n",
        "    .groupby(pd.Grouper(freq='M')).sum().value.nlargest()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8mVg4feIaUa"
      },
      "source": [
        "Perhaps the previous result was surprising. The saying goes \"April showers bring May flowers\"; yet April wasn't in the top 5 (neither was May for that matter). Snow will count towards precipitation, but that doesn't explain why summer months are higher than April. Let's look for days that accounted for a large percentage of the precipitation in a given month. \n",
        "\n",
        "In order to do so, we need to calculate the average daily precipitation across stations and then find the total per month. This will be the denominator. However, in order to divide the daily values by the total for their month, we will need a series of equal dimensions. This means we will need to use `transform()`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rkj9ycvhIaUa"
      },
      "outputs": [],
      "source": [
        "weather.query('datatype == \"PRCP\"')\\\n",
        "    .rename(dict(value='prcp'), axis=1)\\\n",
        "    .groupby(level=0).mean()\\\n",
        "    .groupby(pd.Grouper(freq='M'))\\\n",
        "    .transform(np.sum)['2018-01-28':'2018-02-03']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8_uW7E-IaUa"
      },
      "source": [
        "Notice how we have the same value repeated for each day in the month it belongs to. This will allow us to calculate the percentage of the monthly precipitation that occurred each day and then pull out the largest values:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "la2iJuwuIaUb"
      },
      "outputs": [],
      "source": [
        "weather\\\n",
        "    .query('datatype == \"PRCP\"')\\\n",
        "    .rename(dict(value='prcp'), axis=1)\\\n",
        "    .groupby(level=0).mean()\\\n",
        "    .assign(\n",
        "        total_prcp_in_month=lambda x: \\\n",
        "            x.groupby(pd.Grouper(freq='M')).transform(np.sum),\n",
        "        pct_monthly_prcp=lambda x: \\\n",
        "            x.prcp.div(x.total_prcp_in_month)\n",
        "    )\\\n",
        "    .nlargest(5, 'pct_monthly_prcp')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PruGS_umIaUb"
      },
      "source": [
        "`transform()` can be used on dataframes as well. We can use it to easily standardize the data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uVxzeZKfIaUb"
      },
      "outputs": [],
      "source": [
        "fb[['open', 'high', 'low', 'close']]\\\n",
        "    .transform(lambda x: (x - x.mean()).div(x.std()))\\\n",
        "    .head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99CGH5D5IaUc"
      },
      "source": [
        "## Pivot tables and crosstabs\n",
        "We saw pivots in [`ch_03/4-reshaping_data.ipynb`](../ch_03/4-reshaping_data.ipynb); however, we weren't able to provide any aggregations. With `pivot_table()`, we get the mean by default. In its simplest form, we provide a column to place along the columns:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "052YIxJiIaUc"
      },
      "outputs": [],
      "source": [
        "fb.pivot_table(columns='trading_volume')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZBFm4WYIaUc"
      },
      "source": [
        "By placing the trading volume in the index, we get the transpose:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R1b2wJOLIaUc"
      },
      "outputs": [],
      "source": [
        "fb.pivot_table(index='trading_volume')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9d9CJ3l7IaUd"
      },
      "source": [
        "With `pivot()`, we also weren't able to handle multi-level indices or indices with repeated values. For this reason we haven't been able to put the weather data in the wide format. The `pivot_table()` method solves this issue:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DwN8qS-SIaUd"
      },
      "outputs": [],
      "source": [
        "weather.reset_index().pivot_table(\n",
        "    index=['date', 'station', 'station_name'], \n",
        "    columns='datatype', \n",
        "    values='value',\n",
        "    aggfunc='median'\n",
        ").reset_index().tail()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuk7sMXFIaUe"
      },
      "source": [
        "We can use the `pd.crosstab()` function to create a frequency table. For example, if we want to see how many low-, medium-, and high-volume trading days Facebook stock had each month, we can use crosstab:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Voa8qKNeIaUe"
      },
      "outputs": [],
      "source": [
        "pd.crosstab(\n",
        "    index=fb.trading_volume,\n",
        "    columns=fb.index.month,\n",
        "    colnames=['month'] # name the columns index\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glhqgqjjIaUf"
      },
      "source": [
        "We can normalize with the row or column totals with the `normalize` parameter. This shows percentage of the total:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nJMgHiUXIaUf"
      },
      "outputs": [],
      "source": [
        "pd.crosstab(\n",
        "    index=fb.trading_volume,\n",
        "    columns=fb.index.month,\n",
        "    colnames=['month'],\n",
        "    normalize='columns'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksPqMuM0IaUf"
      },
      "source": [
        "If we want to perform a calculation other than counting the frequency, we can pass the column to run the calculation on to `values` and the function to use to `aggfunc`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tNCLAhLlIaUg"
      },
      "outputs": [],
      "source": [
        "pd.crosstab(\n",
        "    index=fb.trading_volume,\n",
        "    columns=fb.index.month,\n",
        "    colnames=['month'],\n",
        "    values=fb.close,\n",
        "    aggfunc=np.mean\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJU7CGHyIaUg"
      },
      "source": [
        "We can also get row and column subtotals with the `margins` parameter. Let's count the number of times each station recorded snow per month and include the subtotals:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IukkkVW4IaUh"
      },
      "outputs": [],
      "source": [
        "snow_data = weather.query('datatype == \"SNOW\"')\n",
        "pd.crosstab(\n",
        "    index=snow_data.station_name,\n",
        "    columns=snow_data.index.month,\n",
        "    colnames=['month'],\n",
        "    values=snow_data.value,\n",
        "    aggfunc=lambda x: (x > 0).sum(),\n",
        "    margins=True, # show row and column subtotals\n",
        "    margins_name='total observations of snow' # name the subtotals\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kF-9gcglIaUh"
      },
      "source": [
        "<hr>\n",
        "<div>\n",
        "    <a href=\"./2-dataframe_operations.ipynb\">\n",
        "        <button>&#8592; Previous Notebook</button>\n",
        "    </a>\n",
        "    <a href=\"./4-time_series.ipynb\">\n",
        "        <button style=\"float: right;\">Next Notebook &#8594;</button>\n",
        "    </a>\n",
        "</div>\n",
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4-4. time series**"
      ],
      "metadata": {
        "id": "aT4oi2demDWF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34dpejn1lYq7"
      },
      "source": [
        "# Working with Time Series Data\n",
        "\n",
        "## About the Data\n",
        "In this notebook, we will be working with 5 datasets:\n",
        "- (CSV) Facebook's stock price daily throughout 2018 (obtained using the [`stock_analysis` package](https://github.com/stefmolin/stock-analysis)).\n",
        "- (CSV) Facebook's OHLC stock data from May 20, 2019 - May 24, 2019 per minute from [Nasdaq.com](https://old.nasdaq.com/symbol/fb/interactive-chart).\n",
        "- (CSV) melted stock data for Facebook from May 20, 2019 - May 24, 2019 per minute from [Nasdaq.com](https://old.nasdaq.com/symbol/fb/interactive-chart).\n",
        "- (DB) stock opening prices by the minute for Apple from May 20, 2019 - May 24, 2019 altered to have seconds in the time from [Nasdaq.com](https://old.nasdaq.com/symbol/aapl/interactive-chart).\n",
        "- (DB) stock opening prices by the minute for Facebook from May 20, 2019 - May 24, 2019 from [Nasdaq.com](https://old.nasdaq.com/symbol/fb/interactive-chart).\n",
        "\n",
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "la5uZQwdlYrA"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "fb = pd.read_csv('data/fb_2018.csv', index_col='date', parse_dates=True).assign(\n",
        "    trading_volume=lambda x: pd.cut(x.volume, bins=3, labels=['low', 'med', 'high'])\n",
        ")\n",
        "fb.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuUptxq7lYrD"
      },
      "source": [
        "## Time-based selection and filtering\n",
        "Remember, when we have an index of type `DatetimeIndex`, we can use datetime slicing. We can provide a range of dates. We only get three days back because the stock market is closed on the weekends:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y3_QGs5mlYrF"
      },
      "outputs": [],
      "source": [
        "fb['2018-10-11':'2018-10-15']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A31_v9rflYrH"
      },
      "source": [
        "We can select ranges of months and quarters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QnPE4JiSlYrI"
      },
      "outputs": [],
      "source": [
        "fb.loc['2018-q1'].equals(fb['2018-01':'2018-03'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaxppDfUlYrI"
      },
      "source": [
        "The `first()` method will give us a specified length of time from the beginning of the time series. Here, we ask for a week. January 1, 2018 was a holiday—meaning the market was closed. It was also a Monday, so the week here is only four days:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bb-OzroSlYrJ"
      },
      "outputs": [],
      "source": [
        "fb.first('1W')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWolNTVGlYrL"
      },
      "source": [
        "The `last()` method will take from the end:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JWKoHHYwlYrM"
      },
      "outputs": [],
      "source": [
        "fb.last('1W')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSrLvYBilYrM"
      },
      "source": [
        "Suppose that we reindexed the Facebook stock data to include all dates for 2018. We would have null entries for January 1st:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9hG4aS_blYrN"
      },
      "outputs": [],
      "source": [
        "fb_reindexed = fb.reindex(pd.date_range('2018-01-01', '2018-12-31', freq='D'))\n",
        "fb_reindexed.first('1D').isna().squeeze().all()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yO7Fhu52lYrO"
      },
      "source": [
        "We can use `first_valid_index()` to give us the index of the first non-null entry in our data, which is the first day the market was open in Q1 2018:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WfIN990ElYrP"
      },
      "outputs": [],
      "source": [
        "fb_reindexed.loc['2018-Q1'].first_valid_index()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEbN8UiylYrQ"
      },
      "source": [
        "Conversely, we can use `last_valid_index()` to get the last entry of non-null data. For Q1 2018, this is March 29th:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "liKdyzIolYrQ"
      },
      "outputs": [],
      "source": [
        "fb_reindexed.loc['2018-Q1'].last_valid_index()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jULrjv7alYrR"
      },
      "source": [
        "We can use `asof()` to find the last non-null data before the point we are looking for. If we ask for March 31st, we will get the data from the index we got from `fb_reindexed.loc['2018-Q1'].last_valid_index()`, which was March 29th. Note that this works regardless of whether we reindexed:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xCOshV5wlYrb"
      },
      "outputs": [],
      "source": [
        "fb_reindexed.asof('2018-03-31')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0W1jqOjlYrh"
      },
      "source": [
        "For the next few examples, we need datetimes, so we will read in the stock data per minute file:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "39xLHrGplYrj"
      },
      "outputs": [],
      "source": [
        "stock_data_per_minute = pd.read_csv(\n",
        "    'data/fb_week_of_may_20_per_minute.csv', index_col='date', parse_dates=True, \n",
        "    date_parser=lambda x: pd.to_datetime(x, format='%Y-%m-%d %H-%M')\n",
        ")\n",
        "\n",
        "stock_data_per_minute.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZbgsFKSlYru"
      },
      "source": [
        "We can use a `Grouper` object to roll up our data to the daily level along with `first` and `last`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tnwKh0Q5lYrv"
      },
      "outputs": [],
      "source": [
        "stock_data_per_minute.groupby(pd.Grouper(freq='1D')).agg({\n",
        "    'open': 'first',\n",
        "    'high': 'max', \n",
        "    'low': 'min', \n",
        "    'close': 'last', \n",
        "    'volume': 'sum'\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TgLwxF4lYrw"
      },
      "source": [
        "The `at_time()` method allows us to pull out all datetimes that match a certain time. Here, we can grab all the rows from the time the stock market opens (9:30 AM):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_4jFzyS3lYrx"
      },
      "outputs": [],
      "source": [
        "stock_data_per_minute.at_time('9:30')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hw16taGblYry"
      },
      "source": [
        "We can use `between_time()` to grab data for the last two minutes of trading daily:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fTCA02DMlYrz"
      },
      "outputs": [],
      "source": [
        "stock_data_per_minute.between_time('15:59', '16:00')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLUDI83flYrz"
      },
      "source": [
        "On average, are more shares traded within the first 30 minutes of trading or in the last 30 minutes? We can combine `between_time()` with `group_by()` and `filter()` from the [`3-aggregations.ipynb`](./3-aggregations.ipynb) notebook to answer this question. For the week in question, more are traded on average around opening time than closing time:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQl0Wih7lYrz"
      },
      "outputs": [],
      "source": [
        "shares_traded_in_first_30_min = stock_data_per_minute\\\n",
        "    .between_time('9:30', '10:00')\\\n",
        "    .groupby(pd.Grouper(freq='1D'))\\\n",
        "    .filter(lambda x: (x.volume > 0).all())\\\n",
        "    .volume.mean()\n",
        "\n",
        "shares_traded_in_last_30_min = stock_data_per_minute\\\n",
        "    .between_time('15:30', '16:00')\\\n",
        "    .groupby(pd.Grouper(freq='1D'))\\\n",
        "    .filter(lambda x: (x.volume > 0).all())\\\n",
        "    .volume.mean()\n",
        "\n",
        "shares_traded_in_first_30_min - shares_traded_in_last_30_min"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "El6a3c-0lYr0"
      },
      "source": [
        "In cases where time doesn't matter, we can normalize the times to midnight:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4yojBD_GlYr1"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(\n",
        "    dict(before=stock_data_per_minute.index, after=stock_data_per_minute.index.normalize())\n",
        ").head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SW7bFx5_lYr3"
      },
      "source": [
        "Note that we can also use `normalize()` on a `Series` object after accessing the `dt` attribute:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LghZaXAylYr5"
      },
      "outputs": [],
      "source": [
        "stock_data_per_minute.index.to_series().dt.normalize().head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vvm4QMPJlYr6"
      },
      "source": [
        "## Shifting for lagged data\n",
        "We can use `shift()` to create lagged data. By default, the shift will be one period. For example, we can use `shift()` to create a new column that indicates the previous day's closing price. From this new column, we can calculate the price change due to after-hours trading (after the close one day right up to the open the following day):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_puPKlb2lYr7"
      },
      "outputs": [],
      "source": [
        "fb.assign(\n",
        "    prior_close=lambda x: x.close.shift(),\n",
        "    after_hours_change_in_price=lambda x: x.open - x.prior_close,\n",
        "    abs_change=lambda x: x.after_hours_change_in_price.abs()\n",
        ").nlargest(5, 'abs_change')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYLUuYKtlYr7"
      },
      "source": [
        "If the goal is to to add/subtract time, we can use `pd.Timedelta` objects instead:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CWKVWLs6lYr9"
      },
      "outputs": [],
      "source": [
        "pd.date_range('2018-01-01', freq='D', periods=5) + pd.Timedelta('9 hours 30 minutes')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtJjIQrRlYr-"
      },
      "source": [
        "## Differenced data\n",
        "Using the `diff()` method is a quick way to calculate the difference between the data and a lagged version of itself. By default, it will yield the result of `data - data.shift()`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FePwFbjElYr_"
      },
      "outputs": [],
      "source": [
        "(\n",
        "    fb.drop(columns='trading_volume') \n",
        "    - fb.drop(columns='trading_volume').shift()\n",
        ").equals(\n",
        "    fb.drop(columns='trading_volume').diff()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ST5qf1gwlYsB"
      },
      "source": [
        "We can use this to see how Facebook stock changed day-over-day:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q2j8oftQlYsC"
      },
      "outputs": [],
      "source": [
        "fb.drop(columns='trading_volume').diff().head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsNgvPZslYsC"
      },
      "source": [
        "We can specify the number of periods, can be any positive or negative integer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QbdXdnwjlYsD"
      },
      "outputs": [],
      "source": [
        "fb.drop(columns='trading_volume').diff(-3).head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGIn0BEKlYsJ"
      },
      "source": [
        "## Resampling\n",
        "Sometimes the data is at a granularity that isn't conducive to our analysis. Consider the case where we have data per minute for the full year of 2018. Let's see what happens if we try to plot this, and then look at the daily aggregation of this data.\n",
        "\n",
        "*Plotting will be covered next chapter.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z6NK82uFlYsK"
      },
      "outputs": [],
      "source": [
        "from visual_aids.misc_viz import resampling_example\n",
        "resampling_example()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01jXv8DnlYsM"
      },
      "source": [
        "The plot on the left has so much data we can't see anything. However, when we aggregate to the daily totals, we see the data. We can alter the granularity of the data we are working with using resampling. Recall our minute-by-minute stock data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4C_-Dn2glYsM"
      },
      "outputs": [],
      "source": [
        "stock_data_per_minute.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLjBE1PPlYsO"
      },
      "source": [
        "We can resample this to get to a daily frequency:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9pqGSr1SlYsO"
      },
      "outputs": [],
      "source": [
        "stock_data_per_minute.resample('1D').agg({\n",
        "    'open': 'first',\n",
        "    'high': 'max', \n",
        "    'low': 'min', \n",
        "    'close': 'last', \n",
        "    'volume': 'sum'\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kanASUrNlYsP"
      },
      "source": [
        "We can downsample to quarterly data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RjhsF7yflYsP"
      },
      "outputs": [],
      "source": [
        "fb.resample('Q').mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7J4eD-2UlYsQ"
      },
      "source": [
        "We can also use `apply()`. Here, we show the quarterly change from start to end:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EkzPt4pVlYsR"
      },
      "outputs": [],
      "source": [
        "fb.drop(columns='trading_volume').resample('Q').apply(\n",
        "    lambda x: x.last('1D').values - x.first('1D').values\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHQd2e4YlYsS"
      },
      "source": [
        "Consider the following melted stock data by the minute. We don't see the OHLC data directly:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVgQ0sP0lYsS"
      },
      "outputs": [],
      "source": [
        "melted_stock_data = pd.read_csv('data/melted_stock_data.csv', index_col='date', parse_dates=True)\n",
        "melted_stock_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufM7lCaHlYsT"
      },
      "source": [
        "We can use the `ohlc()` method after resampling to recover the OHLC columns:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xlusROBzlYsT"
      },
      "outputs": [],
      "source": [
        "melted_stock_data.resample('1D').ohlc()['price']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vac-bkkxlYsU"
      },
      "source": [
        "Alternatively, we can upsample to increase the granularity. Note this will introduce `NaN` values:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fZ9qp0IMlYsW"
      },
      "outputs": [],
      "source": [
        "fb.resample('6H').asfreq().head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLUIexv7lYsW"
      },
      "source": [
        "There are many ways to handle these `NaN` values. We can forward-fill with `pad()`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5BRoUsGlYsX"
      },
      "outputs": [],
      "source": [
        "fb.resample('6H').pad().head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BR3tcfJWlYsX"
      },
      "source": [
        "We can specify a specific value or a method with `fillna()`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CV0KAbh8lYsZ"
      },
      "outputs": [],
      "source": [
        "fb.resample('6H').fillna('nearest').head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5HC_MvTlYsZ"
      },
      "source": [
        "We can use `asfreq()` and `assign()` to specify the action per column:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ec178WdTlYsa"
      },
      "outputs": [],
      "source": [
        "fb.resample('6H').asfreq().assign(\n",
        "    volume=lambda x: x.volume.fillna(0), # put 0 when market is closed\n",
        "    close=lambda x: x.close.fillna(method='ffill'), # carry forward\n",
        "    # take the closing price if these aren't available\n",
        "    open=lambda x: x.open.combine_first(x.close),\n",
        "    high=lambda x: x.high.combine_first(x.close),\n",
        "    low=lambda x: x.low.combine_first(x.close)\n",
        ").head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOaY1QVolYsb"
      },
      "source": [
        "## Merging\n",
        "We saw merging examples the [`1-querying_and_merging.ipynb`](./1-querying_and_merging.ipynb) notebook. However, they all matched based on keys. With time series, it is possible that they are so granular that we never have the same time for multiple entries. Let's work with some stock data at different granularities:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g9L5ST3clYsb"
      },
      "outputs": [],
      "source": [
        "import sqlite3\n",
        "\n",
        "with sqlite3.connect('data/stocks.db') as connection:\n",
        "    fb_prices = pd.read_sql(\n",
        "        'SELECT * FROM fb_prices', connection, \n",
        "        index_col='date', parse_dates=['date']\n",
        "    )\n",
        "    aapl_prices = pd.read_sql(\n",
        "        'SELECT * FROM aapl_prices', connection, \n",
        "        index_col='date', parse_dates=['date']\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovtbfQFhlYsb"
      },
      "source": [
        "The Facebook prices are at the minute granularity:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eOzKpwB3lYsc"
      },
      "outputs": [],
      "source": [
        "fb_prices.index.second.unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xnsa7rdulYsc"
      },
      "source": [
        "However, the Apple prices have information for the second:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YP9qUlVclYsd"
      },
      "outputs": [],
      "source": [
        "aapl_prices.index.second.unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6s96_fylYsd"
      },
      "source": [
        "We can perform an *as of* merge to try to line these up the best we can. We specify how to handle the mismatch with the `direction` and `tolerance` parameters. We will fill in with the `direction` of `nearest` and a `tolerance` of 30 seconds. This will place the Apple data with the minute that it is closest to, so 9:31:52 will go with 9:32 and 9:37:07 will go with 9:37. Since the times are on the index, we pass in `left_index` and `right_index`, as we did with `merge()` earlier this chapter: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OA72nsi0lYsd"
      },
      "outputs": [],
      "source": [
        "pd.merge_asof(\n",
        "    fb_prices, aapl_prices, \n",
        "    left_index=True, right_index=True, # datetimes are in the index\n",
        "    # merge with nearest minute\n",
        "    direction='nearest', tolerance=pd.Timedelta(30, unit='s')\n",
        ").head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsiygU8UlYse"
      },
      "source": [
        "If we don't want to lose the seconds information with the Apple data, we can use `pd.merge_ordered()` instead, which will interleave the two. Note this is an outer join by default (`how` parameter). The only catch here is that we need to reset the index in order to join on it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JM1huLP2lYse"
      },
      "outputs": [],
      "source": [
        "pd.merge_ordered(\n",
        "    fb_prices.reset_index(), aapl_prices.reset_index()\n",
        ").set_index('date').head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBFcho25lYsf"
      },
      "source": [
        "We can pass a `fill_method` to handle `NaN` values:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yq9nuDZYlYsf"
      },
      "outputs": [],
      "source": [
        "pd.merge_ordered(\n",
        "    fb_prices.reset_index(), aapl_prices.reset_index(),\n",
        "    fill_method='ffill'\n",
        ").set_index('date').head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReiUcPPRlYsf"
      },
      "source": [
        "Alternatively, we can use `fillna()`.\n",
        "\n",
        "<hr>\n",
        "<div style=\"overflow: hidden; margin-bottom: 10px;\">\n",
        "    <div style=\"float: left;\">\n",
        "         <a href=\"./3-aggregations.ipynb\">\n",
        "        <button>&#8592; Previous Notebook</button>\n",
        "    </a>\n",
        "    </div>\n",
        "    <div style=\"float: right;\">\n",
        "        <a href=\"../../solutions/ch_04/solutions.ipynb\">\n",
        "            <button>Solutions</button>\n",
        "        </a>\n",
        "        <a href=\"../ch_05/1-introducing_matplotlib.ipynb\">\n",
        "            <button>Chapter 5 &#8594;</button>\n",
        "        </a>\n",
        "    </div>\n",
        "</div>\n",
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5-1. introduce matplotlib**"
      ],
      "metadata": {
        "id": "tDQ59kqtmIpc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtqdzMOAbKnp"
      },
      "source": [
        "# Getting Started with Matplotlib\n",
        "\n",
        "Pandas uses `matplotlib` to create visualizations. Therefore, before we learn how to plot with `pandas`, it's important to understand how `matplotlib` works at a high-level, which is the focus of this notebook.\n",
        "\n",
        "\n",
        "## About the Data\n",
        "In this notebook, we will be working with 2 datasets:\n",
        "- Facebook's stock price throughout 2018 (obtained using the [`stock_analysis` package](https://github.com/stefmolin/stock-analysis))\n",
        "- Earthquake data from September 18, 2018 - October 13, 2018 (obtained from the US Geological Survey (USGS) using the [USGS API](https://earthquake.usgs.gov/fdsnws/event/1/))\n",
        "\n",
        "## Setup\n",
        "We need to import `matplotlib.pyplot` for plotting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y4_iMth8bKnz"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7oMIslJbKn1"
      },
      "source": [
        "## Plotting lines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ehpCTEcMbKn2"
      },
      "outputs": [],
      "source": [
        "fb = pd.read_csv(\n",
        "    'data/fb_stock_prices_2018.csv', index_col='date', parse_dates=True\n",
        ")\n",
        "\n",
        "plt.plot(fb.index, fb.open)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2RHvxAmbKn4"
      },
      "source": [
        "Since we are working in a Jupyter notebook, we can use the magic command `%matplotlib inline` once and not have to call `plt.show()` for each plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97nEd-7gbKn5"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "fb = pd.read_csv(\n",
        "    'data/fb_stock_prices_2018.csv', index_col='date', parse_dates=True\n",
        ")\n",
        "plt.plot(fb.index, fb.open)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2Mi91KFbKn7"
      },
      "source": [
        "## Scatter plots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzOFvFyZbKn9"
      },
      "source": [
        "We can pass in a string specifying the style of the plot. This is of the form `[marker][linestyle][color]`. For example, we can make a black dashed line with `'--k'` or a red scatter plot with `'or'`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NnNPKa_IbKn-"
      },
      "outputs": [],
      "source": [
        "plt.plot('high', 'low', 'or', data=fb.head(20))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIIb8UBIbKn_"
      },
      "source": [
        "Here are some examples of how you make a format string:\n",
        "\n",
        "| Marker | Linestyle | Color | Format String | Result |\n",
        "| :---: | :---: | :---: | :---: | --- |\n",
        "| | `-` | `b` | `-b` | blue solid line|\n",
        "| `.` |  | `k` | `.k` | black points|\n",
        "|  | `--` | `r` | `--r` | red dashed line|\n",
        "| `o` | `-` | `g` | `o-g` | green solid line with circles|\n",
        "| | `:` | `m` | `:m` | magenta dotted line|\n",
        "|`x` | `-.` | `c` | `x-.c` | cyan dot-dashed line with x's|\n",
        " \n",
        "Note that we can also use format strings of the form `[color][marker][linestyle]`, but the parsing by `matplotlib` (in rare cases) might not be what we were aiming for. Consult the *Notes* section in the [documentation](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.plot.html) for the complete list of options.\n",
        "## Histograms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIEzDpB8bKoA"
      },
      "outputs": [],
      "source": [
        "quakes = pd.read_csv('data/earthquakes.csv')\n",
        "plt.hist(quakes.query('magType == \"ml\"').mag)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yxS3Y7VbKoB"
      },
      "source": [
        "### Bin size matters\n",
        "Notice how our assumptions of the distribution of the data can change based on the number of bins (look at the drop between the two highest peaks on the righthand plot):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tiACI1NzbKoC"
      },
      "outputs": [],
      "source": [
        "x = quakes.query('magType == \"ml\"').mag\n",
        "fig, axes = plt.subplots(1, 2, figsize=(10, 3))\n",
        "for ax, bins in zip(axes, [7, 35]):\n",
        "    ax.hist(x, bins=bins)\n",
        "    ax.set_title(f'bins param: {bins}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8MigbIybKpa"
      },
      "source": [
        "## Plot components\n",
        "### `Figure`\n",
        "Top-level object that holds the other plot components."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4I1k54uJbKpa"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6rwqXXSbKpb"
      },
      "source": [
        "### `Axes`\n",
        "Individual plots contained within the `Figure`.\n",
        "\n",
        "## Creating subplots\n",
        "Simply specify the number of rows and columns to create:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WHCTwrN5bKpb"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3zMbnl3bKpg"
      },
      "source": [
        "As an alternative to using `plt.subplots()` we can add `Axes` objects to the `Figure` object on our own. This allows for some more complex layouts, such as picture in picture:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m9UXQ2hPbKph"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(3, 3))\n",
        "outside = fig.add_axes([0.1, 0.1, 0.9, 0.9])\n",
        "inside = fig.add_axes([0.7, 0.7, 0.25, 0.25])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xZQ9OTMbKpi"
      },
      "source": [
        "## Creating Plot Layouts with `gridspec`\n",
        "We can create subplots with varying sizes as well:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7lEdeNqcbKpi"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(8, 8))\n",
        "gs = fig.add_gridspec(3, 3)\n",
        "top_left = fig.add_subplot(gs[0, 0])\n",
        "mid_left = fig.add_subplot(gs[1, 0])\n",
        "top_right = fig.add_subplot(gs[:2, 1:])\n",
        "bottom = fig.add_subplot(gs[2,:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EM-KQxBJbKpk"
      },
      "source": [
        "## Saving plots\n",
        "Use `plt.savefig()` to save the last created plot. To save a specific `Figure` object, use its `savefig()` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AQLC50B0bKpk"
      },
      "outputs": [],
      "source": [
        "fig.savefig('empty.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTcopQTbbKpl"
      },
      "source": [
        "## Cleaning up\n",
        "It's important to close resources when we are done with them. We use `plt.close()` to do so. If we pass in nothing, it will close the last plot, but we can pass in the specific `Figure` object to close or say `'all'` to close all `Figure` objects that are open. Let's close all the `Figure` objects that are open with `plt.close()`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IjZ-TjfubKpl"
      },
      "outputs": [],
      "source": [
        "plt.close('all')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qeg_FjlhbKpm"
      },
      "source": [
        "## Additional plotting options\n",
        "### Specifying figure size\n",
        "Just pass the `figsize` argument to `plt.figure()`. It's a tuple of `(width, height)`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mvRyrSl4bKpm"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(10, 4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vt_hTn0ybKpm"
      },
      "source": [
        "This can be specified when creating subplots as well:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SY47w9OzbKpm"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(10, 4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dM6IWgM8bKpn"
      },
      "source": [
        "### `rcParams`\n",
        "A small subset of all the available plot settings (shuffling to get a good variation of options):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t4RJ87EIbKpn"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import matplotlib as mpl\n",
        "\n",
        "rcparams_list = list(mpl.rcParams.keys())\n",
        "random.seed(20) # make this repeatable\n",
        "random.shuffle(rcparams_list)\n",
        "sorted(rcparams_list[:20])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Dy9siBEbKpo"
      },
      "source": [
        "We can check the current default `figsize` using `rcParams`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xiu7hi55bKpo"
      },
      "outputs": [],
      "source": [
        "mpl.rcParams['figure.figsize']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdqfRphWbKpo"
      },
      "source": [
        "We can also update this value to change the default (until the kernel is restarted):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sAWL9OX0bKpo"
      },
      "outputs": [],
      "source": [
        "mpl.rcParams['figure.figsize'] = (300, 10)\n",
        "mpl.rcParams['figure.figsize']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxmlcxPybKpp"
      },
      "source": [
        "Use `rcdefaults()` to restore the defaults. Note this is slightly different than before because running `%matplotlib inline` sets a different value for `figsize` ([see more](https://github.com/ipython/ipykernel/blob/master/ipykernel/pylab/config.py#L42-L56)). After we reset, we are going back to the default value of `figsize` before that import:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N7muA45PbKpp"
      },
      "outputs": [],
      "source": [
        "mpl.rcdefaults()\n",
        "mpl.rcParams['figure.figsize']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2tdvfmlbKpq"
      },
      "source": [
        "This can also be done via `pyplot`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i3P_m4HWbKpq"
      },
      "outputs": [],
      "source": [
        "plt.rc('figure', figsize=(20, 20)) # change `figsize` default to (20, 20)\n",
        "plt.rcdefaults() # reset the default"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBOCNd9MbKpq"
      },
      "source": [
        "<hr>\n",
        "<div>\n",
        "    <a href=\"../ch_04/4-time_series.ipynb\">\n",
        "        <button>&#8592; Chapter 4</button>\n",
        "    </a>\n",
        "    <a href=\"./2-plotting_with_pandas.ipynb\">\n",
        "        <button style=\"float: right;\">Next Notebook &#8594;</button>\n",
        "    </a>\n",
        "</div>\n",
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5-2.plotting with pandas**"
      ],
      "metadata": {
        "id": "Gyk1lyIdmRCK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nb7ctbklbhg"
      },
      "source": [
        "# Plotting with Pandas\n",
        "The `plot()` method is available on `Series` and `DataFrame` objects. Many of the parameters get passed down to `matplotlib`. The `kind` argument let's us vary the plot type. Here are some commonly used parameters:\n",
        "\n",
        "| Parameter | Purpose | Data Type |\n",
        "| --- | --- | --- |\n",
        "| `kind` | Determines the plot type | String |\n",
        "| `x`/`y` | Column(s) to plot on the *x*-axis/*y*-axis | String or list |\n",
        "| `ax` | Draws the plot on the `Axes` object provided | `Axes` |\n",
        "| `subplots` | Determines whether to make subplots | Boolean |\n",
        "| `layout` | Specifies how to arrange the subplots | Tuple of `(rows, columns)` |\n",
        "| `figsize` | Size to make the `Figure` object | Tuple of `(width, height)` | \n",
        "| `title` | The title of the plot or subplots | String for the plot title or a list of strings for subplot titles |\n",
        "| `legend` | Determines whether to show the legend | Boolean |\n",
        "| `label` | What to call an item in the legend | String if a single column is being plotted; otherwise, a list of strings |\n",
        "| `style` | `matplotlib` style strings for each item being plotted | String if a single column is being plotted; otherwise, a list of strings |\n",
        "| `color` | The color to plot the item in | String or red, green, blue tuple if a single column is being plotted; otherwise, a list |\n",
        "| `colormap` | The colormap to use | String or `matplotlib` colormap object |\n",
        "| `logx`/`logy`/`loglog` | Determines whether to use a logarithmic scale for the *x*-axis, *y*-axis, or both | Boolean |\n",
        "| `xticks`/`yticks` | Determines where to draw the ticks on the *x*-axis/*y*-axis | List of values |\n",
        "| `xlim`/`ylim` | The axis limits for the *x*-axis/*y*-axis | Tuple of the form `(min, max)` |\n",
        "| `rot` | The angle to write the tick labels at | Integer |\n",
        "| `sharex`/`sharey` | Determines whether to have subplots share the *x*-axis/*y*-axis | Boolean |\n",
        "| `fontsize` | Controls the size of the tick labels | Integer |\n",
        "| `grid` | Turns on/off the grid lines | Boolean |\n",
        "\n",
        "## About the Data\n",
        "In this notebook, we will be working with 3 datasets:\n",
        "- Facebook's stock price throughout 2018 (obtained using the [`stock_analysis` package](https://github.com/stefmolin/stock-analysis))\n",
        "- Earthquake data from September 18, 2018 - October 13, 2018 (obtained from the US Geological Survey (USGS) using the [USGS API](https://earthquake.usgs.gov/fdsnws/event/1/))\n",
        "- European Centre for Disease Prevention and Control's (ECDC) [daily number of new reported cases of COVID-19 by country worldwide dataset](https://www.ecdc.europa.eu/en/publications-data/download-todays-data-geographic-distribution-covid-19-cases-worldwide) collected on September 19, 2020 via [this link](https://opendata.ecdc.europa.eu/covid19/casedistribution/csv)\n",
        "\n",
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Osj4zasilbhq"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "fb = pd.read_csv(\n",
        "    'data/fb_stock_prices_2018.csv', index_col='date', parse_dates=True\n",
        ")\n",
        "quakes = pd.read_csv('data/earthquakes.csv')\n",
        "covid = pd.read_csv('data/covid19_cases.csv').assign(\n",
        "    date=lambda x: pd.to_datetime(x.dateRep, format='%d/%m/%Y')\n",
        ").set_index('date').replace(\n",
        "    'United_States_of_America', 'USA'\n",
        ").sort_index()['2020-01-18':'2020-09-18']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKb_EaKSlbhs"
      },
      "source": [
        "## Evolution over time\n",
        "Line plots help us see how a variable changes over time. They are the default for the `kind` argument, but we can pass `kind='line'` to be explicit in our intent:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X6-221Zmlbht"
      },
      "outputs": [],
      "source": [
        "fb.plot(\n",
        "    kind='line',\n",
        "    y='open',\n",
        "    figsize=(10, 5),\n",
        "    style='-b',\n",
        "    legend=False,\n",
        "    title='Evolution of Facebook Open Price'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYOpresMlbhv"
      },
      "source": [
        "We provided the `style` argument in the previous example; however, we can use the `color` and `linestyle` arguments to get the same result:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1qYi3i1vlbhw"
      },
      "outputs": [],
      "source": [
        "fb.plot(\n",
        "    kind='line',\n",
        "    y='open',\n",
        "    figsize=(10, 5),\n",
        "    color='blue',\n",
        "    linestyle='solid',\n",
        "    legend=False,\n",
        "    title='Evolution of Facebook Open Price'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkOIsWiJlbhx"
      },
      "source": [
        "We can also plot many lines at once by simply passing a list of the columns to plot:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2BFtsb1lbhx"
      },
      "outputs": [],
      "source": [
        "fb.first('1W').plot(\n",
        "    y=['open', 'high', 'low', 'close'],\n",
        "    style=['o-b', '--r', ':k', '.-g'],\n",
        "    title='Facebook OHLC Prices during 1st Week of Trading 2018'\n",
        ").autoscale()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoVNqBgBlbhy"
      },
      "source": [
        "### Creating subplots\n",
        "When plotting with `pandas`, creating subplots is simply a matter of passing `subplots=True` to the `plot()` method, and (optionally) specifying the `layout` in a tuple of `(rows, columns)`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w1MH6WXKlbhz"
      },
      "outputs": [],
      "source": [
        "fb.plot(\n",
        "    kind='line',\n",
        "    subplots=True,\n",
        "    layout=(3, 2),\n",
        "    figsize=(15, 10),\n",
        "    title='Facebook Stock 2018'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cyopg6DGlbh4"
      },
      "source": [
        "Note that we didn't provide a specific column to plot and `pandas` plotted all of them for us.\n",
        "\n",
        "Sometimes we want to make subplots that each have a few variables in them for comparison. This can be achieved using the `ax` parameter. To illustrate this, let's take a look at daily new COVID-19 cases in China, Spain, Italy, the USA, Brazil, and India:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tYh5pd_plbh4"
      },
      "outputs": [],
      "source": [
        "new_cases_rolling_average = covid.pivot_table(\n",
        "    index=covid.index, \n",
        "    columns='countriesAndTerritories', \n",
        "    values='cases'\n",
        ").rolling(7).mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCsidhkWlbh6"
      },
      "source": [
        "Since there is a lot of fluctuation in these values, we will plot the 7-day moving average of new cases using the `rolling()` method (discussed in chapter 4). Rather than create a separate plot for each country (which makes it harder to compare) or plot them all together (which will make it difficult to see the smaller values), we will plot countries that have had a similar number of cases in the same subplot:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6qdmAc3vlbh7"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "new_cases_rolling_average[['China']].plot(ax=axes[0], style='-.c')\n",
        "new_cases_rolling_average[['Italy', 'Spain']].plot(\n",
        "    ax=axes[1], style=['-', '--'], \n",
        "    title='7-day rolling average of new COVID-19 cases\\n(source: ECDC)'\n",
        ")\n",
        "new_cases_rolling_average[['Brazil', 'India', 'USA']]\\\n",
        "    .plot(ax=axes[2], style=['--', ':', '-'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqdKzuMblbh8"
      },
      "source": [
        "*NOTE: we specified the line styles here so that the lines can be distinguished in the text as a black and white image.*\n",
        "\n",
        "In the previous figure, we were able to compare countries with similar levels of new COVID-19 cases, but we couldn't compare all of them in the same plot due to scale. One way around this is to use an **area plot**, which makes it possible for us to visualize the overall 7-day rolling average of new COVID-19 cases and at the same time how much each country is contributing to the total. In the interest of readability, we will group Italy and Spain together and create another category for countries other than the USA, Brazil, and India. The combined height of the plot areas is the overall value, and the height of given shaded region is the value for the individual country."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EokX7YKRlbh9"
      },
      "outputs": [],
      "source": [
        "cols = [\n",
        "    col for col in new_cases_rolling_average.columns \n",
        "    if col not in ['USA', 'Brazil', 'India', 'Italy & Spain']\n",
        "]\n",
        "new_cases_rolling_average.assign(\n",
        "    **{'Italy & Spain': lambda x: x.Italy + x.Spain}\n",
        ").sort_index(axis=1).assign(\n",
        "    Other=lambda x: x[cols].sum(axis=1)\n",
        ").drop(columns=cols).plot(\n",
        "    kind='area', figsize=(15, 5), \n",
        "    title='7-day rolling average of new COVID-19 cases\\n(source: ECDC)'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzJXSIkZlbh-"
      },
      "source": [
        "Another way to visualize evolution over time is to look at the cumulative sum over time. Let's plot the cumulative number of COVID-19 cases in China, Spain, Italy, the USA, Brazil, and India, using `ax` to create subplots as we did in the previous example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9RS6gMFXlbh-"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 3, figsize=(15, 3))\n",
        "\n",
        "cumulative_covid_cases = covid.groupby(\n",
        "    ['countriesAndTerritories', pd.Grouper(freq='1D')]\n",
        ").cases.sum().unstack(0).apply('cumsum')\n",
        "\n",
        "cumulative_covid_cases[['China']].plot(ax=axes[0], style='-.c')\n",
        "cumulative_covid_cases[['Italy', 'Spain']].plot(\n",
        "    ax=axes[1], style=['-', '--'], \n",
        "    title='Cumulative COVID-19 Cases\\n(source: ECDC)'\n",
        ")\n",
        "cumulative_covid_cases[['Brazil', 'India', 'USA']]\\\n",
        "    .plot(ax=axes[2], style=['--', ':', '-'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9AAnsF6lbiA"
      },
      "source": [
        "*NOTE: we specified the line styles here so that the lines can be distinguished in the text as a black and white image.*\n",
        "\n",
        "## Visualizing relationships between variables\n",
        "### Scatter plots\n",
        "We make scatter plots to help visualize the relationship between two variables. Creating scatter plots requires we pass in `kind='scatter'` along with a column for the x-axis and a column for the y-axis:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jh3luz1RlbiB"
      },
      "outputs": [],
      "source": [
        "fb.assign(\n",
        "    max_abs_change=fb.high - fb.low\n",
        ").plot(\n",
        "    kind='scatter', x='volume', y='max_abs_change',\n",
        "    title='Facebook Daily High - Low vs. Volume Traded'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M58su5HulbiC"
      },
      "source": [
        "The relationship doesn't seem to be linear, but we can try a log transform on the x-axis since the scales of the axes are very different. With `pandas`, we simply pass in `logx=True`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NxbK0KLslbiC"
      },
      "outputs": [],
      "source": [
        "fb.assign(\n",
        "    max_abs_change=fb.high - fb.low\n",
        ").plot(\n",
        "    kind='scatter', x='volume', y='max_abs_change',\n",
        "    title='Facebook Daily High - Low vs. log(Volume Traded)', \n",
        "    logx=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XlxEdP-lbiD"
      },
      "source": [
        "With `matplotlib`, we could use `plt.xscale('log')` to do the same thing.\n",
        "\n",
        "### Adding Transparency to Plots with `alpha`\n",
        "Sometimes our plots have many overlapping values, but this can be impossible to see. This can be addressed by increasing the transparency of what we are plotting using the `alpha` parameter. It is a float in the range [0, 1] where 0 is completely transparent and 1 is completely opaque. By default this is 1, so let's put in a lower value and re-plot the scatter plot:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HZ2eQmnslbiD"
      },
      "outputs": [],
      "source": [
        "fb.assign(\n",
        "    max_abs_change=fb.high - fb.low\n",
        ").plot(\n",
        "    kind='scatter', x='volume', y='max_abs_change',\n",
        "    title='Facebook Daily High - Low vs. log(Volume Traded)',\n",
        "    logx=True, alpha=0.25\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klg059ztlbiL"
      },
      "source": [
        "### Hexbins\n",
        "In the previous example, we can start to see the overlaps, but it is still difficult. Hexbins are another plot type that divide up the plot into hexagons, which are shaded according to the density of points there. With `pandas`, this is the `hexbin` value for the `kind` argument. It may also be necessary to tweak the `gridsize`, which determines the number of hexagons along the y-axis:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IWcM2cExlbiM"
      },
      "outputs": [],
      "source": [
        "fb.assign(\n",
        "    log_volume=np.log(fb.volume),\n",
        "    max_abs_change=fb.high - fb.low\n",
        ").plot(\n",
        "    kind='hexbin',\n",
        "    x='log_volume',\n",
        "    y='max_abs_change',\n",
        "    title='Facebook Daily High - Low vs. log(Volume Traded)',\n",
        "    colormap='gray_r',\n",
        "    gridsize=20, \n",
        "    sharex=False # we have to pass this to see the x-axis\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxZ9YPcflbiM"
      },
      "source": [
        "### Visualizing Correlations with Heatmaps\n",
        "Pandas doesn't offer heatmaps; however, if we are able to get our data into a matrix, we can use `matshow()` from matplotlib:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zuR8TznflbiN"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(20, 10))\n",
        "\n",
        "# calculate the correlation matrix\n",
        "fb_corr = fb.assign(\n",
        "    log_volume=np.log(fb.volume),\n",
        "    max_abs_change=fb.high - fb.low\n",
        ").corr()\n",
        "\n",
        "# create the heatmap and colorbar\n",
        "im = ax.matshow(fb_corr, cmap='seismic')\n",
        "im.set_clim(-1, 1)\n",
        "fig.colorbar(im)\n",
        "\n",
        "# label the ticks with the column names\n",
        "labels = [col.lower() for col in fb_corr.columns]\n",
        "ax.set_xticks(ax.get_xticks()[1:-1]) # to handle bug in matplotlib\n",
        "ax.set_xticklabels(labels, rotation=45)\n",
        "ax.set_yticks(ax.get_yticks()[1:-1]) # to handle bug in matplotlib\n",
        "ax.set_yticklabels(labels)\n",
        "\n",
        "# include the value of the correlation coefficient in the boxes\n",
        "for (i, j), coef in np.ndenumerate(fb_corr):\n",
        "    ax.text(\n",
        "        i, j, fr'$\\rho$ = {coef:.2f}', # raw (r), format (f) string\n",
        "        ha='center', va='center', \n",
        "        color='white', fontsize=14\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mMU6cBdlbiO"
      },
      "source": [
        "Accessing the values in the correlation matrix can be done with `loc[]`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ovN-JWwUlbiO"
      },
      "outputs": [],
      "source": [
        "fb_corr.loc['max_abs_change', ['volume', 'log_volume']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2vBMgQhlbiP"
      },
      "source": [
        "## Visualizing distributions\n",
        "### Histograms\n",
        "With the `pandas`, making histograms is as easy as passing `kind='hist'` to the `plot()` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vn-FxuR5lbiP"
      },
      "outputs": [],
      "source": [
        "fb.volume.plot(\n",
        "    kind='hist', \n",
        "    title='Histogram of Daily Volume Traded in Facebook Stock'\n",
        ")\n",
        "plt.xlabel('Volume traded') # label the x-axis (discussed in chapter 6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCNU4qoplbiQ"
      },
      "source": [
        "We can overlap histograms to compare distributions provided we use the `alpha` parameter. For example, let's compare the usage and magnitude of the various measurement techniques (the `magType` column) in the data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ujs5v_0LlbiQ"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(figsize=(8, 5))\n",
        "\n",
        "for magtype in quakes.magType.unique():\n",
        "    data = quakes.query(f'magType == \"{magtype}\"').mag\n",
        "    if not data.empty:\n",
        "        data.plot(\n",
        "            kind='hist', ax=axes, alpha=0.4, \n",
        "            label=magtype, legend=True,\n",
        "            title='Comparing histograms of earthquake magnitude by magType'\n",
        "        )\n",
        "\n",
        "plt.xlabel('magnitude') # label the x-axis (discussed in chapter 6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8JEs-mNlbid"
      },
      "source": [
        "### Kernel Density Estimation (KDE)\n",
        "We can pass `kind='kde'` for an estimate of the probability density function (PDF), which tells us the probability of getting a particular value:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CTqLrSPFlbie"
      },
      "outputs": [],
      "source": [
        "fb.high.plot(\n",
        "    kind='kde', \n",
        "    title='KDE of Daily High Price for Facebook Stock'\n",
        ")\n",
        "plt.xlabel('Price ($)') # label the x-axis (discussed in chapter 6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUrBvt97lbie"
      },
      "source": [
        "### Adding to the result of `plot()`\n",
        "The `plot()` method returns an `Axes` object. We can store this for additional customization of the plot, or we can pass this into another call to `plot()` as the `ax` argument to add to the original plot. \n",
        "\n",
        "It can often be helpful to view the KDE superimposed on top of the histogram, which can be achieved with this strategy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jxCgGHHilbif"
      },
      "outputs": [],
      "source": [
        "ax = fb.high.plot(kind='hist', density=True, alpha=0.5)\n",
        "fb.high.plot(\n",
        "    ax=ax, kind='kde', color='blue', \n",
        "    title='Distribution of Facebook Stock\\'s Daily High Price in 2018'\n",
        ")\n",
        "plt.xlabel('Price ($)') # label the x-axis (discussed in chapter 6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHGiHJS5lbig"
      },
      "source": [
        "### Plotting the ECDF\n",
        "In some cases, we are more interested in the probability of getting less than or equal to that value (or greater than or equal), which we can see with the **cumulative disribution function (CDF)**. Using the `statsmodels` package, we can estimate the CDF giving us the **empirical cumulative distribution function (ECDF)**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yswd1nsklbig"
      },
      "outputs": [],
      "source": [
        "from statsmodels.distributions.empirical_distribution import ECDF\n",
        "\n",
        "ecdf = ECDF(quakes.query('magType == \"ml\"').mag)\n",
        "plt.plot(ecdf.x, ecdf.y)\n",
        "\n",
        "# axis labels (we will cover this in chapter 6)\n",
        "plt.xlabel('mag') # add x-axis label \n",
        "plt.ylabel('cumulative probability') # add y-axis label\n",
        "\n",
        "# add title (we will cover this in chapter 6)\n",
        "plt.title('ECDF of earthquake magnitude with magType ml')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvxbVwTflbii"
      },
      "source": [
        "This ECDF tells us the probability of getting an earthquake with magnitude of 3 or less using the `ml` scale is 98%:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bVnYMMR3lbii"
      },
      "outputs": [],
      "source": [
        "from statsmodels.distributions.empirical_distribution import ECDF\n",
        "\n",
        "ecdf = ECDF(quakes.query('magType == \"ml\"').mag)\n",
        "plt.plot(ecdf.x, ecdf.y)\n",
        "\n",
        "# formatting below will all be covered in chapter 6\n",
        "# axis labels\n",
        "plt.xlabel('mag') # add x-axis label \n",
        "plt.ylabel('cumulative probability') # add y-axis label\n",
        "\n",
        "# add reference lines for interpreting the ECDF for mag <= 3 \n",
        "plt.plot(\n",
        "    [3, 3], [0, .98], '--k', \n",
        "    [-1.5, 3], [0.98, 0.98], '--k', alpha=0.4\n",
        ")\n",
        "\n",
        "# set axis ranges\n",
        "plt.ylim(0, None)\n",
        "plt.xlim(-1.25, None)\n",
        "\n",
        "# add a title\n",
        "plt.title('P(mag <= 3) = 98%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIapzVxWlbij"
      },
      "source": [
        "### Box plots\n",
        "To make box plots with `pandas`, we pass `kind='box'` to the `plot()` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dl4oHDfFlbij"
      },
      "outputs": [],
      "source": [
        "fb.iloc[:,:4].plot(kind='box', title='Facebook OHLC Prices Box Plot')\n",
        "plt.ylabel('price ($)') # label the x-axis (discussed in chapter 6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4Kcl7mUlbik"
      },
      "source": [
        "If we pass in `notch=True`, we get a notched box plot. The notch represents a 95% confidence interval around the median, which can be helpful when comparing differences. For an introduction to interpreting a notched box plot, see this [Google sites page](https://sites.google.com/site/davidsstatistics/home/notched-box-plots) and this [Towards Data Science article](https://towardsdatascience.com/understanding-boxplots-5e2df7bcbd51)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJJxNgrrlbil"
      },
      "outputs": [],
      "source": [
        "fb.iloc[:,:4].plot(kind='box', title='Facebook OHLC Prices Box Plot', notch=True)\n",
        "plt.ylabel('price ($)') # label the x-axis (discussed in chapter 6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMAmEVzhlbin"
      },
      "source": [
        "This can also be combined with a call to `groupby()`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vYP38kl3lbio"
      },
      "outputs": [],
      "source": [
        "fb.assign(\n",
        "    volume_bin=pd.cut(fb.volume, 3, labels=['low', 'med', 'high'])\n",
        ").groupby('volume_bin').boxplot(\n",
        "    column=['open', 'high', 'low', 'close'],\n",
        "    layout=(1, 3), figsize=(12, 3)\n",
        ")\n",
        "plt.suptitle('Facebook OHLC Box Plots by Volume Traded', y=1.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-yiPPEVlbip"
      },
      "source": [
        "We can use this to see the distribution of magnitudes across the different measurement methods for earthquakes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "euDtBNEdlbir"
      },
      "outputs": [],
      "source": [
        "quakes[['mag', 'magType']].groupby('magType').boxplot(\n",
        "    figsize=(15, 8), subplots=False\n",
        ")\n",
        "plt.title('Earthquake Magnitude Box Plots by magType')\n",
        "plt.ylabel('magnitude') # label the y-axis (discussed in chapter 6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjRMDa2zlbir"
      },
      "source": [
        "## Counts and frequencies\n",
        "### Bar charts\n",
        "Passing `kind='barh'` gives us horizontal bars while `kind='bar'` gives us vertical ones. Let's use horizontal bars to look at the top 15 places for earthquakes in our data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KQPuSeVqlbis"
      },
      "outputs": [],
      "source": [
        "quakes.parsed_place.value_counts().iloc[14::-1,].plot(\n",
        "    kind='barh', figsize=(10, 5),\n",
        "    title='Top 15 Places for Earthquakes '\n",
        "          '(September 18, 2018 - October 13, 2018)'\n",
        ")\n",
        "plt.xlabel('earthquakes') # label the x-axis (discussed in chapter 6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "749at-ZllbjA"
      },
      "source": [
        "We also have data on whether earthquakes were accompanied by tsunamis. Let's see what the top places for tsunamis are:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3O2ZeoPJlbjB"
      },
      "outputs": [],
      "source": [
        "quakes.groupby('parsed_place').tsunami.sum().sort_values().iloc[-10:,].plot(\n",
        "    kind='barh', figsize=(10, 5), \n",
        "    title='Top 10 Places for Tsunamis '\n",
        "          '(September 18, 2018 - October 13, 2018)'\n",
        ")\n",
        "plt.xlabel('tsunamis') # label the x-axis (discussed in chapter 6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1PmKKTrlbjB"
      },
      "source": [
        "Seeing that Indonesia is the top place for tsunamis during the time period we are looking at, we may want to look how many earthquakes and tsunamis Indonesia gets on a daily basis. We could show this as a line plot or with bars; since we don't want to interpolate, we will use bars here:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ll9QJBDlbjB"
      },
      "outputs": [],
      "source": [
        "indonesia_quakes = quakes.query('parsed_place == \"Indonesia\"').assign(\n",
        "    time=lambda x: pd.to_datetime(x.time, unit='ms'),\n",
        "    earthquake=1\n",
        ").set_index('time').resample('1D').sum()\n",
        "\n",
        "# format the datetimes in the index for the x-axis\n",
        "indonesia_quakes.index = indonesia_quakes.index.strftime('%b\\n%d')\n",
        "\n",
        "indonesia_quakes.plot(\n",
        "    y=['earthquake', 'tsunami'], kind='bar', figsize=(15, 3), \n",
        "    rot=0, label=['earthquakes', 'tsunamis'], \n",
        "    title='Earthquakes and Tsunamis in Indonesia '\n",
        "          '(September 18, 2018 - October 13, 2018)'\n",
        ")\n",
        "\n",
        "# label the axes (discussed in chapter 6)\n",
        "plt.xlabel('date')\n",
        "plt.ylabel('count')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4aNkq5slbjC"
      },
      "source": [
        "### Grouped Bars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GLplPE8glbjD"
      },
      "outputs": [],
      "source": [
        "quakes.groupby(['parsed_place', 'tsunami']).mag.count()\\\n",
        "    .unstack().apply(lambda x: x / x.sum(), axis=1)\\\n",
        "    .rename(columns={0: 'no', 1: 'yes'})\\\n",
        "    .sort_values('yes', ascending=False)[7::-1]\\\n",
        "    .plot.barh(\n",
        "        title='Frequency of a tsunami accompanying an earthquake'\n",
        "    )\n",
        "\n",
        "# move legend to the right of the plot\n",
        "plt.legend(title='tsunami?', bbox_to_anchor=(1, 0.65))\n",
        "\n",
        "# label the axes (discussed in chapter 6)\n",
        "plt.xlabel('percentage of earthquakes')\n",
        "plt.ylabel('')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_sz8cC8lbjD"
      },
      "source": [
        "Using the `kind` arugment for vertical bars when the labels for each bar are shorter:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8j6TOslflbjE"
      },
      "outputs": [],
      "source": [
        "quakes.magType.value_counts().plot(\n",
        "    kind='bar', title='Earthquakes Recorded per magType', rot=0\n",
        ")\n",
        "\n",
        "# label the axes (discussed in chapter 6)\n",
        "plt.xlabel('magType')\n",
        "plt.ylabel('earthquakes')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYQ-_R0XlbjF"
      },
      "source": [
        "### Stacked bars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o72FYjm7lbjF"
      },
      "outputs": [],
      "source": [
        "pivot = quakes.assign(\n",
        "    mag_bin=lambda x: np.floor(x.mag)\n",
        ").pivot_table(\n",
        "    index='mag_bin', columns='magType', values='mag', aggfunc='count'\n",
        ")\n",
        "pivot.plot.bar(\n",
        "    stacked=True, rot=0, ylabel='earthquakes', \n",
        "    title='Earthquakes by integer magnitude and magType'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yup7NVFolbjG"
      },
      "source": [
        "#### Normalized stacked bars\n",
        "Plot the percentages to be better able to see the different `magTypes`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zgCvSTsulbjG"
      },
      "outputs": [],
      "source": [
        "normalized_pivot = pivot.fillna(0).apply(lambda x: x / x.sum(), axis=1)\n",
        "ax = normalized_pivot.plot.bar(\n",
        "    stacked=True, rot=0, figsize=(10, 5),\n",
        "    title='Percentage of earthquakes by integer magnitude for each magType'\n",
        ")\n",
        "ax.legend(bbox_to_anchor=(1, 0.8)) # move legend to the right of the plot\n",
        "plt.ylabel('percentage') # label the axes (discussed in chapter 6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPwjDDT3lbjH"
      },
      "source": [
        "We can also create horizontal stacked bars and do so using `groupby()` and `unstack()`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uM_HfKfnlbjH"
      },
      "outputs": [],
      "source": [
        "quakes.groupby(['parsed_place', 'tsunami']).mag.count()\\\n",
        "    .unstack().apply(lambda x: x / x.sum(), axis=1)\\\n",
        "    .rename(columns={0: 'no', 1: 'yes'})\\\n",
        "    .sort_values('yes', ascending=False)[7::-1]\\\n",
        "    .plot.barh(\n",
        "        title='Frequency of a tsunami accompanying an earthquake', \n",
        "        stacked=True\n",
        "    )\n",
        "\n",
        "# move legend to the right of the plot\n",
        "plt.legend(title='tsunami?', bbox_to_anchor=(1, 0.65))\n",
        "\n",
        "# label the axes (discussed in chapter 6)\n",
        "plt.xlabel('percentage of earthquakes')\n",
        "plt.ylabel('')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKw8S2vmlbjK"
      },
      "source": [
        "<hr>\n",
        "<div>\n",
        "    <a href=\"./1-introducing_matplotlib.ipynb\">\n",
        "        <button>&#8592; Previous Notebook</button>\n",
        "    </a>\n",
        "    <a href=\"./3-pandas_plotting_module.ipynb\">\n",
        "        <button style=\"float: right;\">Next Notebook &#8594;</button>\n",
        "    </a>\n",
        "</div>\n",
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5-3. pandas plotting module**"
      ],
      "metadata": {
        "id": "cq9xPgk0mZ0H"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfjPwSBylcJI"
      },
      "source": [
        "# The `pandas.plotting` module\n",
        "Pandas provides some extra plotting functions for some new plot types.\n",
        "\n",
        "## About the Data\n",
        "In this notebook, we will be working with Facebook's stock price throughout 2018 (obtained using the [`stock_analysis` package](https://github.com/stefmolin/stock-analysis)).\n",
        "\n",
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0XA6WSkjlcJR"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "fb = pd.read_csv(\n",
        "    'data/fb_stock_prices_2018.csv', index_col='date', parse_dates=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzVgfOrwlcJT"
      },
      "source": [
        "## Scatter matrix\n",
        "Easily create scatter plots between all columns in the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VaV5bduClcJU"
      },
      "outputs": [],
      "source": [
        "from pandas.plotting import scatter_matrix\n",
        "scatter_matrix(fb, figsize=(10, 10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4FzONkplcJV"
      },
      "source": [
        "Changing the diagonal from histograms to KDE:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o3OocPcglcJW"
      },
      "outputs": [],
      "source": [
        "scatter_matrix(fb, figsize=(10, 10), diagonal='kde')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NKgd-DolcJW"
      },
      "source": [
        "## Lag plot\n",
        "Lag plots let us see how the variable correlates with past observations of itself. Random data has no pattern:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-u8tNGvVlcJX"
      },
      "outputs": [],
      "source": [
        "from pandas.plotting import lag_plot\n",
        "np.random.seed(0) # make this repeatable\n",
        "lag_plot(pd.Series(np.random.random(size=200)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YyCNCmWlcJX"
      },
      "source": [
        "Data with some level of correlation to itself (autocorrelation) may have patterns. Stock prices are highly autocorrelated:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xdl4BKyBlcJY"
      },
      "outputs": [],
      "source": [
        "lag_plot(fb.close)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIasbcpwlcJY"
      },
      "source": [
        "The default lag is 1, but we can alter this with the `lag` parameter. Let's look at a 5 day lag (a week of trading activity):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yqe6cCg8lcJc"
      },
      "outputs": [],
      "source": [
        "lag_plot(fb.close, lag=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYdF3JnjlcJd"
      },
      "source": [
        "## Autocorrelation plots\n",
        "We can use the autocorrelation plot to see if this relationship may be meaningful or is just noise. Random data will not have any significant autocorrelation (it stays within the bounds below):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UIknjz__lcJe"
      },
      "outputs": [],
      "source": [
        "from pandas.plotting import autocorrelation_plot\n",
        "np.random.seed(0) # make this repeatable\n",
        "autocorrelation_plot(pd.Series(np.random.random(size=200)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VTkZmmqlcJf"
      },
      "source": [
        "Stock data, on the other hand, does have significant autocorrelation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1eXKXp5LlcJf"
      },
      "outputs": [],
      "source": [
        "autocorrelation_plot(fb.close)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sdz5WYpplcJg"
      },
      "source": [
        "## Bootstrap plot\n",
        "This plot helps us understand the uncertainty in our summary statistics:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QL1bhxcIlcJg"
      },
      "outputs": [],
      "source": [
        "from pandas.plotting import bootstrap_plot\n",
        "fig = bootstrap_plot(fb.volume, fig=plt.figure(figsize=(10, 6)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fkos-SuhlcJy"
      },
      "source": [
        "<hr>\n",
        "\n",
        "<div style=\"overflow: hidden; margin-bottom: 10px;\">\n",
        "    <div style=\"float: left;\">\n",
        "        <a href=\"./2-plotting_with_pandas.ipynb\">\n",
        "            <button>&#8592; Previous Notebook</button>\n",
        "        </a>\n",
        "    </div>\n",
        "    <div style=\"float: right;\">\n",
        "        <a href=\"../../solutions/ch_05/solutions.ipynb\">\n",
        "            <button>Solutions</button>\n",
        "        </a>\n",
        "        <a href=\"../ch_06/1-introduction_to_seaborn.ipynb\">\n",
        "            <button>Chapter 6 &#8594;</button>\n",
        "        </a>\n",
        "    </div>\n",
        "</div>\n",
        "<hr>"
      ]
    }
  ]
}